What follows is a couple of terms used in the book, as we follow through:

Mac OS X : This refers to both Mac OS X  and Mac OS X Server. There is some important differences between the two.

Mac OS X Versions : Since the underlying OS changes very little, this book focuses on the 10.5 version.

Text you enter: The text you enter in examples, are bolded. The $ that begins at the line, is just part of Linux, it is not manually typed.

Filenames: Linux OS files are not case-sensitive.

Strings: They are displayed by bolded text, in examples.

Whenever you log into a terminal, the version is usually written on the top of the screen.
You are also asked to log in, where it prompts when you last logged in, and from where.

Since we run Linux on a VM, we connect using an emulation of linux - And we can log in using usernames and passwords,
but also with IP's/Name of different VM's to log in with, depending on the program we use.

For instance, we could use SSH(Secure), telnet(Not secure) or whatever emulation we wish to use.
We can then in some circumstances, provide the IP of the System we wish to log onto.

By default, Mac OS X, does not allow for Remote login.

The reason SSH is secure, is because it encrypts the information when it sends it - Where as of telnet, does not.
Thus, SSH, is more safe, in terms of remote connections - and is found in a lot of places.

If we log into a System with the same Username as we do on the remote one, we can omit the @ notification of logging in,
to which we otherwise cannot. An example of such a login on a SSH based system, is as follows:

ssh max@tiny

if we had the same on both systems, however:

ssh tiny

We can modify a variable called TERM, if we wish, to define how the text is displayed for us.

The reason we use a Text Prompt and CMD unit, is that using a GUI takes up System resources, and for each additional operation,
the more vurnable the code becomes. And further more, it's more sparesome both on Server Resources and on the reality of that
some situations may not have a GUI installed, simply put.

Thus, Linux defaults, oftenly, to a text prompt, albeit there are GUI environments as well.

There are some Psuedographical utilities as well, that uses Text in a painting manner, to create GUI's.

The main prompt way of giving commands, is done in the Shell, of Linux. In this book, we mainly discuss two, which is the
bash (born again shell) or TC shell(tcsh) - most likely it's bash - but others can occur, such as tcsh or Z shell (zsh).

To find what shell we are operating on, we can write ps.

When we write commands, we can usually use the backspace character (<-) to delete chars, or Delete or CTRL+H.
If none of thoose work, we can reset them to their defaults, by the following: stty ek

We can also remove entire words by the virtue of CTRL+W

If we wish to susepnd a program, we can use the command CTRL+Z - and to return it to it's natural state, we can use the
following command: fg

To delete a line, we could use CTRL-X or CTRL-U

To terminate a program, we use CTRL+C or Delete.
if this does not work, we could given the command of jobs, and then do a kill command with -TERM %<number of task>.
An example is as follows:

bigjob //it runs a bigjob
CONTROL-Z
[1]+ Stopped 	bigjob //it stopped a job, in this case, bigjob
$ jobs //we write jobs to find out what it's running
[1]+ Stopped 	bigjob
$ kill -TERM %1 //we terminate the first running program, which occurs to be the big job
$ RETURN //Return
[1]+ Killed 	bigjob

When we use the interuppt method, instead of kill, our programs may interact differently, depending on how they are built:

Some exit immideatly, some interuppt after performing a task, some ignore it. 

Bash has a different set of handling interuppts, where it simply awaits execution of a program if one is running, the other option
being that it simply discard the interuppt line.

To go back to an earlier command, press the UP arrow-key, and if you wish to go to a later one, press the DOWN arrow-key

Just as in MySQL, the higehst instance of priveleges in terms of a system - is the root user - Which has the highest set
of privaleges.

Keep in mind, the root privlege, is akin to admin rights - And means that you should never experiment in Root mode,
as that can damage linux to the point of having to reinstall it.

To gain root privleges, you either log on as the root user, or temporarily elevate yourself to use root privleges, through
the virtue of a Substitute user, aka su.

say we want to open a directory called lost+found

$ls -l /lost+found
ls: can't open directory /lost+found: Permission denied
$su -c 'ls -l /lost+found' //We run a substitute user and run the command again, by virtue of -c 'ls -l /lost+found', where -c is running a command command
Password:
total 0
$

Whenever we work with root privleges, we usually have a # marker, instead of a $

some distributions, such as Ubuntu, run with the root account locked, and instead they only allow access to root through your own password.
It runs this through using something called sudo, instead, and works like this:

$ sudo ls -l /lost+found
[sudo]password for sam:
total 0
$

to spawn a new shell, with root privelages, we can use the -i command in sudo

$sudo -i
[sudo] password for sam: //Enter your password
#ls -l /lost+found
total 0
#exit //exit the root privelages account
logout
$

if we need help on reminders in terms of statements to make and likewise, we can use the --help option, if not, they run with -h or -help:

$ cat --help //we ask for help on what cat does
Usage: cat[OPTION] [FILE]...
Concatnate FILE(s), or standard input, to standard output.

-A, --show-all 		equivilant to -vET
-b, --number-nonblank 		number of nonblank output lines
-e 							equivilant to -vE
-E, --show-ends 			display $ at end of each line
...

if the output from the help command runs off the screen, we can run the command through the lesser pipe as such:

$ ls --help | less

linux does not contain a hardcopy of a manual, but keeps online documentation - which can be accessed using the man command

to use it, we simply enter man followed by what we want to know more about. We could get ot know more about man, by writing man man.

The man usually sends it output through a lesser pipe, which distributes the information over several pages. To switch between pages,
you simply have to press Space - or q, if you wish to quit

Every manual command is split into 10 sections, as follows:

1.User Commands
2.System calls
3.Subroutines
4.Devices
5.File Formats
6.Games
7.Misc
8.System Administration
9.Kernel
10.New

sometimes, a command exists in several sections, and can as such be accessed using a number quatation on what section it is in:

$ man passwd (first section)

$ man 5 passwd (fifth section)

When we wish to search for keywords, we can use the appropos or using the -k line on the man

An example, is as follows, where we search for different keywords related to who

$ apropos who
at.allow(5) 		- determine who can submit jobs via at or batch
at.deny(5)			- determine who can submit jobs via at or batch
from(1)				- print names of those who have sent mail
w(1) 				- Show who is logged on and what they are doing.
w.procps(1)			- Show who is logged on and what they are doing.
who(1) 				- shows who is logged on
whoami(1)			- print effective userid

we can also search for keywords, using the whatis command, in some cases, as follows:

$ whatis who //normally this is not installed by default
who 			(1) 	-	show who is logged on

whatis does the same thing as appropos, but follows only to complete queries that match the whole word into the search

we can also use the info command, to get more up to date information about stuff and to find the full documentation of things

the commands we can use in info, is as follows:

? //lists info commands
Space //Scrolls through the menu of items for which information is available
m followed by the name of a menu displays that menu
m+space //displays a list of menus
q or ctrl+c //quits info

info uses a few shorthand notations to refer to other keys, as follows:

C-h //the C stands for control, so basically ctrl+h
M-x //the M stands for meta key, or the alt key, so alt-x

an example, if we wish to understand the menu for sleep, we can write:

m sleep

In the menus, every menu item is a link to the info page for that specific kind of menu
we can scroll through them by the virtue of arrowkeys

to iterate through nodes, which is a set of menus, you can press space for the next or p to see the previous.

if we need to understand context and how something relates to another part, we can use the HOWTO, which is found
at tlpd.org

we can also use local shared documents to find info, this provides a lot deeper info than the other versions do:

$ less /usr/share/doc/grep/README //gives info on grep
$ zcat /usr/share/doc/info/README.gz | less //Gives less info on info

In linux, we can access different consoles that are run at the same time, by virtue of Ctrl+Alt
we can also give quick access to a number of a console by virtue of CTRL+ALT+F<numberofConsole>

so for instance, CTRL+ALT+F5, gives the 5th console.

Usually, in the ground, there are 6 consoles run on Linux. The first console, is the system console,
where as of any Graphical ones you would open, will open in a new one, which leads to being usually the 7th,
since we run with the 6 standard ones already.

when we connect on larger networks, we need to specify the computer we wish to connect to, as different
machines handle usernames and passwords on a different basis (they are stored locally)

to log out, press CTRL+D or write exit. This gives a EOF(End of File) character to the Shell, and terminates it. (that specific shell, that is)

to change your password, provide the passwd command, it will first ask for a password, which is the current one, followed by a request for a new password

we can use the utility of pwgen to generate almost random passwords. 

The system then asks you to provide the password again, as per usual.

to access graphical applications, we use the Keychain password, and as such, if we reset out password, we do not reset the Keychain password, which needs to be
changed individually.

When we work with the shell, there are some built in utilities, they are as follows:

ls, cat, rm, less and hostname : There are several, but these are the most basic ones.

The following characters are special characters, that you will use in a special context, thus, they are not relevant to use in naming conventions etc.

& ; | * ? ' ` [ ] ( ) $ < > { } # / \ ! ~

Backspace: ends a command line or execution of a command line

space and tab: they are considered whitespace

to quote special characters, we use escape characters on it - so we put it in quotation marks \(. This works for all, except for the /, which is always
a pathname separator.

When two special characters occur in tandem, we have to seperate them with \, as \*\*, for instance

You can quote the \ with just a normal \, as well

Normal quoting can also occur through '' notation, as per usual.

ls: this utility displays all the files in the home directory.

assuming we made a text file called practice, we could do the following:

$ ls
practice
$ cat practice //open the file practice with the cat commando, short for catenate, displays a text file
This is a small file that i created
with a text editor.
$ rm practice //We remove the practice file, with the command rm
$ ls //list all the files
$ cat practice //attempt to open the file with the cat commando, which is to display a text file, file does not exist, 404's
cat: practice No such file or directory

we can use a safety mode with removemode, that adds a conditional acceptance - as in - it prompts us to provide y to assure that we
wish to delete said file with the name displayed.

So say we want to safely remove a file called "derp", we could do as follows:

$ rm -i derp
rm: remove regular file 'derp'? y

We could also create a alias for the rm in the start up configuration, so that rm always identifies to be as rm -i

We can use the pager functions of less and more, to to go one page forward or one page backwards, at a time.
They display one page at a time and can be naviagetd by the spacebar. to find help, we could write h

to find out what machine we are currently operating upon, we can use the hostname command:

$ hostname
dog

we can copy files, using the cp command, where we specify the file to copy and the destination file to put the info into:

cp source-file destination-file

an example:

$ ls
memo
$ cp memo memo.copy
$ ls
memo memo.copy

it is good praxis to provide date of copy on a file, and can be done as such:

cp memo memo.copy0130 //we added that the copy is made 30:th of january

Since Cp auto-overwrites unless in interactive mode, it is advised to run cp in interactive mode, which is as follows:

$ ls //list all the files
memo memo.copy
$ cp -i memo memo.copy
cp: overwrite 'memo.copy'? y

to change a name of a file, we can use the mv command.

rename can also destroy files, as in the case of that if there exists a identical file with the name of which you are renaming to, it will overwrite
the earlier one. thus, we should use interactive mode on renaming conventions:

$ ls
memo
$ mv memo memo.0130
$ ls
memo.0130

to rename with interactive mode, we do the same process as copy and removing:

$ mv -i <filename> <new filename>

the lpr command is the way we use printers. In case of printers, there can occur that there is a pool of printers, and then we maybe only wish to access 
one or find the list of printers, we can find a list of printers by the virtue of the lpr -p command, as follows:

$ lpstat -p

we can instruct a printer to take an order in queue, by using the P command, as follows:

$ lpr -P

assume that would want to write out the file named report, then we could do the following:

$ lpr report

assume that we would like to send a queue to a printer called mailroom:

$ lpr -P mailroom report

we can see what jobs are on the print queue, by the virtue of the lpstat -o command or just the lpq:

$ lpq

or

$ lpstat -o

we would get the output of:

lp is ready and printing
Rank Owner 	Job Files 			Total Size
active max 	86   (standard input)  954061 bytes

the interesting thing, is that we can find the id of the printer, by the virtue of where it says job.
by this, we are able to remove the job, for instance, with lprm, as such:

$ lprm 86

one can send more than one file to a printer, for printing:

$ lpr -P laser1 0.5txt 108.txt 12.txt

We can use grep to search files with a regex:

$ cat memo //display the contents of the file memo

Helen:

blah blah credit blah blah

$ grep 'credit' memo
blah blah credit blah blah

of course, since grep utilizes regex, it can do a lot more, we'll get into that later

then we have the head command, that shows the 10 first lines of a file (10 is the default value):

$ head something

$ tail -5 

the - notation is to limit the amount of results we get

head can also refer to counts of blocks or characters

the default for tail is also 10, but can be configured.

To interuppt the running of a tail or heads command - we can do this by hitting the interuppt, which is CTRL+C

Also, if we wish to write the contents of a file to a file, we could do this to a logfile, as follows:

$ tail -f logfile

we can find the days from a file with days, for instance:

$ cat days

We can also call sort on a file, to display it in order:

$ sort days

this sorts the occurence of days in the file

if we call the -u, we generate a set from the file, with no duplicates.

if we run -n, we put a number to each row generated

the -u command, can run by the same factor as uniq

We can compare two files, and see what the difference is between them, by virtue of diff.

when we compare files, the outputs of which have +, occur only in the + file, - in the minus file, a space, means it occurs in both, on the same spot.

We can do comparison of files, according to this logic:

$ diff -u colors.1 colors.2

the output we would get from comparing these two files, would be as follows:

--- colors.1    2009-07-29 16:41:11.0000000000000 -0700
+++ colors.2 	2009-07-29 16:41:17.0000000000000 -0700
@@ -1,6 +1,5 @@
red
+blue
green
yellow
-pink
-purple
orange

to find out some info about data, we can simply write file and what it's called, to get some info about it, as follows:

$ file letter_e.bz2
letter_e.bz1: bzip2_compressed data, block size = 900k

$ file memo zach.jpg //info about two files, memo and zach
memo:  ASCII Text
zach.jpg  JPEG image data, ... resolution (DPI), 72 x 72


| (pipe) : Communication between processes
Basically, a pipe is the core way of processes communicating with each other, and religating information with each other.
For instance, we could retrieve the 4 first lines of a sort search, by virtue of a pipe - as follows:

$ sort months | head -4 //Sort months, pipe the input of months to the head command, and get the 4 first lines of head
Apr
Aug
Dec
Feb

we can also check to see how many files are in a directory:

$ ls | wc -w //wc stands for wordcount and -w is denotion of words, assuming no space
14

we can also repeat commands with echo:

$ ls
memo memo.0714 practice
$ echo Hi
Hi
$ echo This is a sentance.
This is a sentance.
$ echo star: *
star: memo memo.0714 practice
$

We can also use echo to redirect info and put it into something, with the > operator:

$ echo 'My new file.' > myfile
$ cat myfile
My new file.

we can display current date:

$ date
Wed Mar 18 17:12:20 PDT 2009

we can format the output

$ date + "%A %B %d"
Wednesday March 18

we can capture a session with script, by default, the file output is sent to a file called typescript

$ script
Script started, file is typescript
$ whoami
sam
$ ls -l /bin | head-5 //List all the current files of bin, send the 5th of the first ones to display
total 5024
-rwxr-xr-x 1 root root 		2928 Sep 21 21:42 archdetect
-rwxr-xr-x 1 root root 		1054 Apr 26 15:37 autopartition
-rwxr-xr-x 1 root root 		7168 Sep 21 19:18 autopartition-loop
-rwxr-xr-x 1 root root 	  701008 Aug 27 02:41 bash
$ exit
exit
Script done, file is typescript

we can view the file we created, with a editor, with cat, less, more etc.

$ cat typescript
Script started on Thu Sep 24 20:54:59 2009
$ whoami
sam
$ ls -l /bin | head-5
-rwxr-xr-x 1 root root 		2928 Sep 21 21:42 archdetect
-rwxr-xr-x 1 root root 		1054 Apr 26 15:37 autopartition
-rwxr-xr-x 1 root root 		7168 Sep 21 19:18 autopartition-loop
-rwxr-xr-x 1 root root 	  701008 Aug 27 02:41 bash
$ exit
exit

Script done on Thu Sep 24 20:55:29 2009

if you are going to edit the output, you can eliminate the ^M end characters, with fromdos or dos2unix.

todos and unix2dos, both convert Linux and Mac OS X files to Windows Format

We can do a number of operations on terms of what we want to do with conversion to files:

we can simply convert it:

$ todos memo.txt

or

$ unix2dos memo.txt

if we don't specify anything else, it will by default overwrite the file, provided it finds one by identical name.

if we use the -b extension, we make a backup with the extension .bak of the file.

if we use the -n option on it, we create a new file with that specific name and content.
We could illustrate these by the following:

if we were to convert a windows file to Linux, we could do as follows:

$ cat memo | tr -d '\r' > memo.txt //Concatenate the contents of memo, pipe it, and by using -d '\r' we delete all return statements, then put it into the memo.txt

We can compress files, in linux, as well. Let's begin first by taking a file that has an incredible about of many e's, and then see what we can do about that:

$ ls -l //the -l is long format, meaning we get more info about said files
-rw-rw-r--   1 sam sam 58400 Mar 1 22:31 letter_e //it tells us that the letter_e was 58400 bytes long.

we can compress it, with bzip, as follows:

$ bzip2 -v letter_e
letter_e: 11680.00:1. 0.001 bits/byte, 99.99% saved, 584000 in, 50 out. //We compressed it from 584000 bytes, to 50 bytes.

when we compressed a file, the extension name becomes bz2, as such:

$ ls -l
-rw-rw-r--  1 sam sam 50 Mar 1 22:31 letter_e.bz2

Generally, compression works best on things that use a lot of repetetive info, like text formats and akin - usually images are already compressed, so it does not
save a lot of space to compress it - Perhaps a rough 28% would be a good estimate on the space we would save on it.

When we compress files in Linux, the original file is lost, thus, if we wish to keep it, we can run the command -k to keep it, when compressing:

an example:

$ ls -l //find info about files
-rw-r--r-- 1 sam sam 33287 Mar  1 22:40 zach.jpg

$ bzip2 -v -k zach.jpg //compress the file, and keep the original
zach.jpg: 1.391: 1  5.749 bits/byte, 28.13% saved, 33287 in, 23922 out. 

$ ls -l
-rw-r--r-- 1  sam sam 23922 Mar   1 22:40 zach.jpg.brz2
-rw-r--r-- 1  sam sam 33287 Mar   1 22:40 zach.jpg

we can also unzip items, with bunzip, which uncompresses the file. Keep in mind, this removes the original file as well. (as in, the bzip)

couple of examples using bunzip

$ bunzip2  letter_e.bz2
$ ls -l
-rw-rw-r--   1 sam sam 584000 Mar  1 22:31 letter_e
$ bunzip2 zach.jpgbz2
$ ls -l
-rw-r--r--  1 sam sam 33287 Mar 22:40 zach.jpg

we can also simply choose to display the data from a compressed file, with bzcat, where we concatenate the compressed data and present it.
here, we have an example of where we bzcat on a compressed file and pipe the result to a header and choose the first 2 lines:

$ bzcat letter_e.bz2 | head -2
will give us 2 lines of e's, as that is what was in the file nad we choose to only present the first 2 lines.

we can also use a bzip2recover, which is when we wish to retrieve some data from corrupt sources.

We can also use the form of gzip, where we basically do the same thing as bzip, except it's an older and more outdated version.

we can use gzip, gunzip and zcat to do the same things as bzip and bunzip and bzcat.
NOTE: DO NOT USE GZIP, USE BZIP etc.

We can pack entire archives and directories, in form of a tar file.
in this exmaple, we create an archive from the files g b and d, make it verbose (more info) and then write/read to an info.
the sum we get, is thus:

$ ls -l g b d  //show file contents
-rw-r--r-- 	1 	zach other 1178 Aug 20 14:16 b
-rw-r--r--  1   zach zach  3783 Aug 20 14:17 d
-rw-r--r--  1   zach zach  1302 Aug 20 14:16 g

$ tar -cvf all.tar g b d //we call upon tar, using c(create), v(verbose (more info)), f(write/read from a file) and put it all in a tar file called all
g
b
d

$ ls -l all.tar
-rw-r--r--  1 zach 		zach 		9728 Aug 20 14:17 all.tar //NOTE, creating files with a tar, makes a small overhead, which is most noticable on small files

$ tar -tvf all.tar //Here we use t, which is display the info in a table; if we want to extract it, we woulda done x instead
-rw-r--r-- zach /zach 		1302 2009-08-20 14:16 g
-rw-r--r-- zach /other 		1178 2009-08-20 14:16 b
-rw-r--r-- zach /zach 		3783 2009-08-20 14:17 d

if you wish for the tar to do it's work silently, we simply omit the v part.

a tar file can have been compressed, to create a .tar.bz2 or .tbz - which means it's a tar that has been compressed.
To uncompress these, two steps must be taken, as is shown in the following example:

what follows is an exmaple of such an interaction:

$ ls -l mak*
-rw-r--r-- 1 sam sam 1564560 Mar 26 18.25 make-3.81.tar.gz //The * denotes any character, so basically we run a regex on the name with matching against any char, in other words
//any file that begins iwth the sentance of mak

$ gunzip mak* //unzip all the files that begin with mak
$ ls -l mak*
-rw-r--r-- 1 sam sam 6072320 Mar 26 18:25 make-3.81.tar

$ tar -xvf mak*
make-3.81/
make-3.81/config
make-3.81/config/dospaths.m4 
etc.

as we can see, if we use the xvf command (x for extract, v for verbose, f to read/write, in this case, read) - we extract the entire directory that we got from
the compositon of the tar file.

we can further modify the view of viewing files and directories by putting a -d to it, which only shows direcotires and compressed files, instead of all the files iwthin
the directories, as follows we illustrate this here:

$ ls -ld mak*
drwxr-xr-x 8 sam sam   4096 Mar 31  2006 make-3.81
-rw-r--r-- 1 sam sam  6072320 Mar 26 18.25 make-3.81.tar

$ ls -l make-3.81
total 2100
tons of files.

we see that through the process of uncompressing the make-3.81 file, we made a directory and put all the files into it.
And we see then, that when we do a search on the current working directory and only showing directories and compressed files, by -ld mak*,
we find that we have 2 files that begin with mak, the tar and the working directory that is make-3.81

NOTE: Using the -x option, may extract a lot of files, thus, it can be smart to make a new directory to deposit all the things into, first, using the 
mkdir command. We can also view the files, within the dir, by virtue of -t

when we run the -t command on a tar file, we can conclude how it's going ot unpack and where it's going to put everything.

NOTE: as per always, when you use -x, you overwrite ; Make certain that you either run it in -i (interactive mode) or that you double check the directory
you are going to extract it into - otherwise you may cause serious damage.

We could also theoretically combine gunzip and tar commands to make a one line, by virtue of redirecting information with a pipe:

$ gunzip -c make-3.81.tar.gz | tar -xvf - //c is for create, gunzip is the unzip, the | is the pipe, and we pipe the into to the tar, then use the command of -xvf 
(x for extract, v for verbose, f for write/reading), where the last - character is for retrieving standard output

we could also condense the baove line, of unzipping and making files and redirecting it, by putting in the -z option on the tar, which is shorthand for calling
gunzip, as follows:

$ tar -xvzf make-3.81.tar.gz (x for extract), (v for verbose), (z for gunzip or compress), (f for reading/writing)
in the above line, we have made the same as the lengthier version with a pipe, simply by calling the z command on it as well

There are a number of commands that helps us locate things, as follows:

whereis and slocate, can allow us to locate things, slocate is the seach function to find them on the local system.

NOTE: WHen we search for things, it searches in the specified search path - and it is possibly that it won't find or finds incorrect ones,
based on this information.

We can change the default search path for finding files.

Which let's us find files by also specifying a full path name, and we can find files by virtue of name, as follows:

$ which tar //We search for the location of a tar
/bin/tar 

the reason we would want to use which, is for instance if we have abnormal behaviour of a tar or the likes, which it is then possible that we are using
the local version of a tar - which could imply error in terms of the tar.

Thus we would perhaps want to replace it and make a new one, or just find a new one.

to find the full path of a file, we can use the whereis command, as is illustrated:

$ whereis tar
tar: /bin/tar /usr/include/tar.h /usr/share/man/manual/tar.1.gz //here, it finds 3 tars, 1 tar utility file, a tar header file and the manual page of tar.

NOTE: which only finds the first instance of a file, thus, the one you would run.

NOTE: Both whereis and which only finds files on the disk, i.e it does not find builtin versions - and reports of errornous verisons.
An example of this would be when we try to find the location of echo.

it is a builtin utility, but it also exists on the disk. Thus, we owuld do best in running the builtin command in bash, to find if it's a builtin utility that
we are using.

we can also use the slocate/locate (slocate is for safe locate) to find things on the local system, for instance, we could do the following:

$ slocate motd

which locates all files related to motd, where it finds it, also giving us the root path of where it is located.

What if we wanted to find what user is currently utilizing the system?

if we want to find full info about users on another network, as in, we are connected to a network on our local place, we can use the command of finger
to find all the info we need about users on that specific network

But if we want to search on our local network, we can use the who command:

$ who am i  //We ask for inf oabout the current user
max     tty2    	2009-07-25 16:42 

$ who //we ask for all the users who are logged on
sam 	tty4 		2009-07-25 17:18
max 	tty2 		2009-07-25 16:42
zach 	tt1 		2009-07-25 16:39
max 	pts/4 		2009-07-25 17:27 (coffee)

we could further manipulate the information about who, by redirecting the information to a lesser or a more input, to scroll through it, as follows:

$ cat who | lesser | lpr //This takes the who information, concatanates it, puts it in lesser format, and then pipes it further to lpr to print it

if we want more complete info about users, in terms of connections, time, etc. etc., we can use the command finger, as such:

$ finger
Login 		Name 		tty 		idle 		login time 	Office ...
max 		max wild    *tty2 					Jul 25 16:42
max 		max wild    pts4           3 		Jul 25 17:27 (coffee)
sam 		sam the great *tty4       29 		Jul 25 17:18 
zach 		zach brill  *tty1        1:07 		Jul 25 16:39

if we wanted to get a lot more specific info about a use, we could do:

$ finger max
Login : max 				Name : max wild
Directory: /home/max 		Shell: /bin/tcsh
On since Fri jul 25 16:42 (PDT) on tty2 (messages off)
on since Fri jul 25 17:27 (PDT) on pts/4 from coffee
	3 minutes 7 seconds idle
new mail recived Sat jul 25 17:16 2009 (pdt)
	Unread since Sat jul 25 16:44 2009 (pdt)
Plan:
i will be at conference in hawaii all next week.
if you need to see me, contact Zach Brill, 	x1693

NOTE: files that are named by the convention of .plan, are normally not listed by Is and are called hidden filenames.

if max had not been logged in, only his user info, last time he logged in, and last time he read his email and his plan would be displayed.

NOTE: On systems where security is a risk, finger can be disabled, because it poses a security risk to attackers being able to access vital info.

we can also use partial information to get info about users, with finger, as such:

$ finger HELEN
Login: hls 					Name: Helen simpsons

$ finger simpson
Login: hls 					Name: Helen simpsons

if we wish to know about all the users who are logged into the system, at that current point of time, we can use the w command

$ w

w reveals a lot of info, user name, Connection, if there has been external connections, last time of login, Idle time, how much CPU is used, how much cpu has been used on current
task, and what they are currently running.

What follows, is a collection of what each command reveals:

w: Username, terminal-line-identification (tty), Login time, idle time, Program user is executing, CPU time used, System uptiem and avg load

who: username, terminal-line-identification (tty), Login date and time

finger: Username, TTY, Login date and time, Idle time, Location the user is logged in from, Full info (name, password etc. etc.), user-supplied vanity info

We can communicate with other users, by the virtue of write:

$ write max
Hi max, are you there? o

Message from max@bravo.example.com on pts/0 at 16:23...
yes zach, i'm here. o

We can either use username, or username and device, if we wish to specify what system the person is located at.
This is useful if the person is connected to several different systems or logged into several different systems.

to write o, is convention for "over", to quit a conversation, convention is "oo", over and out.

to send a EOF to the other user (i.e, terminate communication), one must use ctrl+D.

Then you close communication, and the other user must do the same to quit.

If a banner is clogging up your view of the communications view, press CTRL+L or Ctrl+R to refresh the screen.

By default messages are blocked - to allow them, you have to write the following:

$ mesg y

likewise, if we wish to disable messaging, we can write $ mesg n

if we try to write/recieve from a person who has their messaging disabled, we will get a prompt telling us that it's not doable.

In the Linux OS, when we speak about files, we talk about directories.

By default, a user has one directory. You can add subdirectories to this directory and so forth, to create a net of directories.

Just as in windows, files in a directory is unique by virtue of name.

Assume that we would want to display how many characters a File name can contain, then we could do as follows:

$ stat -f /home | grep -i name //i is for ignore case in terms of grep, and the -f tells us of filesystem status instead of file status

Some file names are case-sensitive in terms of linux, albeit most are not, thus, you different casings results in different files, technically.

In case of space in a filename, we have two options, either we use an escape character to escape it, or we encase the entire name in quotations:

$ lpr my\ file
$ lpr "my file"

In most cases, when we talk about file extensions (as in .txt and what not), in terms of linux, we usually don't have to provide one, as it defaults
to one that is appropiate.

In Linux, we are allowed to use several dots in the notations of the extentions, as in "somefile.4.10.11", etc. this is a valid practice in terms of linux

In terms of idetifying what type the file is, Linux usually uses type codes and creator codes.

What follows, are some filename extensions in linux:

compute.c  		A c programming language source file
compute.o 		The object code file for compute.c
compute 		the executable for compute.c
.txt 			a text file
.pdf 			a pdf, viewed with xpdf or kpdf in a gui
.ps 			a postscript, viewed with gs or kdpf in a gui
.Z 				a file compressed with compress (zipped) : use gunzip or uncompress to uncompress it
.tgz/tar.gz 	a tar file that has been compressed with gzip : use gunzip or uncompress to uncompress it
.gz 			a file that has been compressed with gzip : view with zcat or decompress with gunzip.
.bz2 			a file compressed with bzip2 : view with bzcat or decompress with bunzip2
.html 			webbrowser file
.jpg, gif etc.  Graphical info

There are also hidden filenames, they are called so because they are usually not displayed.
To display them, and others, we can just write : ls -a

to find out our current working direcotry, we can simply write: pwd (print working directory)

To find out our home directory, we can just the pwd command when we have logged in, as we start in the home directory

When we start up our program, we run the standard configuration files that occur at start. These define the attributes of the following:
What kind of terminal you are using, executes the stty (set terminal) utility to establish the erase and line kill keys.

You can also define a shell startup file that runs a specific script whenever you start the program.

In terms of paths, we speak about absolute paths and relative paths.

An absolute path, is when we are at the point of a root path - which means that we absolutely define the full path to a place.
Otherwise, we call it a simple path or filename/basename.

If we give the absolute pathname, it does not matter what directory we do it from.
An example of listing all the files in an absolute directory, follows:

$ pwd (print working directory)
/home/sam
$ ls /usr/bin
7z 					kwin
7za					kwin_killer_helper
822-date 			kwin_rules_dialog

Also, we can use the ~ character, to use it for shorthand in terms of writing home directory:

$ less ~/.bashrc //This allows us to examine the bashrc file of our own system.

assume that we have the rights to view someone elses system, then we could do it as follows:

$ less ~sam/.bashrc //view sams bashrc

Any directory that does not start with the root or home directory (~), is a relative path.

If you refer a file in the working directory, we just need to state the simple filename.

Where as of, if it is in another directory, we need to specify filepath.

When we wish to create a directory, we use the mkdir command.
When we switch current directory, we used the cd command.
To remove a directory, we use rmdir.

what follows, is an example of how to create directories, using mkdir:

$ pwd
/home/max
$ ls
demo names temp
$ mkdir litterature
$ ls
demo litterature names temp
$ ls -F  //the -F command in terms of ls, shows / after directories, and a * after each executable file
demo litterature/ names temp
$ ls literature
$ //empty, because literature was just made

when we create directories, we can use either relative paths or absolute paths

$ pwd //Current working directory
/home/max
$ mkdir literature/promo

$ mkdir /home/max/literature/promo

by using the -p command, we can create multiple direcotires at once:

$ pwd //current working directory
/home/max
$ ls
demo names temp
$ mkdir -p literature/promo

or we could do

$ mkdir -p /home/max/literature/promo

one uses current working directory, the other goes by absolute path

we can change directories with the cd command

either we do this one step at a time, or we do it by absolute path as follows:

$ cd /home/max/literature //Absolute path
$ pwd
/home/max/literature

$ cd
$ pwd
/home/max
$ cd literature
$ pwd
/home/max/literature

keep in mind, to actually have a cd take effect, we must provide pwd, otherwise it resorts to defaulting to the home default path.

Whenever we do a mkdir, the mkdir generates a . and .. entry in the created dir. These are references of the parent dir (..) and working dir (.)

What follows is an example of how to navigate with .. :

$ pwd
/home/max/literature
$ ls ..
demo literature names temp
$ cp memoA .. //copy memoA to the parent direcotry
$ ls ..
demo literature memoA names temp

if we change the current working dir to promo, we could call a relative path to edit with vim in the home directory, as follows:

$ cd promo
$ vim ../../names

rmdir removes a directory
You cannot delete a directory that has files in it, the files have to be removed first, then

what follows is an example of how to remove a directory

$ rmdir /home/max/literature/promo //assuming the promo dir is empty, it will be removed.

We can do a recursive function call on rmdir, which recursively removes the current working dirr and files in in it, one by one, with -r, as follows:

rm -r //Note: Using this command with * can wipe your entire system. do not use.

We can use the touch command to create an empty file, as follows:

$ cd
$ pwd 
/home/max
$ touch letter //create an empty file, called letter

we can copy letter to the promo dir, as follows:

$ cp letter literature/promo/letter.0610

we can then use the vim editor, to edit the file, as follows:

$ vim literature/promo/letter.0610

or we can just direct cd to promo first, to edit it there 

$ cd literature/promo
$ pwd
/home/max/literature/promo
$ vim letter.0610

if we wish to change working directory to the parent directory, we could do as follows:

$ cd ..
$ pwd
/home/max/literature

if we wish to move files, we have the mv command:

$ mv names temp literature //move names and temp to literature

the cp utility does the same thing, except it copies existing file lists into the destination directory

the -r option (recursive call) works on cp as well, which means they move entire structures (by virtue of recursive calls)

if we try to move to a new directory name, and it does not exist, the current working directory must have only one directory,
which mv renames into the new directory and moves the stuff into

What follows, are some directories that may or may not be present in the linux system, and what their function is:

/root : always there, ancestor of all files in the filesystem

/bin : Essential command binaries Holds the files needed to bring the system up and run it when it first comes up in single-user or recovery mode.

/boot : static files of the boot loader. Contains all files needed to boot the system.

/dev : Device files Contains all files that represent peripheral devices, such as disk drives, terminals, and printers. Previously this directory was filled with all
possible devices. The udev utility provides a dynamic device directory that enables /dev to contain only devices that are present on the system.

/ctc : Machine-local system configuration files Holds admin, Config and other system files. One of the most important is ctc/passwd, which contains a list of all
users who have permission to use the system. Mac OS X uses Open Directory in place of ctc/passwd

/ctc/opt : Configuration files for add-on software packages kept in /opt

/ctc/X11 : Machine-local configuration files for the X window system

/home : user home directories Each user's home directory is typically one of many subdirectories of the /home directory. As an example, assuming that 
user's directories are under /home, the absolute pathname of Zach's home directory is /home/zach. One some systems the user's directories may not be 
found under /home but instead might be spread among other directories such as /inhouse and /clients. Under Mac OS X user home directories are typically located in /Users.

/lib : Shared libraries

/lib/modules : Loadable kernel modules

/mnt Mount point for temporarily mounting filesystems

/opt Add-on (optional) for software packages

/proc Kernald and process information virtual filesystem

/root Home directory for the root account

/sbin Essential system binaries Utlities used for system admin are stored in /sbin and /user/sbin. The /sbin directory includes utilities needed during the booting process,
and /usr/sbin holds utilities used after the system is up and running. In older versions of Linux, many system admins utils were scattered through several directories
that often included other system files(/etc, /usr/bin, /usr/adm, /usr/include)

/sys Device psuedofilesystem

/tmp Temporary files

/Users User home directories Under Mac OS X, each user's home directory is typically one of many subdirectories of the /Users directory. Linux typically stores
home directories in /home.

/usr Second major hierarchy Traditionally includes subdirectories that contain information used by the system. files in /usr subdirectories do not change often and may be
shared by several systems.

/usr/bin Most user commands Contains standard Linux/OS X utility programs - that is, binaries that are not needed in single-use or recovery mode.

/usr/games Games and educational programs

/usr/include Header files used by C programs

/usr/lib Libraries

/usr/local Local hierharchy Holds locally important files and directories that are added to the system. Subdirecotries can include bin, games, include, lib, sbin, share and
stc.

/usr/bin Nonvital system admin binaries 

/usr/share Architecture-independent data Subdirectories can include dict, doc, games, info, locale, man, misc, terminfo and zoneinfo.

/usr/share/doc Documentation

/usr/share/info GNU info system's primary directory

/usr/share/man Online manuals

/usr/src Soruce Code

/var Variable data Files with contents that vary as the system runs are kept in subdirectories under /var. The most common examples are temporary files , system log files
, spooled files, and user mailbox files. Subdirecotires can include cachce, lib, lock, log, mail, opt, run, spool, tmp and yp. Older versions of linux scattered such
files through several subdirs of /usr (/usr/adm, /usr/mail, /usr/spool, /usr/tmp)

/var/log Log Files contains lastlog (a record of the last login by each user), messages (system messages from syslogd), and wtmp (a record of all logins/logouts),
amongst other log files

/var/spool Spooled application data Contains anacron, at , cron, lpd, mail, mqueue, samba, and other dirs.

Access Permissions:

There are three levels of permissions in relativity to a file : owner, user by group (related) and others.

There are three levels of attempted permissions : read, write and execute.

We can contain permisions in two ways, one is to use a ACL security list, which allows greater security application on users and who gains usage to the different
files, whilst the other is to use the normal security protocols of Linux.

When we write the ls -l to inspect a file, we can find out things about it, such as security level and other things.

The long list that we usually get before a file that we get from the output of ls -l on a file, is where we get this info.

The following example showcases this:

$ ls -l letter.0610 check_spell
-rwxr-xr-x 1 max pubs 852 Jul  31  13:47 check_spell
-rw-r--r-- 1 max pubs 3355 Jun 22 12:44  letter.0610

Let's break this down, with the -rwxr-xr-x line:

-rwxr-xr-x
^
Type of file. - indicates that it is a normal file. If we were to have a d instead, it would be directory file. There are other types as well, and we will cover thoose later.

The next three characters, in our case, rwx, are in terms of owner of the file.

r indicates read permissions, w indicates write permissions, x indicates execute permissions.

If there is a - in respective position, then respective right is resigned from respective level of user.
So, if there is a - in thoose lines for the owner of the file, the owner does not have the right to do that specific right/rights.

The next three, is the permission levels for users associated by group. This means, that users who are associated with the group to access the file,
have their permissions restricted by the virtue of that it gates them.

In our case, it is r - x , which means we have the right to read and execute it, but not write to it, as a user.

And the final three, present the rest, i.e, everyone.

Since it's r - x on that as well, others and group associated people, have the same right to the file. in this specific case.

Depending on the context, it can be appropiate to grant execute rights or not grant it. As in the case of a letter, we merely discuss the idea of reading, as being
the most relevant (or writing).

Where as of executing makes no sense, this is instead served for scripts or binary files that we have use of executing.

NOTE: Root privleges is higher than all privleges, thus, if a user has root priveleges, they circumvent any normal means of access levels.

If we were to want to change permission levels, we could use the chmod command, as follows:

$ ls -l letter.0610
-rw-------- 1 max pubs 3355 Jun 22 12:44 letter.0610 //here, we have a file that only the owner can read and write from. 
$ chmod a+rw letter.0610 //Give access level of read and write, to ALL users, on this file. a is for all, + means grant privleges.
$ ls -l letter.0610
-rw-rw-rw 1 max pubs 3355 Jun 12:44 letter.0610 //we can now see, that all levels, have been granted reading and writing levels.

Shell script files need to have reading (to be run) and execution (to run on cmdline) rights to be able to be run, where as of binary only need execution.

$ ls -l check_spell
-rwxr-xr-x 1 max pubs 852 Jul 31 13:47 check_spell
$ chmod o-rx check_spell //remove rights of reading and execution, for the group of others, on the check_spell shell script
-rwxr-x--- 1 man pubs 852 Jul 31 13:47 check_spell

The user reference in terms of granting/removing capabilities, is as follows:

a (all), o (other), g (group), u (user, although user can be owner. Unpredictable to tell who this is)

we can also chain commands in terms of how we refer to users and others, such as making a statement of the following:

$ chmod og-rwx check_spell //removes all rights for others and all group associated people on check_spell

we can also use numbers in terms of allocating privleges, as is shown below:

$ chmod 600 letter.0610 //This removes all rights, except for the owner,who retains write and read permission (1 is Execute, 4 is read, 2 is write) (thus 4+2 = 6)
$ ls -l letter.0610 
-rw------- 1 max pubs 3355 Jun 22 12:44 letter.0610

the next one is a bit different:

$ chmod 755 check_spell //Grant all privleges to Owner, give Group associated and others Execue and read rights
$ ls -l letter.0610
-rwxr-xr-x 1 max pubs 3355 Jun 22 12:44 letter.0610

We can elevate permissions by projecting a users or groups permissons on to a person, by virtue of setuid or setgid
When a process then runs whatever is given the rights of setuid or setgid, respective user is elevate to the level of owner of respective file.

Note: This kind of projection applies to root privlege file as well, thus, we should not elevate people to root level.
One of the few nessecary setuid files we have, is passwd.

what follows, is an example of what can happen, when we run with root privleges and elevate people left and right:

$ ls -l myprog*
-rwxr-xr-x 1 root pubs 19704 Jul 31 14:30 myprog1
-rwxr-xr-x 1 root pubs 19704 Jul 31 14:30 myprog2

# chmod u+s myprog1 //When we have the s level privlege, it is the same as a projected security level access. So if a root user did this, they have root access on that point.
# chmod g+s myprog2 //this is projecting the level of security access unto respective group : u being owner, g being associated group of people, they both gain root
//since root projected it.

$ ls -l myprog*
-rwsr-xr-x 1 root pubs 19704 Jul 31 14:30 myprog1 //root privlege on owners execution part, since elevation through projection was done from root privlege.
-rwxr-sr-x 1 root pubs 19704 Jul 31 14:30 myprog2 //root privlege on group related execution part, since elevation through projection was done from root privlege.

The next example, is projection through elevation in terms of numeric characters instead.

$ ls -l myprog*
-rwxr-xr-x 1 root pubs 19704 Jul 31 14:30 myprog1
-rwxr-xr-x 1 root pubs 19704 Jul 31 14:30 myprog2

# chmod 4755 myprog1 //When we do numeric assignment of privlege, we set user in the first digit. 4 is elevate the Owner/User, 2 is Group assocaited, 1 is setting the sticky bit.
# chmod 2755 myprog2 //elevate group associated to root level (the 2), the rest is privleges levels (all on Owner, read and execute on associated group/others)

$ ls -l myprog*
-rwsr-xr-x 1 root pubs 19704 Jul 31 14:30 myprog1 //Showcasing how root privleges were projected unto a owner of file, through numeric presentation instead of numerical
-rwxr-sr-x 1 root pubs 19704 Jul 31 14:30 myprog2 

Note: NEVER, EVER, EVER give shell scripts Setuid permissions. This is a severe security breach.

We can apply the same level of logic to directories, except the x, excution, is instead leveled to be cd:ing into the directory and examining things within.

to view the permission we have related to a directory, we run the ls command with the appendix of ld:

$ who am i //confirm who is current user
zach 		pts/7 		aug 21 10:02
$ ls -ld /home/max/info
drwx-----x 		2 max pubs 512 aug 21 09:31 /home/max/info //The d implies it's a directory. 
$ ls -l /home/max/info //Since we do not have reading rights, we get permission denied when trying to access the file.
ls: /home/max/info: permission denied 

we can give people permission to read the directory, as is illustrated by max doing here:

$ chmod o+r (grant the group of others, reading permissions) /home/max/info

we can further illustrate limitations of security:

$ ls -ld /home/max/info //l stands for long, d for directory, which gives us the long info version about the directory, in other words, access lvls
drwx---r-x 		2 max pubs 512 Aug 21 09:31 /home/max/info
$ ls -l /home/max/info
total 8
-rw------  		1 max pubs 34 Aug 21 09:31 financial
-rw-r--r- 		1 max pubs 30 Aug 21 09:32 notes
$ cat /home/max/info/financial
cat: financial: Permission denied //As we can observe from the ls -l command on info, that we do not have any rights in regards to financial file
$ touch /home/max/info/newfile  	//We tried to create a new file, but since we do not have writing permissions in this directory, we cannot do that
touch: cannot touch 'home/max/info/newfile': Permission denied

ACLs is a set of more fine-grained security that allows us to restraint certain types of interactions in specific ways.
There is a number of issues with ACLs, tho.

It reduces performance, when we move them or copy files that are affected by ACLs, they can be lost - losing the meaning, also, you cannot copy
ACL affected systems into systems that do not support ACL.

ACL consists of set of rules, in who can access what, and how. The ACL is generally divided into two sets:

access rules and default rules.

An access rule, defines restrictions in terms of a single file or directory. 

A default ACL rule, however, defines an entire directory, and extends to anything that is not explicit exempt from the ACL ruleset by virtue of having a explicit ACL applied to it.

when used with -p (preserve) or -a (archive), cp preserves ACLs when it copies files. The mv utility also preserves ACLs. When you use cp with -p or -a option and it is not
able to copy ACL, the operations will halt and give an error.

Other utilities, like tar, cpio and dump, does not preserve ACL rulesets. However, you can use the -a command with cp to copy directory hierarchies, including ACLs.
You can never copy ACLs to a filesystem that does not support ACLs or to a filesystem that does not have ACL turned on.

So, how do we enable ACLs?

The following way, applies only to Linux terms of applying ACLs.

Linux only officialy supports ACL on ext2, ext3 and ext4 filesystems, although informal support for the ACL is available on other systems as well.
to use an ACL on a ext filesystem, you must mount the device with the acl option (no_acl is the default option), for example, if you want
to mount the device represented by /home so you can use ACLs on files in /home, you can add ACL to it's options list in /etc/fstab by the following way:

$ grep home /etc/fstab //find the fstab directory located in etc.

After changing the fstab, you need to remount the /home before you can use ACLs. If no one else is using the system, you can unmount it and mount it again
(working with root privleges) as long as the working directory is not in the /home hierarchy. Alternatively you can use the remount option to mount to
remount /home while the devise is in use:

# mount -v -o remount /home //with root privlages, remount home
/dev/hda3 on /home type ext3 (rw, acl)

The following rules, still only apply to the linux version:

we have both the setfacl and getfacl to set/get a files ACL mode. 
if we use the getfacl on a file that does not have ACL on, we technically get the same info as ls -l (list long command)
except in a different format:

$ ls -l report
-rw-r--r--  1 max max 9537 Jan 12 23:17 report

$ getfacl report
# file: report
# owner: max
# group: max
user::rw-
group::r--
other::r--

if we wish to remove the first line, which linux treats as the header, we can use the omit command:

$ getfacl --omit-header report
user::rw-
group::r--
other::r--

if we wish to add constraints to a ACL ruleset, we have to use the modify command, in tandem with setfacl, as follows:

setfacl --modify ugo:name:permissions:file-list //Note, ugo is basically just the shorthand notations of owner(u), group(g), others(o)
name is the name of the user/group of people that the permissions account for, permissions is permisisons in symbolic or aboslute format (so either number(1 execute,
2 read, 4 write)) format or rwx)

If we were to talk about the specific group of people, that is others, we must omit the name part.

setfacl only accepts a parameter of one length, so if we do the absolute path of talking about constraints, we give 1 number, to encapsulate the total,
whilst the letters are still in row, albeit in the format of rwx

for example, both of the following lines, grants Sam permission to read and write on that specific file:

$ setfacl --modify u:sam:rw- report //read and write permissions
 
or

$ setfacl -- modify u:sam:6 report //read and write permissions

$ getfacl report //just to make a point
# file: report
# owner: max
# group: max
user::rw- //When we do not have an associated value, as in a user named something or akin, we leave the middle space empty with no value to show for.
user:sam:rw- //Here we do talk about a concrete user, so it's put in the middle
group::r--
mask::r--
other::r--

when an file has an ACL, it displays a + after the permissions, even if the ACL ruleset is empty

$ ls -l report
-rw-rw-r--+  1 	max max 9537 jan 12 23:17 report

In ACL there is something we call mask. Mask is a further restraint, that can be set, after a ACL has been declared for a file.
If you modify this, you can make a file or directory have a "Effective" security level - meaning that even if a users specified security
level is higher than that of the specified one, he will still not get through, as the "Effective" one, can lock him out.

We illustrate this in the following example:

$ setfacl -m mask::r-- report

$ getfacl report
# file: report
# owner: max
#group: max
user::rw-
user:sam:rw-
group::r--
mask::r-- //NOTE: Since the mask level is set to R only, sams rights of rw does not matter, since the is the effective security levle put for everyone in regards to this file
other::r--
#effective:r--

setfacl can modify more than one ACL rule at a time, as we will see in the following example:

$ setfacl -m u:sam:r--,u:zach:rw- report //we can chain rights with the modify statement of -m on users etc.
$ getfacl --omit-header report
user::rw-
user:sam:r--
user:zach:rw-
group::r--
mask::rw-
other::r--

the -x option in this context, removes the rights for a specific user:

$ setfacl -x u:sam report

$ getfacl --omit-header report
user::rw-
user:zach:rw-
group::r--
mask::rw-
other::r--

when we use the -x statement in the Acl context, we must not specify premisisons, but simply specify the user/group/other and name.
if we provide -b and the file name only, in a ASCL context, we remove all the associated ASCL upon that specific file or directory.

We can also set default rules and interaction sets for directories, as we will see in the following example:

$ ls -ld dir
drwx------ 2 max max 4096 Feb 12 23:15 dir
$ getfacl dir
# file: dir
# owner: max
# group: max
user::rwx
group::---
other::---

$ setfacl -d -m g:pubs:r-x,g:admin:rwx dir //here, we set the FACL constraint to be so that for this directory, we modify the rights of the group pubs, to be r and cd, but not write, and for the group of admin, we give all the rights to

when we check for permissions and rulesets of ACLs, we can trace the priority list by ordering in the hierarchy list of which the rules exist, as follows:

$ ls -ld dir
drwx------+ 2 max max 4096 feb 12 23:15 dir
$ getfacl dir
#file: dir
#owner: max
#group: max
user::rwx
group::--
other::--
default:user::rwx
default:group:---
default:group:pubs:r-x
default:group:admin:rwx
default:mask::rwx
default:other::---

as we can clearly see, we can specify the details of security to an extreme, if we so please, making ACL filters, making groups, making default privleges and the likes.

the default rules pertain to thoose who do not have explicit ACL rules.
We could also, of course, do rulesets for the directory itself.

if we were to write a file into a ACL protected file, we have to keep in mind that security levels are projected unto the file that is written as well,
which leads to having the situation where we can end up having a effective mask overwriting the ACL security levels, as is shown in the following example:

$ cd dir
$ touch new
$ ls -l new
-rw-rw------+ 1 max max 0 feb 13 00:39 new
$ getfacl --omit new
user::rw-
group::---
group:pubs:r-x
group:admin:rwx
mask::rw-
other::---
#effective:r--
#effective:rw-

if we were to change the traditional rights of the owner and group of this directory, to rwx, then the ruleset would reflect this
in that the default updates to be rwx and default becomes rwx

$ chmod 770 new //apply read/write/"execute" rights to u and g but not u
$ ls -l new
-rwxrwx---+  1  max max  0 feb 13 00:39 new
$ getfacl --omit new
user::rwx
group:---
group:pubs:r-x
group:admin:rwx
mask::rwx
other::---

by having changed the default ruleset, the ruleset is updated and the effective level is put to rwx, as wanted

Links, in linux, is a pointer to a file and it's memory adress.
generally what links does, is allow you to share files across systems, assuming you have declared access rights correctly in terms of who is allowed to rwx and so forth.

Generally, there are two form of links, hard links and soft links, albeit soft links are more modern than hard links, hard links are more of a legacy thing.
A hard link, appears as a file, that can be created from being in the same system as the destionation file.

an example of how to establish a hard link, is as follows:

$ pwd
$ ln draft /home/max/letter //create a hard link between draft and letter

the link will be created with the same name as the destination files name, so it ends up being called letter, but is put in the max directory.
even if it appears in max's directory, Zach is considered owner of the file, as he created the link.

do not mistake a link for being a copy of the file, as that is not what it is, it is a link.

to illustrate this point, let us use cp to really cover it: (copy)

$ cat file_a
This is file A.
$ ln file_a file_b
$ cat file_b 
this is file A.
$ vim file_b //we edit the name of the target file with vim, i guess?
...
$ cat file_b
this is file b after the change.
$ cat file_a 
this is file b after the change.

if we try cp instead, we will see what occurs:

$ cat file_c
This is file C.
$ cp file_c file_d
$ cat file_d
This is file C.
$ vim file_d
...
$ cat file_d
This is file D after the change.
$ cat file_c
This is file C.

the number that we keep seeing in the output of ls -l, where it's 1 or 2 etc. is the amount of links to that item.

A neat method to compare files and their states, is as follows:

$ ls -l file_a file_b file_c file_d
           v--------------------------------------- This here number, is the amount of links active to the said item
-rw-r--r-- 2 max pubs 33  May  24 10:52 file_a
-rw-r--r-- 2 max pubs 33  May  24 10:53 file_b
-rw-r--r-- 1 max pubs 16  May  24 10:55 file_c
-rw-r--r-- 1 max pubs 33  May  24 10:57 file_d

ls does not specificly tell us, if they are linked or not. But if we use ls with the -i command, we can find that 
we figure out the inode number, which is much akin to the instance of a method or a object

if two files have different inode numbers, they are different files, where as of if they have the same, they are the same.

What follows, is an exmaple of such:

$ ls -i file_a file_b file_c file_d
3534 file_a 	3534 file_b 	5800 file_c 	7328 file_d

note: as long as a file has a link, you can access it, but if you remove all, you are unable to access it.
Which also means, that if it has more than 1 link, we can sever one, and still use links to the item.

Symbolic links, is simply a pointer to the hard pointer of the File.
The advantage of symbolic links, is that you can have symbolic links on directories, to which hard pointers are not
able to reach or available for.

Another difference, is that the hardpointer must point to a file that is within the same file system, due to referencing.
But, a soft pointer, or a indirect pointer, is not dependant upon being part of the system, thus it circumvents this too.

Another big difference, is that hard pointer, cannot point to a object that gets removed - because it keeps pointing to that
object, and won't understand when a new object of the exact same type or the same file is recreated.

So, basically, hard pointers are literally just that - hard pointers-  where as of soft pointers, are a "kind of" pointer, a indirect
pointer - to which we guess what we are talking about ; cause we understand it indirectly.

The disadvantage of the Symbolic links, is their signifying status - as it is not as prevelant as hard links. (Expectedly so)

So, how do we create symbolic links? What follows, are some examples:

$ ln --symbolic /home/max/sum/tmp/3
$ ls -l /home/max/sum/tmp/s3
-rw-rw-r-- 	1 max max 38 Jun 12 09:51 /home/max/sum
lrwxrwxrwx  1 max max 14 Jun 12 09:52 /tmp/s3 -> home/max/sum
$ cat /tmp/s3
This is sum.

a Symbolic link is smaller than that of the file, and has a lesser status.

NOTE: WHen we do symbolic links, ALWAYS use a absolute pathname.

An example of the literacy of the soft pointer:

$ pwd //current working directory
/home/max
$ ln --symbolic sum/tmp/s4
$ ls -l sum /tmp/s4
lrwxrwxrwx 		1 max 	max 		3 jun 12 10:13 /tmp/s4 -> sum
-rw-rw-r-- 		1 max 	max 	   38 jun 12 09:51 sum
$ cat /tmp/s4
cat: /tmp/s4 no such file or directory

Thus, we hsould always use an absolute path, in terms of links. Regardless.

If we try to do a CD call with a symbolic link, the results might be confusing, if we do not grasp
the fact that we are operating on a symbolic link. Due to it will print the name of the link, instead.
To counteract this, we can use the built in /bin/pwd to get a assured path printed

$ ln -s /home/max/grades /tmp/grades.old
$ pwd
/home/max

$ cd /tmp/grades.old
$ pwd
/tmp/grades.old
$ /bin/pwd
/home/max/grades

If we were to change back the directory to the parent directory, we would end up in the directory holding
the symbolic link:

$ cd ..
$ pwd
/tmp
$ bin/pwd
/tmp

Under Mac OS X /tmp is a symbolic link for /private/tmp - if we give the cd .. command the pwd would be /private/tmp 

What if we wish to remove a link, then? Well, we could use the command of rm to remove a link.
Deletion in Linux works so that, whenever the last remaining hard link to an object is removed, then the program
releases the resources which was allocated to that file. In terms of retrieving a file that has been deleted,
that is actually very hard and would require the time and effort of an hacker to be done.

What follows is an example of a file that has a symbolic link remaining but had all of it's hard links removed,
so it's no longer able to be accessed:

$ ls -l sum
-rw-r--r-- 1 max pubs 981 May 24 11:05 sum
$ ln -s sum total
$ rm sum
$ cat total
cat: total: No such file or directory
$ ls -l total
lrwxrwxrwx 1 max pubs 6 May 24 11:09 total -> sum //Here the total remains as a symbolic link to a file that no longer exists. When we delete files, we must clean up
//things and remove the remaining stuff as well to assure that it's been all done properly.

we can remove it with rm:

$ rm total

What follows, is the chapter of Shells, where we will talk about interactions of Bash and the programming shell, how it grasps code and how it executes it.
Generally, the shell, executes command lines that we provide it. and generally the structure of giving a command, is as follows:

command [arg1] [arg2] ... [argn] RETURN

Usually, when we speak about optional arguments, we include the []'s, where as of when it's a special command, we include one or two -'s instead

When we get an error message, it's usually called a usage message.

in termonology of the Shell language, we find that every sequence of nonblank characters is called a token or a word. An argument is a token, a name is a token etc.

for instance, what follow is an example of copying a file from the temp directory to the tempcopy directory:

$ cp temp tempcopy

for each argument in the command, we find that have indexes to them, just as arrays etc.

What follows is an example on how to display files and sort them:

$ ls 
hold mark names oldstuff temp zach 
house max office personal test
$ ls -r
zach temp 	oldstuff names mark hold
test personel office max house
$ ls -x
hold house mark max names office 
oldstuff personal temp test zach
$ ls -rx 
zach test temp personal oldstuff office
names max mark house hold

An options command is the - commands that we have been seeing in running ls and all the different kinds of things appended.
A option can come in a singular form or many followed by each other, as in -rx or just -x etc.

Options are command based, not shell based, so whilst you may write -r on one command ,it does different things to different commands.

As far as ls goes, r stands for reversed (can also be written --reverse) and x stands for horizontal sorted rows

For most utilities, you can write the order of the commands in any order, thus -xr would give the same output as -rx in terms of the ls command

Sometimes, options themselves require arguments, as follows with the gcc command that creates a executable file, with the specified name:

$ gcc -o prog prog.c //Each time we have a situation where we must specify an option argument, we must include a space.

To display a file in another format than just plain bytes (which is the standard format), we can use the -h command when we read a file.

Generally, when we would make statement calls, sometimes, these statements can become ambigious, by virtue of the fact that we find that
when we write the name or something, that perhaps it started with a hypen, that cuases the program to be confused when it is to run
commands such as $ ls -l for instance ; Either you wish to display all the files in cwd with a long format, or you want to display a file named -l in the cwd.
In this case, it interpets it as the first of the two options, as in, it displays´all files in the cwd in a long format.

If we wish to create files with - as the beginning of their names, we could simply use the virtue of -- to accomedate the fact that we are trying to initiate
upon a filename that has a -, that follows is an example of how to do that:

$ ls -- -l //This would give the listing of a file that begins with the name -l

We could also refer to it in a different format, such as taking a file from the current working directory, as is:

$ ls ./-l //This goes to the cwd and finds the file of which begins with -l

If we wish to find a long listing of the file, we could do as follows:

$ ls -l -- -l

To find help on how to write these options, we could use the xargs in a shellscript to find out more about forming arguments.

Also, these are conventions of naming, not hard n fast rules - Which means that it does not apply to all utilities fully.

If we wish to know more about a utility, we can use the --help option , as is shown in the following example:

$ bzip2 --help

This would give us tons of commands related to bzip2 and how it works.

There are two ways we can call upon utilities, either we do it simply by providing the name or we do it by providing full path name

$ ls
$ /bin/ls

This will either call ls by virtue of utility name, whilst the bin ls calls it by virtue of finding its full path and calling it from there

The ordering of commands on the command line is not important, as it structures itself accordingly:

we could for instance redirect the output flow and input flow to files, as is specified:

$ >bb <aa cat

If we provide the full length of the name, it checks in that specific directory - where as of if we provide only the name
it goes through a standardized list of values, which is specified in the PATH variable. 

if it cannot find the utility or the commend, it simply displays an error message that tells us that it was not found, as follows:

$ abc 
bash: abc: command not found

One reason the shell may not be able to find it, is that its not specified in the PATH variable. We can specify values to PATH with a temporary command in
bash, by virtue of the following:

$ PATH=$PATH:. //This . command adds the cwd to the PATH variable temporarily. It is a security risk to apply it permanently.

If it can find the file, but we lack executive rights - It says as a Permission denied message.

We can execute commands in the cwd by writing ./<filename>, so for instance if test is a filename, we could do ./test to run test from the cwd

When we direct information to programs, we have both a input and a output. The output and input of files is not standardized, and as such we need to
direct the flows of information to where its coming from and where it goings.

There is also a standardized location for sending Errors : Which means that we could redirect the output of errors, as such as redirecting them to logs, files
or the likes where we wish to see information about the Error of the program.

Normally the standardized outputs and inputs don't know where related units are, such as screens or other units. But, the kernel can ask for these inputs
accessing the location of different directotires or the likes to find out where the standardized input/output/error is.

in terms of Linux, we handle the screen as a place of output, which means that we can direct it as a file directory. Usually it is stored in the directory of dev/<user name>

the cat command, copies text that we have as input, until the point of where we give a terminal command (CTRL+D), an example of this is as follows:

$ cat
This is a line of text //written
-II- copied
etc.

When we work with multiple screens, each screen is treated as a seperate entity by the tty utility, that handles the concept of screens as files.

This basically leads us to being able to copy the effect of System.in of Java, but in terms of using the screen as a form of input/output, with the 
cat combined with using the tty to access said screen that we wish to process info on/from

Cat without an argument redirects information to the standard output location, which can be specified to be different things, but by default is standardized to be
the screen. This means that if we want to write to a file, we would just have to do the $ cat <filename> command to direct information into the cat file.
This could be useful for creating logs in a easy directed manner or redirecting workflow from other processes into wanted files.

We can redirect output and input, as said. What follows, are some examples of how to do this:

To redirect stuff into a file, we use the > directional argument, where as of if we want to direct FROM a file, we use the < directional argument.
This allows us to easily read and write to files by terms of using CAT.

The following is an example of redirection:

$ cat > sample.txt
This text is being entered at the keyboard //user input
cat is copying it into a file //user input
CTRL-D //Terminal signal given to the tty 
$

NOTE: Redirection is a non-safe-overwrite way of putting data to something. use it with caution or enable the uncobbler. To enable/disable the unclobber do the following:
NOTE: IT DOES NOT ALLOW NON-OVERWRITING, IT SIMPLY PREVENTS YOU FROM OVERWRITING BY DISPLAYING AN ERROR MESSAGE IF YOU ATTEMPTED TO OVERWRITE A FILE THAT EXISTS.

set -o noclobber //ENABLES NOCLOBBER
set +o noclobber //DISABLES NOCLOBBER

Noclobber is the function of doing so that we can write to a file without performing overwriting.

with the cat utility, we are able to concatenate several files into one file that holds many values : What is shown below, is an example of a file that redirects
several files contents to one file:

$ cat stationary
contents of first file
$ cat test2
contents of second file
$ cat test3
contents of third file
$ cat stationary test2 test3 > final //This sums up all the contents of each respective specified file and put them into the file called final

There are a number of utilties that work in this function in terms of being able to redirect information and direct information,
they consist of the following:

lpr, sort, grep and Perl.

It is the utility persay, that works like this, not the actual Shell.

The following example attemps to create a file with touch, turns on the noclobber, redirects echo to the file, and then turns of noclobber, to showcase again 

$touch temp
$ set -o noclobber
$ echo "hi there" > temp
bash: tmp: cannot overwrite existing file //we get this because noclobber is enabled

$ set +o noclobber
$ echo "hi there" > temp

If we wish to catenate two files, we have to keep in mind that it will only be able to proceed to do the first ommand in the line of ordering that exists.
The problem with this is the fact that we have to redirect the flow into a tmp file which we then rename using the mv function on the temp file.

$ cat orange pear > orange //This will destroy the original contents of orange, and put in the contents of Pear. This is incorrect usage

$ cat orange pear > tmp
$ mv tmp New //this will rename the file of temp to New

What follows is also a form of error, when we try to redirect information from a file to parse it into the grep function where we want to then put it into a.file, but the error writes wrong:

$ grep apple a b c > a output //Normally we would have to provide a.output, as this erornous code will remove the contents of a and give us an error message after some time.

We can override the noclobber function by using a pipe after the redirect symbol (tsch uses a ! mark for pipes), as follows:

$ date > tmp2
$ set -o -noclobber
$ date > tmp2
bash: a : cannot overwrite existing file
$ date >| tmp2 //This will overwrite the clobber function, allowing us to write to tmp2 despite the fact of that the clobber function is active.

if we wish to append instead of overwriting the data within the file, we simple have to redirect with two >'s instead of one, as such:

$ cat orange >> tmp //appends the contents of orange unto the tmp file

the noclobber function does not stop cp or mv, thus, we must keep in mind that we do not trust the clobber.
instead, we have backup of files, we use the >> command instead and we make sure to use cp -i or mv - i (interactive mode, safemode)

What follows is an example of redirecting and appending input:

$ date > whoson
$ cat whoson
Fri Mar 27 14:31:18 PST 2009
$ who >> whoson
$ cat whoson
Fri Mar 27 14:31:18 PST 2009
sam 		console 	Mar 27 05:00
max 		pts/4 		Mar 27 12:23
max 		pts/5 		Mar 27 12:33
zach 		pts/7 		Mar 26 08:45

If we wish to completely remove a file, we could dump it into a data sink, which is allocated in the /dev/null. Anything we put in here, disappears

$ echo "hi there" > /dev/null //Will remove the echo result as it goes into the null sink

if we wish to empty a file, we could redirect the contents of the null sink into a file, that effectively removes everything in it

$ls -l messages
-rw-r--r-- 	1 max 	pubs 25315 Oct 24 10:55 messages
$ cat /dev/null > messages
$ ls -l messages
-rw-r--r--  1 max 	pubs 0 Oct 24 11:02 messages

the following example showcases the ease of using pipes instead of using full written commands, in some cases:

$ ls > temp //redirect standard output to be going to temp
$ lpr temp //redirect standard output to be going from temp
$ rm temp //remove temp

we could rewrite all the 3 lines above, in one line, using a pipe. as follows:

$ ls | lpr //We redirect the readings of standard output to the printer, lpr

We can use pipes with any utilities that accept either from a file on the specified commandline  or from standard output
We can also use pipes with utilities that only accepts arguments from standard output

the tr utility, allows us to translate things, i.e change values of a character to another specified one, as follows:

$ cat abstract | tr abc ABC //we write out the contents of abstract when we have translated abc to ABC, as in, a -> A, b -> B, c -> C
$ tr abc ABC < abstract //Does the same thing.
//NOTE: The Tr utility cannot change the base file or the source file because it does not know the actual source.

An example of how to redirect flows of data, is as follows:

$ who > temp
$ sort < temp
max 	pts/4 	Mar 24 12:23
max 	pts/5 	Mar 24 12:33
sam 	console Mar 25 05:00
zach 	pts/7 	Mar 23 08:45
$ rm temp 

//Above, we redirect the information of who into a temp file, pipe the contents of sort into a standard output that is sorted, then we remove temp

//in the case of sort, it can take it's input from either a command line or a file, thus, we could ommit the < and still have the same result as above

íf we want to find one specific user, would pipe the output of who to grep to find a certain person, Grep is like a regex:

$ who | grep 'sam'
sam 	console 	Mar 24 05:00

As per usual, we wish to process the data in a lesser output, as in to be able to read it more comprehensively or whatever, we can use the keyword of less

$ ls | less

less displays one line at a time, and allows us to cycle through it with the controls of SPACE (another screen), one more line (RETURN), h(HELP) or q(QUIT)

If we wish to run something called a "Filter", we can pipe the output data of something through a pipe to another utility that processes, and then run it to yet another utility, as follows:

$ who | sort | lpr //This runs the output of who to sort which gives us a sorted output and the nruns it to the printer to print it
max 	pts/4 	Mar 24 12:23
max 	pts/5 	Mar 24 12:33 //This output is the hypothetical output we would get
sam 	console Mar 24 05:00
zach 	pts/7 	Mar 23 08:45

if we wish to send data in more than one direction, we can use the command of tee : it allows us to be able to direct data into two directions.
What follows, is an example where we redirect information of who into both the standard output by virtue of just calling a plain who, and running the
output of grepping after sam in the contents, to put it in a file called who.out, as follows:

$ who | tee who.out | grep sam 			//We send data in two different directions, one of which is running the output of who.out to who, whilst running the
										//grep on the output of who, and searching for sam

When we run utilities and programs, we usually talk about running it in either the foreground or the background. IF you run something in the foreground, you run
the program and the Shell waits for it to execute fully before you are allowed to make more processes and it waits for new commands.

However, we can run utilities in the background and allow for processing of tasks, without having the graphical output of things, and allows us to focus on
doing other things than that of handling the foreground.

WHen we refer to these types of operations, we are talking about something called a Job. View this as a process, basically. A job, is a process that is able
to be piped. And as is with the nature of foreground and background - we can only run one foreground job, whilst we can run many background jobs.

One of the main strengths of Linux is that it can run things in the background, and if we are using a GUI we can just switch terminal to see what our current
task of operations lay in.

If we wish to run a background process, we declare one with the & sign at the end of the processing line. Which gives the output of giving a small number in 
brackets first, which is the number of the job, followed by a ID that is related to the instance of the job. An example of this follows:

$ ls -l | lpr & //Pipe the output of a long runcheck of cwd to the printer and have it be in the background
[1] 22092 //Job number followed by id of the instnace of the job
$

the instance id is basically the number that relates to what kind of operation that specific operation is doing.

When a job is done, it shows a prompt, as follows:
[1]+ Done 		ls -l | lpr

We can move a foreground job to the background, if we wish, and we can interact with it, if we wish. WHenever we move a job to the background, we suspend it.
Thus we must re-run it to be able to make sure that we interacting with it, as follows:

ctrl-z

We could then restart the process, by giving the command of : bg <job-number>

If we have only one job suspended, we do not need to specify the job number.

THe difference between a fg job and a bg job is that a bg job does not accept keyboard input.
To bring a bg job to the foreground, we can run the fg command, which can also be written as %, followed by the job number. An exmaple of this follows:

$ fg 1 //can also be written as $ % 1, and again, if there is only one job in total, there doesn ot need to be a specification of a job number.

The prompt will then appear, that tells you of what the command you used to run it, was, as follows:

$ fg 1 
promptme //This is the command we used to run this hypothetical process that was brought to the fg

A background job, or a bg job, can give output to the main screen. If we wish for it to not interfere with the rest of our operations, we can
pipe the output to something else so that it's not defaulted to the standard output (i.e, the screen)

if we wish to abort a background job, we cannot simple do with the interuppt command of ctrl+C, we have to use the kill command. If we wish to
know the process id, we can run the command of ps to find out all the current running processes. Also, when we perform a kill command, we must have the
process ID - and as per usual, either we write the % command with the job number, or we use kill and the Process ID

What follows are ways to find a file with a specific job number:

$ tail -f outfile &
[1] 18228
$ ps | grep tail
18228 pts/4 	00:00:00 tail
$ kill 18228
[1]+ terminated 			tail -f outfile
$

if we forget the jobs, we can use the jobs command to find out all running jobs, as follows:

$ tail -f outfile & 
[1] 18236
$ bigjob &
[2] 18237
$ jobs
[1]- running 			tail -f outfile & //the -f is used to write to a file
[2]+ running 			bigjob &
$ kill %1 //kill the job with job number 1
$ RETURN
[1]- Terminated 		tail -f outfile &
$

When we search for files, or name files, we can use something called wildcards. WHich is basically a generic type search in the system,
and the system expands the provided argument and tries to find any matches to whatever value we put in. The reason for wildcards, is if we find that
we cannot really remember what a file is called, so to find it, we could use wildcards in our search.

an example of using a wildcard character, is as follows:

$ lpr memo? (where the ? is a wildcard character, we simply do not know the value, so it's going to guess whatever value follows on that point)

an example follows:

$ ls
mem memo12 memo9 memomax newmemo5
memo memo5 memoa memos
$ ls memo?
memo5 memo9 memoa memos

if no match is found, it either passes the actual string which we tried to parse in terms of wildcarding, or if it set up to, it provides a null string to the lpr.

we can also use wildcard characters in the middle of a search statement:

$ ls 
7may4report may4report mayqreport may_report
may14_report may4report.79 mayreport may.report
$ ls may?report
may.report may4report may_report mayqreport

if we want to practice or check the output of a wildchar check, we can use echo to echo the output of what we are doing (assumed we are using default output):

$ echo may?report
may.report may4report may_report mayqreport //the ? acts as a wildcharacter and allows us to match against any value on that point, where as of echo allows us to
//repeat the values we get from calling the wildcard search

if we are searching for things with periods (.), we have to explicitly name them in the search, as ? does not account for .

the * character is basically regex, except it allows for matches against nothing as well, so the following is legit:

memo* (Will find words that begin with memo)
*memo (will find words that end on memo)
*memo* (will find words that contain memo)

if we run ls with the -a command, we find all files (even hidden ones.)
if we run echo * afterwards a input, we echo the output and find that the * explicitly does not find hidden directories by default, instead we have to
run the .* to echo and find all the hidden files in the directory

the following interactions showcases the differences between running ls with a -a command and running echo, even with .p* or a ls with a .* approach

$ ls -a //shows all directories and their contents
. 		.private 	memo.0612 reminder
.. 		.profile    private 	report
$ echo .p*
.private profile //Only finds values of which match the calling of .p*
$ ls .* //showcases the entire directory contents
.private .profile

.: 
memo.0612  private 	reminder report

..: 
.
.
$ echo .*
. .. private.profile

we can use this logic to out favor, as is showcased in the following example, where we throw all the .txt files to the printer in the background:

$ lpr *.txt & //lpr for printer, *.txt for all files that are .txt files, & for background operation

if we wish to narrow down the regex, we can call the [<letters/numbers>] version, we can also combine it with the * operator, for some interesting effects,
as follows:

$ echo [aeiou]* //Find files that begin with a or e or i or o or u
...
$ less page[2468].txt //shows the less version of the output of searching for page2, page4, page6, page8

We can also do ranges, such as :

$ less page[2-8] //searches for page2, page3, page4, page5, page6, page7, page8

$ less page[a-z] // all small letters

$ less page[a-zA-Z] // all english letters

What follows are some different ways of calling files in ranges:

$ lpr part0 part1 part2 part3 part5
$ lpr part[01235] //checks for part0 part1 part2 part3 part5
$ lpr part[0-35] //checks for part0 part1 part2 part3 part5

Another way to find files, is as followed:

$ lpr part[0-9] part[12][0-9] part3[0-8]

we can also run the regex method to find ranges on letters

$ echo [a-m]*
$ echo *[x-z]

the normal rules of regex applies to the [], as in ^ and !, which basically means not

and to be able to match - or ] you have to put it at the position of before the closing ]

the actual utility of ls cannot recognize ambigious characters (the ? character in naming conventions), but is handeled by the shell.
An exmaple of how this works, is showcased as follows:

$ ls ?old //when the shell handles it, it works
hold
$ ls /?old //will give na error, because it cannot find the ?old filename
ls: ?old: No such file or directory

built in utilities, are what we think of when we speak of commands, and we can get a list of the ones specific to the shell we are running by the following:

in bash: info bash

in tcsh: man tcsh followed by /Builtin commands$

The VIM editor:

A VIM editor is a Textfile editor that is very commonly used within Linux environments. 

There are a number of vi editors, like: elvis, nvi, vile and vim.

VIM is meant to be used for writing code, mainly, or notes.

What follows, are the basics of how to interact with VIM:

vimtutor and vim help files are not installed by defualt, so we must install them first if we wish to start learning Vim.

Vims commands are specific to what kind of terminal we are running. Normally, the terminal is set automatically, but we can specify manually
if we wish to have another kind of terminal than the one provided by default.

to create a file in vim, we can do as follows:

$ vim practice

on some linux systems, you can run vim with the command: vi

if we wish to emergency exit from vim, we can do the following:

:q! (This does not save the work, and the : command moves the cursor to the bottom of the screen and performs the command)

if we make a command call to vim with no arguments, we get a tutorial, and if we edit a file, it will show the first few lines of the file and
some status related things to the respective file

Vim has two modes: Command Mode and Input Mode

in command mode, vim allows you to execute commands such as removing files or exiting vim, or entering input mode.

If we are in input mode, it will return whatever we write to display it on the screen. To exit input mode, we can press ESC.

to be able to see the line number next to our input, we can use the following command:

:set number RETURN //this turns on numbering of lines.

:set nonumber RETURN //this turns off numbering of lines

to go to the bottom of the screen, we can use the : command, as it brings us to the last line of the screen.

To be able to start writing to the vim, we have to put it into a input mode, which we can do either with i or a, i for input (before cursor) or a (after cursor)

vim is case sensitive in its commands.

if we are not sure if we are in input mode in vim, we can press ESC to bring us to command mode, or if we are in command mode, it'll do nothing/beep etc.

to be able to run help commands, we have to have the vim-runtime installed.

we can get help about features by using the command of : 

:help [feature]

you must be in command mode when giving this command.

to close, we can do :q, to scroll up half a page we can do ctrl+D

to remove text, we can use the wordkill, linekill or erase keys - which are (CTRL+H, CTRL+U, CTRL+W) respective.

When we do ENTER or write over hte text, after this, then it graphically updates the changes.

If vim is in command mode, we can mode the cursor, with the following keys: RETURN, SPACE, or THE ARROWKEYS.

If the emulator we use, does not support these things, we can use the hjkl keys to move em, in the order of as follows:

h : left, j : down, k : up, l : right

if we wish to delete a specific character, we can move it to a character and press x

if we want to delete a word, we move the cursor infront of a word and write dw

if we want to delete an entire line, we just put the cursor on a line and go dd

Before we begin the list of all the commands, what follows is a list of Abreviations that we use in relation to VIM:

UNBOUND - Has no function

word - relates to a word of lowercase letters

Word - relates to a word of CAPS letters

sentence

paragraph

cursor motion command - any comamnd that positions the cursor

If one goes into Key binding modes, such as insert, replace etc. some keys change functionality:

ESC - leave edit mode, return to command mode

^D - move line backwards one shiftwidth, shiftwidth must be set and either line must be newly added or ^T must have been used.

^T - move all after cursor forwards one shiftwidth

^H - deletes text that was entered during the current edit mode. Most VI editors do not allow deletion to the point of the previous line

^V - inserts next character even if it is a editing character

Some commands allow for numeric placement before execution, as follows:

z - position of n:th number of line
G - goto nth number of line
| - goto nth number column
r - replace next n characters
s - substitute for n next characters
<< - shift n lines left one shiftwidth
^ - ignored?
_ - advance n-1 lines


Vi editors do not allow for easy switching between two different files, but allows editing of the previous file that was recently opened.

To edit a file, we can give the command of : :efilename (so for instance, :etestfile)
if we wish to switch files, we can use the ctrl+^ command

We can use two different filters to refer to the current and the previous recently edited file, as follows:

% refers to the current file

# refers to the previous file 

Here are some examples of what we can do with it, in terms of vi:

:map v :!chmod 644 %^[		make-world readable
:map q :!ci -l %^[ 			RCS checkin
:map V :!diff # %[ 			Compares the previous and the current file

we can map keys to custom commands, and the available ones are the following:

g q v K V # * \ = ^A ^C ^I ^K ^O ^V ^W ^X ^[ ^_

We can also use the :map command to map different keys to different results, as follows:

:map! ^? ^H //makes delete act like space
:map! ^[OA ^[ka //These 4, allow for a sequence of where you exit edit mode, move a cursor, and re-enter cursor mode.
:map! ^[OB ^[ja //Which basically means that we can "move the cursor in Editor Mode", nifty
:map! ^[OC ^[la
:map! ^[OD ^[ha

We can also map abbreviations as follows: :ab <something>

In terms of :ab, we actually have to do so that we write the whole word before we account for the action being done

what follows, are some macro examples of Vi, where we see that we can map different keys to different outputs:

:ab teh the //Allows us to write teh for the
:ab #d #define //redirects #d to be #define
:ab #i #include //these are all abbverbiations mappings
:ab cmain main(argc,argv)^Mint argc;^Mchar **argv;^M{^M}^[O

What follows, is a more complete list of all the vim commands:

																	Followed by:
a - enter insertion mode after current character 					text, ESC

b - back word 														

c - change command 													cursor motion command

d - delete command 													cursor motion command

e - end of word

f - find character after cursor in current line 					character to find

g - UNBOUND

h - move left one character

i - enter insertion mode before current character 					text, ESC

j - move down one line

k - move up one line

l - move right one character

m - mark current line and position 									mark character tag (a-z)

n - repeat last search

o - open line below and enter insertion mode 						text, ESC

p - put buffer after cursor

q - UNBOUND

r - replace single character at cursor 								replacement character expected

s - substitute single character with new text 						text, ESC

t - same as "f" but cursor moves to just before found char 			character to find

u - undo

v - UNBOUND

w - move forward of one word

x - delete single character

y - yank command 													cursor motion command

z - position current line 											CR = top; "." = center, "-" = bottom

A - enter insertion mode after end of line 							text, ESC

B - move back one Word 												

C - change to end of line 											text, ESC

D - Delete to end of file 

E - move to end of Word

F - backwards version of "f"										Character to find

G - goto line number prefixed, or goto  end if none

H - home cursor - goto first line on screen

I - enter insertion mode before first non-whitespace char 			text, ESC

J - join current line with next line

K - UNBOUND

L - goto last line on screen

M - goto middle line on screen

N - repeat last search, but in opposite direciton of original search 

O - open line above and enter insertion mode 						text, ESC

P - put buffer before cursor

Q - leave visual mode (go into "ex" mode)

R - replace mode - replaces through end of current line, then inserts 	text, ESC

S - substitute entire line - deletes line, enters insertion mode 		text, ESC

T - backwards verison of "t"											character to find

U - restores line to state when cursor was moved into it 

V - UNBOUND

W - foreward Word

X - delete backwards single Character

Y - yank entire line

Z - first half of quick save and exit 									"Z"

0 - move to column zero 

1-9 - numeric precursor to other commands

SPACE - move one character to the right

! shell command filter 													cursor motion command, shell command

@ vi eval 																buffer name(a-z)

# - UNBOUND

$ - move to end of line

% - matches nearest [], {}, () on line, to its match (same line or others)

^ - move to first non-whitespace char on line

& - repeat last ex substitution (":s ...") not including modifiers

* - UNBOUND

( - move to previous sentance

) - move to next sentance

\ - UNBOUND

| - move to column zero

- - move to first non-whitespace of previous line

_ - similar to "^", but uses number prefixes

= - UNBOUND

[ - move to previous "{...}" section

] - move to next "{...}" section

{ - move to previous blank line seperation

} - move to next blank line seperation

; - repeat last "f", "F", "t" or "T" command

' - move to marked line, first non-whitespace

` - move to marked line, memorized column

: - ex-submode

" - acess numbered buffer, load or access lettered buffer

~ - reverse case of current character and move cursor forward

, - reverse direction of last "f", "F", "t" or "T" command

. - repeat last text-changing command

/ - search forward 						Search String, ESC, or CR

< - unindent command

> - indent command

? - search backwards					Search String, ESC, or CR

^A - UNBOUND

^B - back (up) one screen 

^C - UNBOUND

^D - down halfscreen

^E - scroll text up (cursor doesn't move unless it has to)

^F - foreward (down) one screen

^G - show status

^H - backspace

^I - (TAB)UNBOUND

^J - Line down

^K - UNBOUND

^L - refresh screen

^M - (CR) move to first non-whitespace of next line

^N - move down one line

^O - UNBOUND

^P - move up one line

^Q - XON

^R - does nothing

^S - XOFF

^T - go to the file/code you were editing before the last tag jump

^U - up half screen

^V - UNBOUND

^W - UNBOUND

^X - UNBOUND

^Y - scroll text down, cursor doesn't move unless forced to

^Z - suspend program

^[ - (ESC) cancel started command, otherwise UNBOUND

^\ - leave visual mode (go into "ex" mode)

^] - use word at cursor to lookup function in tags file, edit that file/code

^^ - switch file buffers

^_ - UNBOUND

^? - (DELETE)UNBOUND

To delete a character in vim, we can use x (delete character), dw (delete word), dd (delete line)

u works like CTRL+Z, except the fact that it can be toggeled to work with editing even earlier inputs, if compatible mode is disabled

:redo - this works like redoing the command we undid. Can also be performed through CTRL+R Same rules for compatible mode yields

i - insert command, needs to be followed by text so that it inserts it

a - appends text

o/O - inserts a blank line at the cursor position - O inserts one above the cursor position. Could be bound to a macro to ensure that
you enter edit mode insert it, go to the last line, and then re-enter the editmode. (basically creating the illusion of that we have
gone to a line, edited it, and then simply continue down the line with our editing process of whatever file we are processing.)

An example of editing text would be the following:

move cursor to word, dw, Insert mode by i, <new word> + space, ESC, 

we can also do the entire process of this, by virtue of using the cw command, which is the change word command which does the same as above,
and then results us in input mode.

Whilst we are editing the text we are working on is stored in the Work Buffer.

To finish up editing, we have to of course save everything from the Work buffer, so that we actually have it on a hard disk. As the Buffer
is basically just temporarily allocated short-term memory.

To save, we can use the ZZ command. This is not the same as CTRl+Z (suspend).

Generally, when we talk about compatible mode that we can put commands into, it is much akin to interactive mode, except it affects
keys in different aspects, based on the key. To find out what the differences are, we can use the command of :help compatible (this will
display help regarding the compatible command).

To see the differences between how vim works differently from vi (basically vim is a vi clone), we can write the following command:

:help vi-diff

To set the compatible option, we have to use the -C option, and to not use it, we have to use the -N command.

To see what version we are running and what current operations are supported in respective vi-clone that we are running, we can give the
vim --version command.

if we wish to find help in Vim, about a feature that is in the vim, we can simply use :help <feature>, where <feature> is whatever feature we
want to find out how it works and why it works.

When we see a |<example word>| notation around a word, that means that it is an active link. An active link is pretty much akin
to that of a keyword which means it has a associated Link in the manual, or that we can run the help command on it to find out
what it does.

an example of this, would be with the following: |tutor|

to find out more about tutor, we could then write :help tutor.

When we are done with reading about the command, we can use the :q! command to exit the help window that we opened up regarding that
specific command.

Some of the common features we might want to run :help on, is insert, dlete and opening-window.

To see a complete list of help-files, as in all the available help commands we can possibly run, we can give the :help doc-file-list command

Vim is based on the ex editor, which has 5 distinct modes, as follows:

ex Command mode

ex Input mode

vim Command mode

vim Input mode

vim Last Line mode

when in command mode, vim does not respond in the way of adding said character to the screen, instead it interpets the respective
character as a formal command.

when in input mode, vim responds by putting characters into the text, outside of special characters that are not escaped, because
thoose are understood as commands (for instance, esc)

Last Line refers to the last line of the editor, that our text is not able to be input to, and generally is reserved for a mode called
Last Line mode, where whatever command we give, appaers there, and must be executed with a ENTER.

In command mode, Vim does not require you to execute each command with ENTER.

Generally, you do not use ex modes, and whenever we write about Input and command modes, we refer to the vim modes, not the ex modes.

When a vim session begins, it always begins in a command mode, and to put it into insert mode, there are a few commands that automatically
do this, such as : Insert and Append. If we press ESC, we always return to command mode.

The change and replace commands, combine the two ; resulting in an action followed by having you in input mode where you are to write
things.

By default, there is a mode enabled that allows us to see what mode we are in ; command mode or input mode. It is, however, able to be
turned off, and it is called showmode.

We can also turn on the statusline mode, which allows us to see what status the current line is operating in, which we do as following:

:set laststatus=2 

Also, NOTE: The difference between a CAPS char and a lowercase char, is a real thing for VIM, so always be weary if you are in fact 
trying to edit things with CAPSLOCK on.

Status Line: This line, which is affected by the above command, works as the output does in Java or the Console in python, where it
defaults to describe what the state of current text is in, what is occuring etc. etc.

Redrawing the screen: if our screen gets cluttered with variables that we did not actually add, they are visual bugs, and can be removed
with going into command mode (ESC) and then hitting CTRL+L.

Every line that is not going to be in use or be printed upon, as in not related to a string value put upon it,
has a ~ symbol placed on it.

We can also use CTRL+W to back up over words.

Generally, when we work with files, it reads in all the contents of a file into the local work buffer - where it stores the data locally,
until the point of where you save it - where it writes the contents of the buffer to the disk.

if we wish to see a file on the disk, and it's contents, we can use the $ view <filename> command

Calling upon the view utility is the same as calling the vim editor with the -R modification, a modification that is read only.

Once we call upon the view command, we cannot write back the contents to the same file as the one which we named in the commandline.
We could always write the contents to another file or do the same functionality except with another program, such as Midnight Commander (mc),
which allows us to write the mcview command instead of using the vim view command.

the vim editor can operate on any file format, assuming that a single line can fit into the available memory.
(That is, the characters between the newline chars)

The VIM editor allows us to open, close and hide multiple windows, each of which allows you to edit a different file. Most of the window
commands consists of CONTROL-W followed by another letter. For example, CONTROL-W s opens another window (splits the screen) that is editing
the same file. Control-W n opens a second window that is editing an empty file. CONTROL-W w moves the cursor between windows, and 
CONTROL-W q (or :q) quits (closes) a window. Give the command :help windows to display a complete list of windows commands.

Generally, when we open files, Vim locks the file. The only time we will see error messages in regards to this, is when you try to
edit a file that you were editing when vim or the system crashed or when you are trying to access a file that someone is already editing.

To quit editing in Vim, with a saving exit, we either run ZZ or :wq from command mode

if we wish to quit without saving or writing the contents of the work buffer, we can do the :q! command

In case of that Vim does not allow us to normally exit, we could do a trick to allow it to let us go:

:w filename //By writing to a filename, we enter vim, and we can then just exit by virtue of the :q command.

The only time we need to provide the ! notation after the q command, is if we have saved things in regards to having saved the contents of the work
buffer to the disk.

if we are in a position that we cannot write to files, because we don't have the right, we can go one step further and create a temp
file, and then overwrite it, accessing it from a absolute path, as follows:

:w filename
ZZ 

//if this gives us an error, we can solve it as follows:

:w home/max/temp or :w ~/temp //both absolute working directories

if they are reported to exist, we can then :w! filename on them. //w! is overwrite, instead of just write.

WHen we suffer a crash or some kind of accident, we can recover some of the data that we were working on, as follows:

$ vim -r //Vim keeps its work buffer in a file called swap, thus, if we crash, we can try to access it and find the lost data from it
//What follows is a hypothetical output:

Swap files found:
	In current directory:
1. 	.part.swp
		owned by: max dated: Sat Jan 26 11:36:44 2008
		file name: ~max/party
		modified: YES
		user name: max 		host name: coffee
		process ID: 18439
2. 	.memo.swp
		owned by: max 	dated: Mon Mar 23 17:14:05 2009
		file name: ~max/memo
		modified: no
		user name: max 	host name: coffee
		process ID: 27733 (still running)
    In directory ~/tmp:
    	-- none --
    In directory /var/tmp:
    	-- none --
    In directory /tmp:
-- none --

With the -r option in vim, we find that we can display a list of swap files it has saved.
What follows, is an example of a user recovering from a crash:

$ vim -r memo
Using swap file ".memo.swp"
Original file "~/memo"
Recovery completed. You should check if everything is OK.
(You might want to write out this file under another name
and run diff with the original file to check for changes)
Delte the ..swp file afterwards

Hit ENTER or type command to continue
:w memo2 //Write the contents to memo2
:q //quit vim
$ rm .memo.swp //remove the swp file for memo

NOTE: We have to be in the respective system that the file crashed in, as we cannot recover a file that is
from a system that we did not have it in.

if we have long string in a Vim editor, and we would want to change so that it is displayed on one line, even if it would be split, we can set settings for this with:

nowrap

We can move the cursor by characters:

h or left arrow key - left

l or right arrow key- right

we can also move a cursor with numeric commands, such as:

7+space or 7l (moves it right, 7 steps)

We can also move the cursor to a specific character:

to forward search, we do the following: 

f<letter/letters we want to search for)

to backwards search, we do the following:

F<letter/letters we want to search for)

; repeats the last find command

We can also move the cursor by words:

the w/W key moves the cursor to the first letter of a word ; w for a lower case letter, W for a uppercase letter.

the b/B key moves the cursor to the first letter of the previous word (b matches any, B matches only alphabetical letters)

the e/E key moves the cursor to the next end of a word.

We can also move the cursor by lines:

j/k : j is the same as down arrow key, k is the same as up arrow key.

We can also move the cursor by sentences and paragraphs:

)/( moves the cursor forward to the beginning of the next sentence/ moves the cursor backwards to the beginning of the previous sentence

}/{ same functionality, except with paragraphs

H: moves the cursor to the begining of the file

M: moves it to the middle line.

L: moves it to the lower line.

If we wish to partially move forward in terms of the vim showing, we can use the CTRL-D or CTRL-U respectively (D for backwards, U for forward)

If we put a number after the command, it will scroll that many lines, each time.

We can also move in terms of entire pages, with the following: 

CTRL-F for 1 page forward at a time

CTRL-B for 1 page backwards at a time

We can also use the G command as (goto) followed by line number

to insert text, we have to be the insert mode, to enter it, we provide the command of : i

We can append text with a/A, a being after the current character, A being after hte last character in the current line

If we want, we can insert special characters by entering input overwriting mode, with ctrl+v

To undo changes, we can use the u/U key, the u only removes latest change, U removes all the latest changes to a respective word that you worked on

to delete a character, we can use the x<numeric input> command ; which allows us to delete <numeric input> amount of characters.

The X command deletes the character to the left of the cursor.

To delete text, we can use the d/D command

Depending on the unit of measure and the repeat factor set, defines how much d will remove.

to delete a single line, we can use dd. To do several, we can put repeat on Dd by adding a number afterwards.

to delete up to the next semicolon, we can use the dt; command, where as of if we want to delete the rest of a line,
we can use D or d$

What follows is a list of commands and interactions:

dl - deltes current char 

dO - deletes from the beginning of the line

d^ - Deletes from first character of line

dw - deletes to end of word

d3w - Deletes to end of third world

db - Deletes from beginning of word

dW - Deletes to end of blank-delimited word

dB - Deletes from beginning of blank-delimieted word

d7B - Deletes from seventh previous beginning of blank-delimitted word

d) - Deletes until end of sentance

d4) - Deletes to end of fourth sentance

d( - Deletes from beginning of sentance

d} - deletes to end of paragraph

d{ - deletes from beginning of paragraph

d7{ - Deletes from seventh paragraph perceding beginning of paragraph

d/text - Deletes up to the next occurance of <text>

dfc - Deletes on current line up until and including next occurence of c

dtc - Deletes on current line up to the next occurance of c

D - deletes to end of line

d$ - Deletes to end of file

dd - deletes current line

5dd - deletes five lines starting with current line

dL - Deletes through last line on screen

dH - Deletes from first line on screen

dG - Deteles through end of Work Buffer

d1G - Deletes from beginning of Work buffer

the c command changes text with new text. We can change position of the changed text, and we can change one word to many things, paragraphes, lines etc.

The C command changes text until the end of line.

after we enter a c command, we find that the Vim editor enters input mode.

What follows, is a list of commands, related to Changing, C:

cl - 	Changes current character

cw - 	Changes to end of word

c3w - Changes to end of third word

cb - changes from beginning of word

cW - Changes to end of blank-delimetted word

cB - Changes from beginning of blank-delimated word

c7B - Changes from beginning of 7th previous blank-delimated word

c$ - Changes to end of line

c0 - Changess from beginning of line

c) - Changes to end of sentance

c4) - Changes to end of 4th sentance

c( - Changes to beginning of sentance

c4( - Changes to beginning of 4th sentance

c{ - changes to beginning of paragraph

c} - changes to end of paragraph

c7{ - changes to 7th paragraph beginning

ctc - changes on current line up to the next occurence of c

C - changes to end of line

cc - changes current line

5cc - Changes five lines starting with current line

the dw and cw command differ, in that dw deletes the entirety of said content, where as of cw only changes the words, and leaves the space seperations as was.

we can also replace text with the virtue of the s/S command. 

The s command deletes the current char and puts vim into input mode until you interuppt it. The S command does the same thing as the cc comand,
it changes the current line. The s command replaces characters only on the current line.

s - replaces one or more characters for current character

S - replaces one or more characters for current line

5s - Subtitutes one or more characters for five characters, starting with current character

we can change from lowercase to uppercase, with ~.

we can put a numeric fix before the ~ command to define how many chars we want to affect in this matter, so for instance, 5~, we do 5 characters to interact with
char casing.

To find characters, we can use the f or F command.

We can also use t or T, where t will put it for the next occurance, and T will put it for the previous occurance.

We can also combine search statements, such as follows:

d2fq //Which means, deletes, from current char to the second uccnrance of q in the current file.
//So, basically, d for delete, 2fq for find occurance of second place where q occurs

We can search for strings in vim, using the / followed by a regex argument.

if we wish to find the previous occurance, we could instead use ?

if we wish to explicitly find a ? or  / char in respective search, we must escape it with a \ char in the search first

If we wish to renter our last search query but not have to repeat the input, we could write n/N.

the n command, searches in the exact same direction as the given search command, whilst the N command searches in the exact opposite direction.

When it searches, it searches through the entire thing, by wrapping around the entire buffer and going through it.

When vim does a normal search (it's default mode), you enter a slash or a question mark followed by the search string.

if we perform a incremental search, however, it will move the cursor based on the string input thus far - which does so that we can match to partial
or entire inputs, based on what we have written so far.

If we wish to turn on, incremental searching, we can give the command of :set incsearch to turn it on, whilst if we want to turn it off, we use noincsearch

What follows is a list of the special commands in terms of regex, when we talk about vim:

^ : this is a beginning of line indicator, so if we were to for instance search for /^the, we'd find the next line that begins with the string the.

$ : This is the end of line indicator, thus, we can do things like /$! to find a line that ends with !, for instance

/> and \< - End of word indicator and Beginning of word indicator. Each respective one of these, finds the beginning of a word and the end of a word.

An example: \<The - Finds the next word that begins with the string The.

We also have quantifiers, in terms of zero more occurances, such as the * operator, as follows:

/dis*m is a regex that runs with a quantifier of trying to find 0-n many s letters at that spot.
Examples of matches would be : dim, dism or dissm.

if we put a set of [] around a set of characters in a regex search, it will find a singular instance of a letter within the [] notifications, as follows:

/dis[ck] 

this will match against: disc OR disk.

What follows are some examples on the interactions of the different operators:

/and - Will find and in any context.

/\<and\> - Will find the next occurance of and, explicitly.

/^The - finds the next line which starts with The

/^[0-9][0-9]) - Will find the next line that begins with two digits followed by a )

/\<[adr] - Finds the next word that starts with a, d or r

/^[A-Za-z] - Finds the next line that starts with an uppercase or lowercase letter

We can combine regex commands with a substitute command that replaces whatever result we found with the regex, as follows:

:[g][adress]s/search-string/replacement-string[/option]

If we do not specify the adress, it will default to searching the current line. If we use a single line number as the adress, it will use that as the adress.
if we provide a format of a range , as in (10, 20), it searches thoose and all the lines between thoose.

We could also provide a string as an adress between two slashes, this way allows us to provide a string value for it to try to match against a line
and operate on that specified line.

if we preceed the first slash with the letter of g, we define it to be global. (Which means that it interacts on ALL lines that matches against the adress)

What follows, is a list of different adresses used in vim:

5 	- This would just be line nr 5

77,100 - This would be lines 77 through 100, so 77 to 100

1,. - Beginning of work buffer (line 1) to current line (.) notation

.,$ - Beginning of current line to end of buffer

1,$ - Entire workbuffer

% - Entire workbuffer

/pine/ - The next line containing the word pine

g/pine/ - All lines containing the word pine

.,.+10 - Current line + the 10 following ones

Generally, the structure of searching and replacing strings, comes in the form of:

:<range><type>/<first string>/<second string>/

Some examples of how to interact with these things, follows:

:s/bigger/biggest/ //this replaces the first occurence of bigger with biggest, s in these cases, are related to the fact that it expect a String type

:1,.s/Ch 1/Ch 2/g //replaces all occurences of Ch 1 with Ch 2

:1,$s/ten/10/g //replaces every occurence of ten with 10

:g/chapter/s/ten/10/ /replaces the first occurence of ten with the string 10, on all lines containing chapter

:%s/\<ten\>/10/g 	- Replaces every occurence of the word ten with the string 10

..,.+10s/every/each/g - Replaces every occurence of the string every with the string each, for current line and following 10 lines

:s\/<short\>/"&"/ - Replaces the occurence of short on the current line, with "short"

by default, the substitute command (as above), replaces the first occurence. To replace ALL occurences, you need to put g in the beginning for Global.

If you wish for vim to ask you for confirmation, put a c in the beginning. 

What follows are some examples of the itneractions between substitute commands:

:/candle/s/wick/flame/ - substitutes flame for the first occurence of wick on the next line that contains candle

:g/candle/s/wick/flame/ - performs the same operation for first occurence, except on every line of the file

:g/candle/s/wick/flame/g - performs the ooperation on every line, and every occurence

if a search-string is the same as an adress, we can leave respective field empty:

:/candle/s//lamp/ is the same as :/candle/s/candle/lamp/

If we wish to join two lines, we can use the J command.

If we wish to display the status of a file, we could use two different commands: :f or Ctrl+G
Generally, this would be produce something along the lines of the following:

"/usr/share/dict/words" [readonly] line 28501 of 98569 --28%-- col 1

the . command, repeats the last command that actually had a changing effect upon things.

The Vim editor, has a General-purpose buffer and 26 named buffers, that can all hold text during a session.

All the text that we edit, delete or yank, is put in the general-purpose buffer. The undo command, retrieves it's info
from the General-purpose buffer, when it runs it's command.

If we wish to remove something but keep it in store for later, we can use the yank command : y/y

the y command and the Y command are like Copy, where y yanks a part, whilst the Y command, yanks an entire line
into the general-purpose buffer.

To only delete one line or yank one line, we must use double citation, as in terms of dd or yy.
If we run a d command or a y command, it deletes/yanks 2 lines.

And whilst D deletes from the pointer to end of line, Y yanks the entire line, regardless of cursor position.

To access the text we had in the buffer, we can use the p or P command, respectively.

If we use the p command, we get it AFTER the current character, whilst the P command, puts it BEFORE the current character.

Whenever we delete things, they are put into the General Purpose buffer, and allows us to interact with it to retrieve values from it.

To access one of the 26 buffers, we can give them a name and store them in there, by having a name reference included when we make
a command that is a yank or delete command, as follows for instance:

"kyy - this would put the value of whatever we yank, into a buffer named k.

Unlike the general purpose buffer, named buffers only clear out and change, when we actually give explicit orders to clear them out.

When we interact with a named buffer, if we provide the name of the buffer as a capslocks variant, it means that we are appending to that
named buffer - whilst if we access it with small letters, then we are overwriting it.

Assume that we have a buffer called a, and we wish to retrieve the information from it through command mode :

we could do as following, then:

" ap

Beyond the 26 buffers and 1 general output buffer, there are 9 numbered buffers.
If we wish to interact with the numbered buffers, we could do as follows:

" 1p //this pastes the info from the first number buffer.

The . interaction (echo a command), works differently for numeric buffers, as they give us the possibility to
"iterate" buffers, meaning that we can access the next buffer in line, by just providing the . command.

The reason for this, is because the numeric buffers act differently with the . notation, where if you called a previous
number buffer with a command, if we recall it with . , it increments by 1. So, if our previous call to a number buffer was 
" 1p , calling . on it, would make it essentialy " 2p , etc.

If we wish to read from files, we could use the :r command. This reads the contents into the Working Buffer,
and saves the content to either the current line (assuming no adress was specified), otherwise, if one is specified,
we get it positioned to the relevant specification.

Generally, the syntax is as follows:

:[address]r [filename]

If we omit the filename part of this command, vim resorts to the current file that we are working on.

When we use the :w command to write to files, we can specify adresses for the text output (as in, where it's going to end up),
and we can specify which adresses it is we wish to write.

If we do not specify any adress, it writes the whole work buffer.

There are two ways of writing to a file : overwriting or appending. As follows, we can see their different syntax:

:[address]w[!] [filename] //overwrites. For security reasons, i.e not accidentaly overwriting, vim forces [!] to be added to the command of w to write it with overwrite.
:[address]w>> filename //Appends to the file

To identify the current file, we can use the :f command.

We can modify the parameters that vim runs with, to customize it. The following places are where we can modify it:

The places of where these modification files, are as follows:

~/.bash_profile - This is the profile for the Born Again Shell (Bash)

~/.tcshrc - This is the profile for the TCSH shell

if we wish to set parameters from within the vim file, we could do as follows:

:set incsearch

If we wish to modify vim on it's initialization, we can do the following:

first, we open our ~/.bash_profile //this is for Bash

we can then enter the general syntax of the following: 

export VIMINIT='set param1 param2...'

An example of a init argument for vim, is as follows:

export VIMINIT='set ignorecase number shell=/bin/tsch wrapmargin=15' //this causes vim to ignore casing on searches, displays line numbers, uses the TC Shell to execute linux
commands and wraps text 15 characters from the right side of the screen

if we were to use abbrevations for the commands, it would look as following:

export VIMINIT='set ic nu sh=/bin/tcsh wm=15' 

if we are using the tcsh, we can put in our ~/.tcshrc startupfile:

setenv VIMINIT 'set param1 param2 ...'

Instead of having to set the parameters in a existing directory, we could just create our own and set the parameters there, as follows:

$ cat ~/.vimrc
set ignorecase
set number
set shell=/bin/tcsh
set wrapmargin=15

$ cat ~/.vimrc
set ic nu sh=/bin/tcsh wm=15

if we give a :set all + ENTER, we find that it will display all of the available parameters and indicate how they are set.

to turn on or off a parameter, we can do as follows:

:set number (to turn on number parameter)

or

:set nonumber (to turn off number parameter)

To change a value, we could do as follows: :set shiftwidth=15

most of the parameters have shorthands for their real names, such as nu for nomber, nonu for nonumber, sw for shiftwidth etc.

magic : This command, when turned on, gives meaning to the following special characters: . [  ]  *

nomagic: as above, except it turns off thoose symbols meanings.

autoindent/ai : This command turns on autoindent. It is off by default. It allows for combination with shiftwidth to provide regular setting of formatting.
if we are in input mode and autoindent is on, we can press CTRL+T to move the cursor from the left margin to the next indent area.

autowrite/aw : to disable this, we can use the command of noaw or noautowrite. If we have it enabled, it does not ask us when we wish to export the working buffer,
as in terms of writing the contents of it to another file or so - It simply does it.

flash/fl : can be deactivated with noflash. Normally, when we give an errornous command, the editor beeps. This changes that function to be so that the screen flashes, instead.

ignorecase/ic : ignores casing in searches. can be disabled with noincsearch.

list : This command lists characters that would normally be invisible, such as tabs (^|) or end of lines with $. Can be turned off with nolist.

laststatus=n / ls=n : can be disabled with giving the number 0. Having the number 1 displays the options if two files of vim are active, 2 always displays it.
This command displays a status line that shows if the file has been edited since it's last outwriting, in which case it has, it's displayed with a [+]
and the position of the cursor.

number/nu : displays line numbers. This is by default turned off. To turn it off again, you can give the nonumber command.

wrap : If this is activated, it makes it so that long text is wrapped around. to turn it off, give the command of nowrap.

wrapmargin=nn/ wm=nn : This is the wrapmargin, and the provided number is the amount of distance automated in terms of formatting. To turn this off, provide it with a 0
argument. 

report=nn : This is by default set to 5. It defines a function that shows reporting status of X amount of lines. As such, when we delete lines etc. this utility is what
defines how many lines are reported about the output.

scroll=nn, scr=nn : This parameter controls the length of which it is to scroll. By default, this is half the page.

shell=path/sh=path : This command is for spawning a new shell, where you must define the path to the respective shell.

shiftwidth=nn, sw=nn : This is the command for changing the shiftwidth (think of tab distance), where nn is the length. THe default is 8.

showmatch/sm : This parameter, when in input mode, if you write a closing ) or }, it briefly jumps to any such match if it finds one. can be disabled with noshowmatch

showmode/smd : When active, displays the mode which vim is in (input mode or command mode). set to noshowmode to disable.

compatible/cp : this is to make vim compatible with vi. It is not, inheritly. To turn it off, just write nocompatible.

wrapscan/ws : Turns off the wrap-around of scanning. is turned off with nowrapscan or wrapscan.

When we are talking about lines, we can use something called markers. markers are arbitrary bookmarks of which we can use to refer to lines.

The way we set a marker, is through the method of m<letter of the marker>. When we declare a marker, we do it for the current line we are operating on.
Thus, if we are on a line and we write mt, it will make that line out to be a marked page called t.

Markers are not saved between sessions.

To refer to a marked place, assuming it has not reset or been deleted, we give the command of '<letter of marker>, so for instance, to go to the place of t, we can give
the command of 't - that will bring us to the place of the marker t.

if we want to go to the exact position of the marker, we can give a `<letter> command, instead.

And when we wish to delete all the contents from the current line to the point of the marker, we can do the following command:

d'<letter>

likewise, if we were to do:

d`<letter> - we would delete from current line, up until the exact position of the marker.

we can use markers as references of adresses, instead of numbers, if we will it:

:'m,.s/The/THE/g - This will replace The with THE on all the lines that span from the m markers line to the current line.

if we wish to edit another file, we can use the e command, as follows:

:e[!] [filename] - the ! is if we do not want to save our contents, otherwise we have to save our contents by using the :w command

if we wish to restart the editing session, we can give a :e! command. If we do, the current working buffer is reset to the last saved 
state, be that which you opened it in or the one that you last wrote it out.

Either case, when this operation is commited, the named and the general purpose buffers are untouched. Meaning we can save things in these, close
the current editing session, switch to another file, and have the info from the numbered buffers still intact to use in the current working one.

thus, :e! - Closes current editing session. Does not delete general purpose buffer or named buffers

if we wish to close the current file we are working with, and open the last one we edited, we can use the following:

:e# - it returns us to the cursors positon of where we last edited in that file, that we open with this command.
if we do not save the current file we are working on, we are prompted to do so - assuming the autopromt is on.

The autopromt can be turned off with noautowrite

if we wish to edit the next upcoming file, we can use the :n command. if we wish to rewind back to the first file, we can use the :rew command.
To force vim to close a file without saving, we can use the :n! command.

If we want, we can map keycommands to different sets of macros. For instance, what follows is a command that maps ctrl+x to the commands of that
it will find the next left bracket on the current line and delete all characters until the next right bracket on the same line, delete the next character,
move the cursor down two lines(2j), and finally move the cursor to the beginning of the line(0):

:map ^X f[df]x2j0

To see all of our current mappings, we can just write :map, to get a list of all the current ones

we can also define our own abbrevations in input mode, as follows:

:abbrev ZZ Sam The Great //This makes it so that every time we write ZZ+Space in input mode, it will replace it with Sam The Great.
This only applies to the actual input mode, instead of the command mode, as the ZZ is only carrying special meaning in the command mode.

We can execute shell commands from within vim, if we wish, as follows:

:sh , for instance, will spawns your default shell. (Usually bash or tcsh).

to return to the vim editor, we can either give a exit command or press ctrl+d to exit to the vim editor

Sometimes, :sh might not interact as we wish, and this depends on how our shell is setup. To fix this, we can acess another Shell, for instance
change to Bash from Tcsh, or vice versa. To do this, we can do as follows:

:set shell=/bin/tcsh  - this changes the shell to launch to be tcsh

When we spawn a new shell, we have to keep in mind that even tho we spawned a new shell, we are still in Vim.

we can also run shell commands in a temporary shell, which terminates when the shell command has been run, as follows:

:!<command> - where command is the shell command that we wish to execute

We can also execute a command and have the result replace a line that we have the cursor on. If we wish to do this, we could do the following:

:!!<command> - This places the results of a command unto the line of where we have placed the cursor. This overwrites, if it is not done on an empty line.

If we wish to issue commands on a broader range of parts of a file or the likes, we can declare the ! statement before making a command.

What follows, is an example of this:

!'qsort - This will perform sort on the marked section that is named q

Press enter and we get the result in a few seconds, to undo the result, we can just do u or undo.

Note: If you remove the depdencny of which the vim editor is operating on (as in it is related or working on another file currently) - it can cause the program to freeze.
Or if we enter the wrong command, we could end up destroying a file by mistake - thus, it is always smart to make a copy of the file, before we operate on it with these kinds of
commands.

When we talk about repeat, we talk about number of repeats, where we talk about the amount of elements we would like to repeat upon - this amount, is quantified by a
measure of unit, as follows:

char - just individuall characters

Word - just as the name implies, except vim considers exclamation points to be the end of a word. So for instance, pear! is two words, where pear is 1 and ! is 1.

Blank-Delimited Words - This is the same as a Word, except it does not exclude puncutation in the same format.

Line - as the name implies, a long coherent line that is not seperated by punctuation.

Sentance - Ends at puncutation, exclamation mark or ? mark followed by 2 spaces or a new line (. and then a new line)

paragraph - a set of text that followes with  1 empty line before and >= 1 empty lines after.

In vim, we can display more than 1 screen at a time, if we wish.

When we make a command, we can do a repeat factor by just putting a number before the command. if none is provided, vim automates to 1.

EMACS:

Emacs are a modeless editor, that does not require to switch between command and input mode.

Instead of giving commands with letters, we simply write with letters, in Emacs.

Emacs allows switching between buffers, without actually having to write to and from a buffer.

Emacs allows you to use Lisp (the main functioning language in Emacs), to write new commands or override old ones.

Usually, it is much more common to have someone add debugged commands and creating en environment that is setup for the given task,
more so than creating entirely new ones. (even if it is possible, with Lisp).

To bring up the textual environment of an Emac, we can use the -nw command.

To exit from Emacs, we can write CTRL+X CTRL+C - this will only work if we have one emac window up.

If we wish to start up a file in a emac, and bypass the intalled emac files (the config files), we can do as follows:

$ emacs -nw -q sample //This will open a file called sample with a text formatted emac and skips the .emacs config files, by virtue of the -q command

If one file has the specified name, it creates one in the mode of creating a new file, with that specified name.
If we start the Emac command without naming a filename, it will display with some tutorial help messages and a list of basic commands.

At the bottom of the screen, we have a "Mode Line", that shows us the current setting of the current file we are working on.
For each window, we get a mode line. The mode line also displays some info about the buffer that the window is viewing : when it was last modified,
what mode it is in etc.

If we wish to find out more help about commands, we can do CTRL+H r , then scroll through the areas with the arrow key.

When we write a command, if an error would occur from it in a Emac, it displays an error for a few seconds and then displays the command again.

If we wish to do a clean exit, as in save and all that, we can use the Ctrl+X or Ctrl+C command.
it will then ask us if we wish to save the info or not.

If we wish to simply abort, with running processes or half-completed commands, we can do CTRL+G

Normal backspace works for deletion in Emacs, but so does CTRL+D (deletes char under the cursor), Delete or Del

if we want more info about emacs, we can write info in the shell, as follows:

$ info emacs 

and then m deletion. This will give us a help document that talks about delition and if we wish to exit, we can press the q button.

The principle of moving the cursor is the same as in vi, just that the key commands differ.

Right key Arrow/Control-F - This moves the cursor one character to the right.

We can also chain commands, as follows:

COntrol-U 7 Control-F moves the cursor 7 characters to the right

Control-U 7 Control-B moves the cursor 7 characters backwards

To move the cursor forwards or backwards, we use Alt + respective key instead (Alt + F for 1 word forward, Alt + B for 1 word backwards)

Control + A - Moves the cursor to the beginning of the line

Control + E - Moves it to the end of the line

Control + P/Up arrow key - Moves the cursor one line up, identically to it's current position but one line up

Control + N/Down arrow key - Moves the cursor one line down, identically to it's current position but one line down

alt + a - Moves the cursor to the beginning of the word that the cursor is currently on

alt + e - Moves the cursor to the end of the word that the cursor is currently on

alt + ( - Moves the cursor to the beginning of the paragraph that the cursor is currently on

alt + ) - moves the cursor to the end of the paragraph that the cursor is currently on

alt + r - moves the cursor to the beginning of the middle line of the window

You can also chain commands, as follows:

CTRL + U + 0 + alt + r - This moves the cursor to the beginning of the first line, as the number here implies line number.

Writing and Deleting is as per usual.

The buffer (the contents of the file), is not saved to the file until explicitly told so. If you quit a work without saving,
all the work is discarded.

Whenever we make backups of a Emac, we will find that we can either do 0, 1, n.

When we actually do a backup, we will find that the naming of the file is as follows: <filename>~ //This is the first level of backup, as in, only one backup
If we do several backups of a file, we get the following format:

<filename>.~n~ - Where n is the iteration number of that specific backup

To save the data of the buffer, we can use Ctrl+S or Ctrl+X

if we wish to copy our current working file into another one, we can use the command of CTRLD-X + CTRL-F

When we refer to the documentation for E-macs, we use the following list of notations and the likes:

a - This is just a normal coresponding lowercase character

A - same as per usual

C-a - This is the notation for Ctrl+a

C-A - This is the notation for CTRL+A

Meta-a - This is the notation for Alt+a

Meta-A - This is the notation for Alt+A

Control-Meta-a - CTRL + Alt + a

Meta-Control-a - ALT + CTRL + a

We can also enter commands by virtue of Alt+x, where we can instead enter a commands name and the emac will run it.

The command giving is also supported by "smart-complete", which is so that if we write tab after a word or space, it will try to autocorrect to what you meant.

To give a numeric quantity argument to commands in Emacs, we use the following technique:

if we want to give a repetition of 10 z's, for instance:

alt+1 alt+0 z - This will spell out 10z and give us that command

or we can use the CTRL+U command to access writing prompt for how much of a numeric argument we'd like to give:

CTRL+U + 20 + alt+f //This will give the command of 20f, which is 20 steps forward on the cursor

if we wish to scroll pages in an emac, we can use the page up/page down characters to move one page at a time, or ctrl+V/alt+v for down/up

If we wish to clear out the screen and rerender it, we can use the CTRL+L command.

There are two different states of delition in Emacs, Kill and Delete. Delete actually permanently deletes things, where as of kill,
yanks the command into a "holding pen", where we could access it later.

if we wish to move to the beginning/end of the buffer, we can use alt+</alt+> respectively

There are a number of ways we could search a file ; incremental searches, sequential searches and regex searches.

if we want to do a forward incremental search, we can do the CTRL+S and if we want a decremental search (backwards), CTRL+R is what we want.

When we do this, the Emac will prompt us with the requirement of having to write in each letter - For each letter we do, we will find that
we can see the result of our search for each letter we enter, increasing the reach and the extent of the search, based on each key we type.

To quit a sequential search sequence, we can give the ctrl+G command.

To give a non-incremental search (as in, a normal search) - we can give the command of CTRL+S or CTRL+R  and Return

if we wish to involve regex searches into the fray, we just gotta press the alt button before evrything, so, if we want a incremental search forward, we can do:

alt + ctrl + s - This prompts for a regex and then starts performing the search as you go.

To acess the menu in a non -nw mode (i.e, gui mode enabled) we can press F10.

We can cancel the menu by Control+G or just hammer ESCAPE 3 times.

When we select things from the menus, we can use the key shorthands to select the subsection, we need not provide Return.

To get the help window up, we can just do CTRL+H

To move forward or backwards in the buffer that contains the help window, we could use either the CTRL+V or Alt+V keys

We can also delete windows by numeric predecense as follows:

CTRL+X + 0 will delete the current window, CTRL+X+1 will delete the "1:th" window.

if we access the help menu, we have to provide what menu or options we are seeking help regarding, and then we can just simply press the ? button
to get a list of all available ones that we want to see.

On many terminals, the backspace or left key arrow, generates CTRL+H, which means that we must keep in mind, if we enter the Help menu,
we can always just exit it with CTRL+G

What follows, is a list of help commands that we can supply:

CTRL+H + a - Prompts for a string and displays a list of commands whose name contains that string.

CTRL+H + b - Contains a long list of commands that are in effect.

CTRL+H + c + <key-sequence> - multiple key sequeneces are allowed - and what this command does, is that it displays the name of the command bound to the key sequence.

CTRL+H + f - Prompts the name of a Lisp function and displays the documentation for it. We can use command names with this, since commands are lisp functions.

CTRL+H + i - Displays the top info menu where you can browse for emacs or other documentation

CTRL+H + k + <key-sequence> - Displays the name and documentation bound to the key sequence

CTRL + h + l - Displays the last 100 characters written.

CTRL + h + m - Displays the documentation and special key bindings for the current Major mode

CTRL + h + n - Displays the emacs news file, which lists recent changes to emacs, ordered with the most recent changes first.

Ctrl + h + r - Displays the emacs manual.

CTRl + h + t - Runs an emacs tutorial session

CTRl + h + v - Prompts for a Lisp variable name and displays the documentation for that variable.

CTRL + h + w - Prompts for a command name and identifies key sequences bound to that command. Multiple key sequences are allowed.

What follows is a list of compatible lisp related help commands that we can find out more about:

backward

dir

insert

previous

view

beginning

down

kill

region

what

buffer

end

line

register

window

case

file

list

screen

word

change

fill

mark

search

yank

char

find

mode

sentence

defun

forward

next

set

delete 

goto

page

sexp

describe

indent

paragraph

up

Every buffer keeps it's own record of the last 20k pressed characters, and acts as a "undo buffer", where we can access the text and undo it, as long as we are within
the reach of 20k chars.

The three states of which are possible for a Emacs file, is -- (unmodified), ** (modified), %% (readonly)

To undo a command, we can do as follows: CONTROL + _  //This Undoes a command

Control + _ + CTRL +F + CTRL + _ //Undoes the last change and then changes it back again

CTRL + _ + CTRL + _ //Undoes two last changes

CTRL + _ + CTRL + _  + CTRL +F + CTRL + _ + CTRL + _ //Undoes two changes and changes them both back again

CTRL + _ + CTRL + _ + CTRL +F + CTRL + _ //Undoes two changes, and changes the last one back again

If we completely wrecked the buffer, we can just re-read it from a file, as follows:

Alt + x + revert-buffer //This will ask for confirmation to wether you wish to re-read the contents of a file into a Buffer

A point, is a place of where we are currently editing the buffer.

And we can also set a Marker, called Mark in the buffer. This creates a region between the Point and the Marker, which a lot of commands can
interact with.

Each buffer has only one mark, and the only way to move a mark is by setting a new one.

To set a mark, we use the command of CTRL+@ or CTRL+SPACE

To move back to a mark that we have set, we can give the CTRl+X CTRl+X command

What follows are some commands to regions:

alt+w - Copies region to the Kill Ring (local buffer for "non-deleted yanks") without any deletion

CTRL+W - Kills region

Alt+x print-region - Sends Region to the printer

alt+x + append-to-buffer - Prompts for a buffer and appends the region to specified buffer

alt+x + append-to-file - Prompts for a filename and appends Region to that file

alt+x + capitalize-region - Converts region to uppercase

CTRL+X+CTRl+L - Converts a Region to lowercase

When we set a Mark in a buffer, we push it into a locally held list of the last 16 entries of previous markers. We can access these and jumps to said
position of Markers, by going into the list.

If we wish to move back in the entries of Marks, CTRL+U + CTRL+@, which moves the Point to the previous Mark in the list, which makes it the oldest one,
and then pops out the most recent one

Some commands set a mark automatically, such as ALT+> (Jump to end of buffer)

A kill ring in an Emac, has 30 spots. It is akin to a area that "Non-deleted delete" text ends up being in.
The kill ring is visible from all buffers.

To retrieve the text from the Kill ring, we have to yank the text out of it.
So, basically Killing and Yanking is Copy and Paste, in other words.

What follows are some commands that interacts in ways of killing, when it comes to text:

alt+d - Kills to end of current word

alt+D - Kills from beginning of previous word

CTRL+K - Kills until end of line, not including LINEFEED

CTRL+U + 1 + CTRL+K - Kills until end of line, including LINEFEED

CTRL+U + 0 + CTRL+K - Kills from beginning of line

alt+w - Copies region to the kill ring but does not erase the text from the Buffer

Ctrl+W - Kills a region (cuts it out)

alt+z + <char> - Kills up to the next occurence of char

CTRL+Y - Yanks most recently killed text into current buffer at point, sets Mark at beginning of this text,
and positions point and the cursor at the end; follow with CTRL+Y to swap Point and Mark

Alt+y - Deletes the just yanked-text, rotates the Kill Ring, and yanks the next item. (NOTE: ONLY WORKS WHEN CTRL+Y HAS BEEN RUN OR Alt+Y HAS BEEN RUN)

Thus, if we want to yank something from the ring, we can use the Ctrl+Y followed by alt+y to cycle through the entries of the kill ring

The pointer is not reset until the next kill command occurs, thus you can cycle through the KIll ring, do some commands on each entry, and then just continue with 
business as per usual.

We could also give a numeric positon to the Last Yank pointer, if we wish.

To insert special characters as a string character, we must use the escape sequence of CTRL+Q in Emacs, followed by whatever we wish to type.

Either that, or we can do CTRl+Q followed by 3 octal digits that represents a byte with that value in the BUffer.

NOTE: Some setups makes Control+Q to crash, due to it clashing with software flow control. Thus, it's not always possible to use it.

Emacs do not have the global buffer access in the same way as Vim, as we do not have the global notation of g, thus, to access an entire buffer,
we just use the pointer to mark an entire area.

What follows are some commands for interacting with Regex, as follows:

Alt+x occur //this will prompt for a regex to put the matches it finds into a Buffer called occur

alt+x delete-matching-lines //Prompts a regex and deletes all the lines that it finds to be a match.

alt+x delete-non-matching-lines //Prompts for a regular expression and deletes each line that does NOT have a match for the expression

By utilizing occur this way, we could create a buffer that keeps references to all kinds of files that we could just pillage through and search for different
things.

The process of this would be something according to the following:

alt+x occur 
CTRL+X o //jumps to the buffer
Move cursor to the relevant position
CTRL+C CTRl+C //jumps to the selected line in the selected buffer.

Generally, in Emacs, there are two different kinds of ways to replace text. Which is interactive and unconditional.

Unconditoinal, does it without asking for permissions, assuming it's been run with a pattern to match,
whilst a interactive allows us to seek confirmation for each step.

What follows are some of the replacement commands we can do:

alt+x replace-string - Prompts for String and newstring and replaces every instance of String with newstring. Mark is set when you give this command, so you can
always return ot the point of where you were at when this command were given, with the command of CTRL+U CTRL+@

alt+x replace-regexp - Prompts for regexp and newstring and replaces every match for regexp with newstring. Point is left at where it was done for last replacement
command (this holds true for the above command as well), but mark is set when this command is given. Can access it again with CTRL+U CTRL+@

alt+% string / alt+x query-replace 	- 	This first form uses string, the second form promts for a string. Both forms prompt for newstring, query each instance of
String and depending on your reponse, replace it with newstring, Point is left at the site on the last replacement, but mark is set when this command is given.
Can return to it with CTRL+U CTRL+@

alt+x query-replace-regexp - Prompts for regexp and newstring, queries each match for regexp, and depending on your response, replaces it with newstring.
Same as before, point is left at last point of replacement, whilst the Marker is set upon this command being given. Can return to this with CTRL+U CTRL+@

When we do the interactive replacement, the program expects some kind of response, as follows, we can see the different return commands that we can give:

RETURN - Do not do any more replacements: quit now.
SPACE - Make this replacement and go on.
DELETE - Do NOT make this replacement, and go on.
, - Make this replacement, display the result and ask for another command.
Any command is legal, except for Delete, which is accounted for as a space and does not undo a change.
. - Make this replacement and quit searching
! - Replace this and all remaining instances without asking any more questions.

To pwd or cd to a wanted directory, we could do as follows:

alt+x+pwd or alt+x+cd. Each respective one brings up a prompt to give the information for the new locations etc.

When we speak about reading into a buffer, we speak about "visiting" files, this is emacs version of "Calling up" a file.

What follows, are some commands in regard to handling Visiting of Files:

ctrl+x Ctrl+F - Gives a prompt for a filename and reads its contents into a new buffer. Assigns the file's simple filename as the buffer name.
Other buffers are unaffected, it is common practice and often useful to have several files open simultaneously for editing.

Ctrl+X Ctrl+V - promts for a filename and replaces the current buffer with the contents of the file. The current buffer, is destroyed in the process.

CTRL+X + 4 + Ctrl+F - Prompts for a filename and reads its contents into a new buffer. Assigns the file's simple filename as the buffer name.
Creates a new window for this buffer and selects that window. The window selected before the command still displays the buffer it was
showing before this operation. Although the new window may cover up parts of the old window.

If we wish to create a new file, we just need to call it up, or visit it. It creates a new file and allows us to edit that and save it.
If we gave the wrong name, we can give the command of ctrl+X + ctrl+V to get a prompt to rename it.

If we wish, we can cycle through different paths when we are in the context of the minibuffer and we are attempting to direct a path, with Tab.
This will make the Emac try to autocomplete our request. There is no guarantee that it can do it, but in most cases, it will be able to do it.

if we type ? or press tab again, we will find a complete list of all the things it can autocomplete to.

To cancel the pathname completion, we can go CTRL+G or Escape Escape Escape, this returns us to the state of which we were earlier.

To go into the tabs we can do this by the virtue of page up and then move with the arrow keys. If we wish to exit that form, we can go ctrl+G
or escape escape escape.

To exit the minibuffer, we just press enter.

To exit a file without a warning, we can do the alt+~ command

When we wish to save, we give the command of Ctrl+X + CTRl+S

If we wish to save several workbuffers, we can give the command of CTRl+X + s- which gives us a prompt for every single buffer that has been modified
that we are trying to close.

alt+x set-visited-file-name - Prompts for a filename, sets it as the "original" name for the current buffer.

CTRl+X + CTRL+W - Prompts for a filename, sets this name as the "original" name for the current buffer, and saves the current buffer
into that file. Equivilant to alt+x set-visited-file-name followed by CTRl+X + Ctrl+S

alt + ~ - Clears the modified flag from the current buffer. if a file is not considered modified upon exiting of emac, it discards all of the
changes in the file, if there are any to the previous original state of the file. 

A buffer in an Emac, is a Storage Object that has all kinds of related values to it, and can range from facts such as special keys, its own mode,
its own file association etc. etc.

In context of buffers, there must always be a buffer which is the current buffer that you are working with. When we have a current buffer, that is the
buffer that we are working on.

What follows, are some commands in regards to buffers:

Ctrl+X + b - This prompts for a buffer name, if it does not exist, it creates it instead.

Ctrl+X + 4 + b - Prompts for a buffer name, creates a buffer window and selects it.

Ctrl + x + Ctrl + b - Creates a buffer named Buffer List and displays it. The new buffer is not selected. as for as symbols go for buffers,
% is a readonly buffer, * is a modified buffer and . is for the selected buffer.

alt+x rename-buffer - Prompts for a buffer name and gives the current buffer the provided name

CTRL+ x + CTRL + Q - Turns on/off readonly mode for the current selected buffer.

alt+x append-to-buffer - Prompts for a buffer name and appends Region to the end of that buffer.

alt+x prepend-to-buffer - Prompts for a buffer name and prepends Region to the beginning of that buffer.

Alt+x copy-to-buffer - Prompts for a buffer name and deletes the contents of the buffer before copying Region to that buffer.

alt+x insert-buffer - Prompts for a buffer name and inserts the contents of that buffer in the current buffer at Point.

CTRl+X k - Prompts for a buffer name and deletes that buffer. If the buffer has been modified but not saved, emac asks for confirmation.

alt+x kill-some-buffers - Goes through the list of buffers, and asks for confirmation for each if it is to be killed.
Same rules as for CTRl+x k applies

if we wish to display a different buffer in the current window, we can use the command of :

ctrl+x + b + <buffer name>

We can also split windows, as follows:

Ctrl+x+ 2 - this splits the current window in two, with one new window appearing above the other. A numeric argument is accepted,
which reflects the amount of lines allocated to respective window.

If we were to give the command of Ctrl +x + 3 , we'd find that we put the window in two, side by side.

We can further customize each window then, with viewing another buffer or the likes, depending on what we wish to do.

To select the other window, we can use the command of : ctrl+x o, which cycles through the windows.

You can also scroll through the other window, with the alt+ctrl+v command

If we want to open another window with contents, we could choose between the following:

ctrl+x 4b - This command prompts for a buffer name and opens it in the other window.

ctrl+x 4f - This command prompts for a file name and opens it in the other window.

Ctrl+x 0 - Deletes the current window

Ctrl+x 1 - Deletes all windows except for the current one

With these commands, no data is lost, and we can just make another window if we feel like it.

We can also manipulate the size of windows, with the following commands:

alt-x shrink-window - This will shrink current window by 1 row

To increase height, we can do the ctrl+X^
To increase width, we can do the Ctrl+X}
To shrink width, we can do the Ctrl+X{

In emacs, we can run a subshell, that allows us to do commands and interact with the buffer that we are in.

What follows, are some commands on what we can do:

alt+! - Prompts for a shell command, execute it and displays the output

CTRL+U alt+! - Prompts for a shell command, executes it and puts the output to directed Point

alt+| - Prompts for a shell command, gives Region as input, filters it through the command and displays the output

CTRL+U + alt+| - Prompts for a shell command, gives Region as a input, filters it through the command, deletes the old Region
and inserts output in that position.

Emacs can also run a subshell that is constantly running in it's own buffer.

We can run processes in the background and feed the output to a buffer, which is always going to be the Compilation buffer.
To send operations to the background and show results of SHell commands, we can do the following:

alt+x compile - prompts for a shell command and runs the given command in the background.

if we wish to close the window we could just call ctrl+x 0 on it if it's current, ctrl+x 1 otherwise 

To switch back, later, to the compilation buffer, we can give the command of CTRl+X b

To kill background commands, we can give the command of alt+x kill-compilation
this will prompt a confirmation, and if we accept, it will kill the background process

To see the next occurence of a error, in the compilation buffer, we can give the following command:

Ctrl+X `

In emacs, we have a few distinct classes of Major Modes, that are defined after a purpose.

There is one for programming languages (C, Lisp, Fortran)
There is one for human reading (txt etc.)
There is one for special purposes (shell, mail etc.)

and there is one for Fundamental, which does nothing special.

Every mode have special interactions with it's rules, with it's way of customs and what commands work there.

To select a specific major mode, we can provide the following command:

alt+x <modename>

To make a file define it's own mode name, we can add the line of -*- <modename> -*- somewhere in the file,
which causes the file to be accounted for as it's own major mode.

What follows are some of the rules for the human reading mode:

alt+f/alt+b - forward/backwards 1 word, both commands accept a numeric argument to say how many words you want to move

alt+d/alt+DELETE - kills words backwards/forwards - Accepts numeric arguments for amount

alt+a/alt+e - Moves backwards to the beginning of a sentance/moves forwards to the end of a sentance

Ctrl+X + DELETE - This kills back to the beginning of the sentance - Accepts numeric arguments

alt+k - Kills until the end of the sentance - Accepts numeric arguments

alt+{ - Moves backwards to the most recent paragraph beginning - leaves point at beginning of line - accepts numeric argument for quantity

alt+} - Moves forward to the next paragraph ending - leaves point at the beginning of line - accepts numeric argument for quantity

alt+h - Marks from point until end of current paragraph as a region, or jumps to the next paragraph, if it is not in one currently

When we are on the subject of Paragraphs, there is a function that allows us to Fill paragraphs. This auto-formats text to fit the
format of the paragraph.

To turn this mode on, we can give the command of : alt+x auto-fill-mode

To be able to fill a region that we already have set parameters for, we can give the following command:

alt+x fill-region or alt+q allows us to Auto Fill a paragraph region that we have selected with a Point and a Marker

We can also change the filling width, if we please, as follows: CTRl+X f , this will fill ut to the current position.

We can further manage the size, if we please, as follows: ctrl+U + <number> + ctrl+X + f to set the fill columns to the number argument,
where number is the left margin.

We can also force uppercase or lowercase for entire regions, as follows:

alt+l - Converts word to the right of Point to lowercase
alt+u - Converst word to the right of Point to uppercase
alt+c - Converts word to the right of Point to initial caps
ctrl+X + Ctrl+L - Convers region to lowercase
CTRL+X + Ctrl+U - Converts region to uppercase

What follows are some examples of case conversions:

HELLOMETA--META-l(lowercase "l") - Gives hello
helloMETA--META-u - Gives HELLO
helloMETA--META-c - Gives Hello

In textmode, all of the basic functions of Human language exists, and we have some extension beyond that, as well, as follows:

alt+x text-mode - This turns on text mode, allowing us to use the tabbing function for next

In Text Mode, tab has a function called tab-to-tab-stop. By default, every stop is set at 8 columns distance.
If we wish to edit the occurence of the stops, we can enter the edit mode for said "Stop Buffer", with the following:

alt + x + edit-tab-stops

To install the new buffers, we must give the command of Ctrl+C Ctrl+C
We can kill this new buffer, with the following command: CTRl+X k 
or we can switch away from it, with the following: Ctrl+X b - This changes away from the buffer, without changing the stops

If we "overshoot" the amount of characters, we find that the Emac fills the remaining void of characters.

We have another mode, as well, called C mode.
This mode is designated to handle the formatting of programming languages.

There are several modes that supports all kinds of languages. What follows, is only about the C language:

All of the following expressions accept numeric inputs and runs backwards if given a negative number. Beyond this,
the C mode supports the normal set of parameters such as ('s and {'s etc. But it does not support full Syntax analyzis.
As such, linting does not exist in this mode.

What follows, is a list of commands for C mode:

Ctrl+alt+f - Moves forward one expression. 

Ctrl+Alt+b - Moves backwards one expression.

Ctrl+Alt+k - Kills an expression forward. 

Ctrl+alt+@ - Sets mark at the position of where CTRL+alt+f would move it - but does not change pos of Point

When we speak of functions in terms of Cmode, we talk about it being a defun, which means that the entire function parameter set
is set to the left ledge of the formatting.

Ctrl+Alt+a - Moves to the beginning of the most recent function definition

Ctrl+alt+e - Moves to the end of the next function definition

ctrl+alt+h - Mark as region, the current function definiiton, or the next function if the cursor is between two functions
This method sets up a entire function region for commands such as kill

In terms of indention, we can do the following:

Tab - This creates indention for the current line. 

LINEFEED - Shorthand for Return followed by a Tab. This is mostly a convinience for new code.

Ctrl+Alt+q - Reindents all lines inside the next pair of matched braces. It assumes the left indent is correctly done and drives the indent from there.

Ctrl+alt+\ - Reindents all lines in a region.

Each buffer in this mode, has it's own comment-column variable, which we can view with CTRl+H v comment-column 

What follows, is a list of commands that is relevant to comments:

alt+; - Inserts a comment on the current line or aligns an existing comment. 

CTRl+X+; - Sets comment-column to the column after Point. 

CTRl+U - CTRL +X  - Kills the comment on the current line. 

Ctrl+U Ctrl+X - Sets Comment-column to the position of first comment found above this line and executes a alt+; command

There are also some special-modes that do not fall under any of the other categories of doing, and they are as follows:

Rmail - For editing mail

Dired - moves around an ls -l display and operates on files

VIP - Simulates a complete vi environment

VC - Allows you to drive version-control systems from within emacs

GUD - Allows you to run and debug C code within the Emac (and other code as well)

Tramp - Allows you to edit files on any remote system you can reach with ftp or scp

Shell - Runs an interactive subshell from inside an emacs buffer

Shell Mode:

To start a sub-shell, give the command of : alt+x shell 

To start a second shell, we must first alt+x rename-buffer to rename the first buffer, and then spawn a second buffer, with alt+x shell

We can give some special commands to the shell, as follows:

RETURN - if point is at the end of a buffer, it returns the last line. Else, it returns the current line till end of line.

CTRL+C + CTRL+D - Sends CTRL+D to the Shell or it's Subshell

Ctrl+C + Ctrl+C - Sends Ctrl+C to the SHell or it's subshell

Ctrl+C + Ctrl+\ - Sends a quit signal to the shell or its subshell

CTRL+C + CTRL+U - Kills the text on the current line not yet completed.

CTRl+C + Ctrl+R - Scrolls back to the beginning of the last shell output, putting the first line of output at the top of the window

Ctrl+C + Ctrl+O - Deletes the last batch of the SHell output

We could customize the key settings of the inner workings of the Emac, where it has a Lisp Interpeter written in C.
Which would allow us to remap commandos etc.

We can also cusotmize the Lisp code which exists in the startup file of the Emac, which is in the .emacs startup file.

To find it, we can find it under ~/.emacs

We could also ignore running it, with the -q command or the -u option which uses the ~user/.emacs bootup file,
it is generally related to key bindings and options settings.

The most common function in the emacs boot up file, is the setq command, we can illustrate how to do options in a function
call, as follows:

(setq c-indent level 8)

The command of ctrl+H v displays the value of the variable

We can also set the default value of something, by calling the setq-default method

To set the specific element of a vector, we can use the aset command.

There are a number of rules to formatting of this, as follows:
The first argument is the name, the second is the offset, and the third is the value of the target entry.
In the new setup, the values are usually Constants, and they are as follows:

Numbers - Decimal integers, with an optional minus sign
Strings - Similar to C String but extensions for CTRL and Alt
Chars - Not like C chars, starts with ? and continue with printing char, so to get "a" for instance, we'd have to do ?a
Booleans - Not true and false, use t for true and nil for false.
Lisp objects - Begin with a single quatation mark and follow with the objects name, so for instance "ball

Bash:

Bash builds itself on a number of shellscripts. Shellscripts are programs in of themselves.

You can choose what Shell you wish to use, with the command of chsh <name of shell we wish to use>.

There are a few shells, as follows:

sh Shell - The original Bourne Shell, uses as an addon to load scripts or ensure some scripts in Unix for instance, as it can be 
present with a symbolic link to assure that operations that require bash still can run

dash Shell - a Smaller shell than Bourne, but it allows much faster loading.

Korn Shell - adds some features beyond the normal Bourne Shell

When we change a shell to another, using the chsh command, we have to supply a absolute path to the shell directory, they are as follows:
/bin/bash or /bin/tcsh etc.

We must have read access to the startup file to run the respective Shell we want to have.

There are a few number of different types of shells, as follows:

Login Shells, Interactive Nonlogin Shells and Noninteractive Shells.

The shells that we start with bash --login, for instance, have the following commands:

/etc/profile - A user with Root priveleges can modify this to make the ground foundation of all other user interfaces and how they will interact, based on this.

To override the normal commands in Profile, we can use the fact of what Bash will do next:

Bash will run the following, after /etc/profile:

Login Shells:

//NOTE: ~/ is shorthand for home directory
~/.bash_profile 

~/.bash_login

~/profile

Since it will run them in that order, if we wish to make it so that each file can run something else or will do something else, to overwrite the original set
of the default profile, we simply modify one of these files, as bash runs them in that order after the original profile file

A shell that is run on a Vm, does not execute commands in these files.

When it logs out, it runs the utility in the ~/.bash_logout file.

Interactive Nonlogin Shells:

Albeit not run directly by the bash, many of the ~/.bashrc files call /etc/bashrc, this allows a user with Root privelges to change the
default characteristics for all Nonlogin Shells.

A interactive nonlogin shell executes commands in ~/.bashrc . Typically a startup file for a shell, such as .bash_profile, runs this file,
thus, both login and nonlogin shells run the commands in .bashrc

Non-interactive Shells:

The previous files are not run by Non-interactive Shells. Non-interactive Shells look for the file called BASH_ENV and run their commands from there.

Albeit many startup files exist, usually the ones we need, are the following:

.bash_profile and .bashrc in our home directory.

Commands similar to the following, are run in the .bash_profile from the .bashrc file:

if [ -f ~/.bashrc ]; then . ~/.bashrc; fi

the [ -f ~/.bashrc ] checks to see if the bashrc exists in our home directory.

Since other shells inherit from the bashrc and commands can be run many times from the bashrc, it's better to put some variables in the .bash_profile,
cause that way we can just reuse the exported values.

for example, the following command adds the bin subdirectory to the home directory of Path and should be in .bash_profile:

PATH=$PATH:$HOME/bin

Thus, it's only run once. In addition, changing in bash_profile applies to other subshells, whilst modifying the .bashrc overrides changes inherited from
a parent shell.

When we set variables in a bash program, we must export them, to have child process be able to have access to them. What follows, is an example
of what would see in a .bash_profile:

$ cat ~/.bash_profile
if [ -f ~/.bashrc ]; then
	. ~/.bashrc 			//Read local startup file if it exists
fi
PATH=$PATH:.				//add the working directory to the path, since this is usually already set in Profile, then we must not export it
export PS1='[\h \W \!]\$'	//Set prompt, PS1 is for User prompt

What follows, is an example of a .bashrc file:

$ cat ~/.bashrc
if [ -f /etc/bashrc ]; then
	source/etc/bashrc 			#read global startup file if it exists
fi

set -o noclobber 				#prevent overwriting
unset MAILCHECK 				#Turn off the "You have new mail notice"
export LANG=C 					#Set the variable of LANG to C and export it
export VIMINIT='set ai aw'		#Set vim options
alias df='df -h' 				#Set up aliases
alias rm='rm -i' 				#always do interactive removes
alias lt="ls -ltrh | tail" 		#assign so that lt means read but with ltrh options and tail option on
alias h='history | tail'
alias ch='chmod 755'

function switch()  				#a function to switch the names on two files
{
	local_tmp=$$switch
	mv "$1" $tmp 				#rename with mv, and rename the first to tmp
	mv "$2" "$1"				#rename second one to the name of the first file with placeholder value
	mv $tmp "$2"				#rename tmp to the placeholder value of second file
								#The $'s means positional parameters
}

we do not need to log in and log out to have the changes take effect in the .bashrc file.
We can instead run the . command or the source command. 
This will run the script itself. This is not something you can only do for .bashrc files, but
it is not very safe to run it on other files, as it may have sideeffects that you don't want to have.

An example follows on how to use the . command:

$ cat ~/.bashrc
export TERM=vt100 				#set the terminal type
export PS1="$(hostname -f):	" 	#set the prompt string
export CDPATH=:$HOME 			#add home to CDPATH string
stty kill '^u' 					#set kill line to  ctrl+u

$ . ~/.bashrc
bravo.example.com:

There are a number of commands, that derive from symbols, as follows:

() - this is the command for Subshell

$() - This is for Substitution

(()) - Arithemtic evaluation; A synonym for let (use when the eclosed value constains an equal sign)

$(()) - Arithmetic expression ; Not for use with an enclosed equal sign

[] - The test command

[[]] - Conditional Expression; Similar to [] but adds string comparisons

Just as we can redirect info, we can redirect errors. This is to ease the process of being able to distinguish what 
info is coming from where or what is related to what.

Whenever we have a file that gets a input or stores it's output somewhere, it puts it through the file descriptor.
In general, when a Linux program runs, it opens 3 file descriptors:

standard input

standard output

standard error

The > symbol which we used for redirect or < respectively, are actually shorthand for 1> (>) and <0 (<).
The redirection for standard error, is 2>, it sends it to the Error output.

What follows, are some examples of redirection:

$ cat y 
This is y
$ cat x 
cat x: No such file or directory

$ cat x y
cat x: No such file or directory
This is y.

$ cat x y > hold
cat: x: No such file or directory
$ cat hold
This is y.

What follows, is an example of the consequence when we redirect one direction into a certain formatting, whilst keeping the other neutral:

$ cat x y | tr "[a-z]" "[A-Z]" //The tr command translates something via a regex match to be another set of things, here it matches lowercase to uppercase
cat: x: No such file or directory
THIS IS Y.

What follows, is an example of redirecting error and standard outputs:

$ cat x y 1> hold1 2> hold2
$ cat hold1
This is y.
$ cat hold2
cat: x: No such file or directory

If we wish to redirect both standard output and error output into one file or place, at the same time, we can do as follows:

$ cat x y $> hold
$ cat hold
cat: x: No such file or directory
This is y.

In the following example, we'll see 1> redirect standard output to hold, and then 2>&1 declares file descriptor 2 to be a duplicate of
file descriptor 1. As a result, both standard output and error output, direct to hold.

$ cat x y 1> hold 2>&1
$ cat hold
cat: x: No such file or directory
This is y.

If we would have switched the ordering, we'd find that standard error would have been made a duplicate of standard output before 
standard output was redirected to hold. Only standard output would have been redirected to hold, in that scenario.

In the next example, we redirect descriptor 2 to be a duplicate of file descriptor 1 and sends the output for file descriptor 1 through
a pipe to the tr command

$ cat x y 2>&1 | tr "[a-z]" "[A-Z]" //Make Descriptor 2 a duplicate of 1 and put the output through tr to Translate it to uppercase output
CAT: X: NO SUCH FILE OR DIRECTORY
THIS IS Y.

Since usually scripts do not direct their output to the standard output, we can use this to our advantage, by virtue of redirecting standard output
to something else, whilst keeping standard error to the standard output. An example, of this, is as follows:

$ cat message_demo
echo This is an error message. 1>&2
echo This is not an error message.

it is possible to create another file descriptor, to dump respective data, if we wish, with the exec command. There is also scripts such as the
lnks that uses the technique of redirecting standard output and only channeling error standard output to the screen.

What follows, is a list of redirection commands:

< filename : redirects standard output from filename

> filename : redirect standard output into filename. If noclobber is not set, it creates this file, if it does not exist.

>| filename : redirect standard output to filename, even if the file exists and noclobber is set.

>> filename : Redirects and appends the standard output into filename. If noclobber is off , this creates a file with filename if it does not exist.

&> filename : Redirects standard output and error standard output into a filename.

<& m : Duplicates standard input from file descriptor m

[n]>&m : Duplicates standard output or file descriptor n if specified from file descriptor m

[n]<&- : Closes standard input or file descriptor n if specified

[n]>&- : Closes standard output or file descriptor n if specified

To have the rights to make a file executable, you need to have read and write rights to the file that contains the script.

What follows is a short example of a file containing a script and showcases what occurs when we don't have the permission to call said file:

$ cat whoson
date
echo "Users Currently Logged In"
who

$ ./whoson
bash: ./whoson: Permission denied

We cannot execute whoson as a command, because we do not have the execute permissions for it.

To make a file executable, we have to use the command of chmod

However, if we call things in Bash or Tcsh, we can write it as follows:

bash whoson //This gives a filename as an argument to bash, where bash becomes the executable and the filename becomes the argument
//When we call upon files this way, assuming they are scripts, we only need read permissions to run them, as bash simply reads them - whilst
//bash itself executes it.
//We can do the same for tcsh shell, if we please.

What follows is a repetition of how to view rights of a file and how to modify them, as follows:

$ ls -l whoson //Calls to see what the file is, and gives us contents of who has rights and what not
-rw-rw-r-- 		1 max group 40 May 24 11:30 whoson

$ chmod u+x whoson //This gives the x rights (executable rights) for u (user/admin, highest level) 
$ ls -l whoson
-rwxrw-r-- 		1 max group 40 May 24 11:30 whoson //We have now added the x right to the "User" level

Remember, when we call upon utilities, if we were to just call the utility name, it would be lost, because it would not know where to
look for it. Thus, to find the utility that exists in the cwd, we must do the following:

$ whoson //Error: Will not find it, due to unspecified location of searching, unless setup to do so
bash: whoson: command not Found

$ ./whoson //Will proceed, due to the ./ notation that denotes cwd

NOTE: To execute a Shell script program, you need both x and r rights, (read and execute), where as of for compiled binary programs, you just need
x rights, as you are not actually "reading" a binary program, you are just running it, whilst a script is something you must read and then execute

Whenever we run a script, we can specify what kind of Shell the script should be excuted by, as the program checks the first few lines first before
it runs a file and tries to identify it, as to see that it is runable, and thus saves it an attempt at running things if it cannot be run.

Alas, the part of a beginning in the script that can allow us to define what shell it should run, is !#<Absolute Path to a Shell>

When it scans the first lines and finds a match against !#, it will be expecting a Absolute Path that allows it to have a shell to run off of.

NOTE: We can run another Shell, but give the command of executing with another shell, if we wish.

Some quick examples of scripts that utilize this:

$ cat bash_script
#!/bin/bash
echo "This is a Bourne Again Shell script"

$ cat tcsh_script
#!/bin/tcsh
echo "This is a tcsh script"
set person = zach
echo "person is $person"

We can use the command of ps -f in a script to find out what shell is running that script

We can define a script that shows us all of the info, on which shell is running what, as follows:

$ cat tcsh_script2
#!/bin/tcsh
ps -f

$ ./tcsh_script2
UID 	PID 	PPID 	C STIME 	TTY 		TIME 		CMD
max 	3031	3030	0 Nov16 	pts/4 	 00:00:00 		-bash
max 	9358 	3031 	0 21:13 	pts/4 	 00:00:00 		/bin/tcsh  ./tcsh_script2
max 	9375 	9358 	0 21:13 	pts/4 	 00:00:00 		ps -f

if we omit the absolute pathname of a file after a #! statement, we get an error, saying it cannot find the file

In a Shell, a # that is not followed by ! and is the first line of the script, is a Comment, as far as the Shell cares.

When we run a Shell Script through the command line, this goes slower than if we were to do it ourselves.
An example of spawning a subshell to deal with our command, is as follows:

$ bash whoson

However, if our standard interactive shell that we are working in, is not bash, we might want to run the script by bash, as above.

When we give commands in relativity to a shell, we must seperate the commands.
We can both seperate commands and group them up.

We can seperate commands, by the virtue of ;

For instance, if we were to call the following:
$ x ; y ; z
Then each will be run seperately, and it won't be an issue.

The difference in writing:

//Nr 1
$ x ; y ; z //NOTE: the ; seperating commands, do not require space in any shape or form

and

//Nr 2
$x
$y
$z

is that, nr 1, only gives a prompt after z finishes, where as of Nr 2 makes a prompt after each command finishes

to continue a long command, we can provide the \ command

What follows are three examples highlighting the differences of \ and how it interacts with things, as follows:

$ echo "Please enter the three values //A lack of a \ escaping character, means that it will simply interpt it without the newline char and no \ involved.
> required to complete the transaction."

will give

Please enter the three values
required to complete the transaction.

$ echo "Please enter the three values \
> required to complete the transaction."

will give

Please enter the three values required to complete the transaction. //This, escapes the newline > sign

$ echo 'Please enter the three values \ //Setting it in ' quotation marks instead of "s, makes it take it literally.
> required to complete the transaction.'

will give

Please enter the three values \
required to complete the transaction.

When we have a lineup of commands, such as $ x | z | y , this accounts for as being 1 job, as they are dependant upon each other.

This would also apply to the following example:

$ ls -l | grep tmp | less



$ d & e & f //The & symbol after a command means that it is sent to the background.

So, in this case, we have two commands that are sent to the background and f which is not. This means, that we have three seperate jobs lined up,
where two is relegated to the backgrounds and the third one is the foregrounds.

It will display a prompt when f finishes.

Each time we give a command, it checks to see if a background job is done, and when it is, it will give a prompt of the Id of the Job 
and the command line that invoked the job.

When job numbers are listed, we find that the last job ends with a + and the previous job starts with a -

An example from the previous case, would be:

[1]- Done
[2]+ Done

//NOTE: Whenever we relegate a job to the background, we get the process ID instantly printed.

When we execute commands without piping, they account for being a seperate job, each one of em.

What follows, is an example of having a combined force of commands become one giant job that we sent to the background:

$ d | e | f & 
[1] 14295

tcsh $ d | e | f &
[1] 14302 14304 14306 //it accounts for there being 1 job and 3 processes with different ID's

if we wish to group commands, we can put them in ()'s and for each command we execute this way, it will spawn a subshell to deal with them.

An example of a job that runs with a group in the background followed by another job:

$ (a ; b) & c &
[1] 15520
[2] 15521

NOTE: Whenever we run things in ()', thoose tasks there within, are run sequentially, not concurrently, unlike $ a & b & c & d & would, for instance

However, if we were to run two different groups of commands, it will run each seperate group sequentially, but the giant thing being concurrent, as follows:

$ (a ; b) & (c ; d) & //This will run, the two groups at the same time, whilst running a and b sequentially, aswell as c and d sequentially
[1] 15528
[2] 15529

What follows is an example of a job being grouped and then piped, having the first tar command be ready for the second set:

$ cat cpdir
(cd $1 ; tar -cf - . ) | (cd $2 ; tar -xvf - ) //The $'s are position of command line arguments, $1 being the first argument, $2 being the second
$ ./cpdir /home/max/sources /home/max/memo/biblio //basically, we call the cpdir script to commit with the arguments of two directories,
//where it copies the file of the first argument into the destination directory, by virtue of the second command

We can also declare several jobs on one command line, by virtue of many commands chained, as follows:

$ find . -print | sort | lpr & grep -l max/tmp/* > maxfiles & //This is two sets of commands, each ending with the & (send to background notation)
//The first command, is find, sort and send it to the printer (lpr), the job then ends at that point with being sent ot the background
//the second command, is run a grep and send the output to maxfiles, which is then relegated to the background

[1] 18839
[2] 18876

if we wish to find what jobs are running, we can simply type the command of jobs, as follows:

$ sleep 60 & //Starts a job of a sleeping thread for 60 seconds (? maybe milliseconds)
[1] 7809
$ jobs
[1] + Running 				sleep 60 &

To bring a job to the foreground, we can do so by using the fg command:

What follows is a basic example of how jobs are assigned their numbers, based on availability and already comitting processes:

$ xclock & //Send a process to the background
[1] 1246
$ date &
[2] 1247
$ Tue Dec 2 	11:44:40 PST 2008
[2]+ Done 		date
$ find /usr -name ace -print > findout & //Run a find command on the user named ace and print it, send the output to findout, releagte to background
[2] 1269 //gets assigned number 2 since date finished and unlocks the position of 2
$ jobs
[1]- Running 		xclock &
[2]+ Running 		find /usr -name ace -print > findout &

if we wish to relegate a background job to the foreground, we can do in two ways, as follows:

$ fg 2 //We can bring a job to the foreground by fg and the job number
find /usr -name ace -print > findout

or

$ %2 //We can bring a job to the foreground by it's job number and preceded by a % sign, 
find /usr -name ace -print > findout

We could also uniquely identify by virtue of the commands fully qualified name or shorthand name, as follows:

$ fg %find

or

$ fg %f

this would work in our scenario, as it uniquely identifies the job[2], which is a find command

if we wish to search the commandline for a certain part, we could include a ?, as follows:

$ fg %?ace //Will bring the 2 job to the fg, since ace is part of the command

We can suspend jobs in the fg by virute of the suspend key (CTRL+Z):

Ctrl+Z
[2]+ Stopped 		find/usr -name ace -print > findout

to move a fg job to bg, we must first suspend it, and then call to peddle it back to the background:

$ bg //This will resume the work of the suspended job, in the background
[2]+ find /usr -name ace -print findout &

if a background job is attempting to read from the terminal, it gets suspended and has to be brought forward to the front 

What follows is an example of this:

$ (sleep 5; cat > mytext) &
[1] 1343
$ date
Tue Dec 2 11:58:20 PST 2008
[1]+ Stopped 			( sleep 5; cat >mytext )   //When it stops, it needs to recieve an action.
$fg   											   //We bring it forth to the foreground
( sleep 5 ; cat >mytext ) 						   //displays process
Remember to let the cat out ! 					   //contents
CTRL+D 											   //send end of file to quit process
$

If we wish to not have a delay in our notifications, we can set the notify utility

if we try to exit a shell while background jobs are running, it will display a prompt, telling us that we have to fix thoose
first, either by viewing them with jobs or by demanding an exit, again.

if huponexit is not set (the default mode), stopped and background jobs keep running in the background, if it is set,
the shell terminates the jobs

In BASH and TCSH we have access to creating a list of the current directories that we are working with ; generally,
it is refered to as the stack. The type of stack this is, is a LIFO stack (last in, first out) 

To display the stack, we can give the dirs command, and if the stack is empty we get the name of the cwd

$ dirs
~/literature

When we wish to cycle directories or change directories, we can push directories, with the pushd command.

When we use pushd with an argument, it pushes the directories so that we change the cwd to that specified directory, as follows:

$ pushd ../demo
~/demo ~/literature
$ pwd
/home/sam/demo
$ pushd ../names
~/names ~/demo ~/literature
$ pwd
/home/sam/names

if we use pushd without an argument, it switches between the current and the previous one, making the previous one the current one:

$ pushd
~/demo ~/names ~/literature
$ pwd
/home/sam/demo

A stack has indexes, and can be accessed with a numeric argument, where the first element is 0, just as per normal indexing:

$ pushd +2 (push the 2:th element to the top)
~/litterature ~/demo ~/names
$ pwd
/home/sam/litterature

We can also use cd - to move to the previous directory that we were working in, regardless of a stack 

To remove a element of the stack, we must pop it, and since its a directory, we thus use popd , as follows:

$ dirs
~/literature ~/demo ~/names
$ popd
~/demo ~/names
$ pwd
/home/same/demo

We can, of course, remove a directory with numeric arguments as per indexing as well

Removing a directory, other than the one that is number 0, does not change the cwd

When we talk about variables in Shell, they have the following rules:

Variables may not contain -'s or start with numbers, anything else is legit.

We can make variables global, making it accessible to the entire shell and all the programs that fork from the Shell.

We can also make a variable readonly, which makes it basically the equivilant of Final in Java (cannot be modified)

when we assign a variable, we may NOT include spaces between the name and the = and value, as such:

num=5 //We MUST, write it in this format. THIS IS FOR BASH

in TCSH, we must preceed with a set command and may have spaces, as follows:

set num = 5 //we CAN write in this format. THIS IS FOR TCSH

in BASH, we are allowed to make local declarations of values, but thoose values will only be available for that specific 
shell that runs the command, and is showcased as follows:

$ cat my_script
echo $TEMPDIR
$ TEMPDIR=/home/sam/temp ./my_script //Create a local variable that is declared to be in the command shell
/home/sam/temp
$ echo $TEMPDIR		//Try to echo the value of TEMPDIR, but find that since we spawn another shell for the new command, the local temp value is not found
					//Also, note that $TEMPDIR is a placeholder value for whatever value we have cached to that adress, thus, we try to access the value of TEMPDIR
					//by $TEMPDIR
$

In BASH, we have keywords as well, we can set them to be readonly in /etc/profile or /etc/csh.cshrc 
The way keywords works, is that the Environment inherits the keywords upon initialization and then can be utilized.

We can also change the values of keywords, in our startup files, if we wish.

The names of special parameters or positional parameters do not resemble variables. They usually have a 1 length name (like ?, # or 1)
and they are referenced (as are all variables) with a $ in the beginning. These values of these parameters, are the values of the direct
interactions of the Shell.

When we give a command, each argument on the command line becomes the value of a positional argument. They allow us to access command-line arguments,
something used with writing Shell Scripts. the command of set allows us to assign values to positional arguments.

Note: We cannot assign values to special parameters.

if we wish to create a variable, we could do as following:

$ something=5 //BASH
$ set something = 5 //TCSH

Example of showcase between just normal name call and $ reference:

$ person=max
$ echo person //Echoes just the text person
person
$ echo $person //echoes variable
max

to quote the literal version of $, instead of referencing the variable, we can use a \ escape char for that, as follows:

$ echo \$person
$person

To account for space in the name of variable assignment, we have to refer to the variable with " in assignment, as follows:

$ person="max and zach"
$ echo $person
max and zach
$ person= max and zach
bash: and: command not found

if we wish to contain several spaces in naming of a variable, we must use " marks in the quotation as is shown here:

$ person="max 	and 	zach"
$ echo $person
max and zach
$ echo "$person"
max 	and 	zach

When we refer to a variable, shells interpet the commands and the variables in a certain order, having it become so that it may expand certain
variables, if it chooses to do so - In case it does so, may occur in terms of variable calling, not variable declaration:

$ memo=max* //Will not be expanded in declaration of variable, in BASH. May occur in TCSH.
$ echo "$memo" 
max*

$ ls 
max.report
max.summary
$ echo memo //This will echo memo, which will echo max*, which will expand into max.report and max.summary
max.report max.summary //Due to variable expansion, it matches these as well

To quote a string variable in declaration, in terms of setting, we have to account for {} around respective variable:

FAULTY WAY OF DOING IT:

$ PREF=counter
$ WAY=$PREFclockwise
$ FAKE=$PREFfeit
$ echo $WAY $FAKE //we get an empty line, because if a variable is not set explicitly and refered explicitly, the default value is null

$

CORRECT WAY OF DOING IT:

$ PREF=counter
$ WAY=${PREF}clockwise
$ FAKE=${PREF}feit
$ echo $WAY $FAKE
counterclockwise counterfeit

The name of the command, in memory referencing is in the 0:th element,and if we wish to refer to a higher element than the $9:th element,
we have to encase it, as follows: ${10}

To remove the value of a variable, we simply have to assign a empty value to a variable. And a variable, per say, is only removed when the
providing SHell is shut down or we issue the unset command.

Examples of the following:

$ person= //Accounts for bash
$ echo $person

$

and if we wish to remove the variable completely

$ unset person

In BASH, we can set a Variable to be readonly (Does not account for TCSH), beyond this, readonly functions like a final value :

$ person=zach
$ echo $person
zach
$ readonly person //NOTE: When we set the readonly attribute, the relevant variable must have a value first
$ person=helen
bash: person: readonly variable

if we were to call readonly without any argument, we get a list of all the variables that are readonly

If we wish to set attributes or values for shell variables, in BASH, we can use declare and typeset (DOES NOT WORK IN TCSH):

The ones that we can set, are as follows:

-a - Declares a variable as an array
-f - Declares a variable to be a function name
-i - Declares a variable to be of type integer
-r - Declares a variable to be readonly
-x - Exports a variable (makes it global)

what follows, is a example of setting some variables:

$ declare person1=max //is the same without the declare statement
$ declare -r person2=zach //declares person2 variable to be read only
$ declare -rx person3=helen //declares person3 to be readonly and global
$ declare -x person4 //declares person4 to be global

the command of $declare -rx person3=helen, can also be written as follows:

$ declare -x -r person3=helen

We can remove attributes as well, and if we wish to do, we can do it with a + sign, instead of the -. NOTE: WE cannot remove the readonly status:

$ declare +x person3 //removes the global status of person3

We can also use typeset instead of declare, if we wish

if we run declare or set without any arguments, we get a complete list of all Shell variables that are set:

We can also run declare with a related attribute to display the list sorted to be according to all variables that has said attribute, as follows:

$ delacer -r //Finds us a list of readonly variables
declare -ar BASH_VERSINFO='([0]="3" [1]="2" [2]="39" [3]="1" ... )'
declare -ir EUID="500"
declare -ir PPID="936"
declare -r SHELLOPTS="braceexpand:emacs:hashall:histexpand:history:..."
declare -ir UID="500"
declare -r person2="zach"
declare -rx person3="helen"

By default, all variable values are set to be String, and when we do arithmetic operations on them and such, it converts them to numbers, does operations,
and then convers them back.

To make variables global in terms of to all other shells, in bash we use the export command and in tcsh, we use the setenv command

By default, our Home directory is set when we put up our account. in linux, we can find it under the /etc/passwd

$ grep sam /etc/passwd
sam:x:501:501:Sam S. x301:/home/sam:/bin/bash

When we log in, the shell inherits the path name specified and assigns it to HOME.
if we give a cd command without any argument, we are brought to the stored path of HOME.

$ pwd
/home/max/laptop
$ echo $HOME //Echo out the valuie of $HOME variable
/home/max
$ cd //change to home dir
$ pwd //print working directory
/home/max 

the ~ character is shorthand for home

What follows, is an example of how to setup the PATH variable, which is the defining factor of when we perform searches with simple names:

$ export PATH=/usr/local/bin:/bin:/usr/bin:~/bin: //This makes the PATH variable global and makes a hierarchy of places to look, it starts with /usr/local/bin
//Carries on to /bin, carries on to /usr/bin, carroes on to HOME/bin - It only "carries on", if the previous check failed.
//The last place it looks, is the cwd, a null string value signifies cwd

We can signify cwd, with the following: . or : at the end or : at the beginning (NOT RECOMMENDED FOR SECURITY REASONS)

We should never put the wd as the first part of Path, because this can lead to an issue of people naming a file as ls in the
wd and thus running the ls file instead of running the actual ls command

if we wish to add values to Path, we can reference the old value and append values, as follows:

$ PATH=/usr/local/bin:$PATH:~/bin

In BASH, the MAIL variable holds where our mail is stored (for tcsh it is mail) (usually in BASH it is stored in /var/mail/name where name is your username)
if MAIL is set and MAILPATH is not set, the system alerts you of when mail arrives in the specified MAIL file.

The MAIL variable does not do anything, unless you have a locally hosted mail-server 

The MAILPATH (does not exist in tcsh) variable, contains a list of filenames seperated by colons - if this variable is set, the shell informs you
of when one of the files is modified, for instance when you recieve Mail. We can follow the filenames by a ? and our own custom message, replace the standard
message of "You got mail".

the MAILCHECK variable checks how often the system checks for a mail. By default, it's set to 60 seconds, and if we provide it to be 0, it checks before
each prompt.

The default PS1 (prompt in tcsh) that we have, is the $ sign. Since PS1, is actually a Variable, we can modify it:

$ PS1="[\u@\h \W \1]$ " 

will give us the following 

[user@host directory event]$

Where user is the username, host is the hostname up to the first period, directory is the basename of the wd, and event is the event number of current command

We could assign the PS1 the hostname, as well, as follows:

$ PS1="$(hostname): "
bravo.example.com: echo test
test
bravo.example.com: 

Here, we have found that we display the $ symbol with calling the adress of hostname with $(hostname)

The above works for BASH, if we wish to change it for tcsh, we have to use the set command:

tcsh $ set prompt = "`hostname`: "

What follows are three examples of modifying prompts:

$ PS1='\h \$ ' //h is for host and $ is for standard mode
bravo $ 

$ PS1='\@ \u $ ' //the \@ is for time, the \u is for user
09:44 PM max $

$ PS1='\$ '
$

To see a full list of prompts modifications, we can see the bash man and search for the second occurence of PROMPTING (/PROMPTING and press n)

What follows, is a short list of different prompts for Bash (NOT ALL):

\$ 		# if the user is running with root privleges, $ otherwise.

\w 		# Pathname of the wd

\W 		# Basename of the wd

\! 		# Current event (history) number

\d 		# Date in Weekday Month Date format

\h 		# Machine hostname, without the domain

\H 		# Full machine name, including domain

\u 		# Username of the current user

\@ 		# Current time of day in 12-hour, AM/PM format

\T 		# Current time of day in 12-hour, HH:MM:SS format

\A 		# Current time of day in 24-hour, HH:MM format

\t 		# Current time of day in 24-hour, HH:MM:SS format

PS2, is the prompt that appears when it wishes for the user to complete a command or the likes.

To set this, in tcsh, we use the command of set prompt2, in BASH it's PS2

What follows is an example of the interaction of said Prompts:

$ echo "demonstration of prompt string
> 2" //Since we tried to feed it in a incomplete argument, it awaits for user to prompt in the complete rest of the command
2
$ PS2="secondary prompt: "
$ echo "this demonstrates
secondary prompt: prompt string 2"
this demonstrates
prompt string 2

the P3 is for select control structures. (prompt3 in tcsh)

the PS4 is for Debugging prompt (DOES NOT EXIST IN TCSH)

In bash, we have a shell variable called IFS (internal field seperator). This variable holds the default values of what can account for as word splitting,
and we can add values that make it so that when sentance expanding occurs, it will account for the said symbols as well added to this variable.

NOTE: Experimenting with IFS is risky, it can give unwanted side-effects, do not use it freely.

What follows, is an example of what would happen if we modify the IFS recklessly:

$ a=w:x:y:z

$ cat $a
cat: w:x:y:z: no such file or directory
$ IFS=":"

$ cat $a
cat: w: No such file or directory
cat: x: no such file or directory
cat: y: no such file or directory
cat: z: no such file or directory

since we specified that the : becomes a seperator, the commandline interpets it as that we gave 4 seperate directories.

NOTE: Word splitting occurs AFTER variable expansion occured

What follows, is an example of what this means:

$ IFS="p"
$ export VAR //the wordsplit does not trigger, because no assignment occured here

$ IFS="p"
$ aa=export //Since assignment occurs here, word splitting occurs here and thus when we echo the output of aa, we find that the p has become a split character
$ echo $aa
ex ort  //this will start the ex editor with a file called ort

$ $aa VAR //Since we have the ex ort since before and now VAR added, it will become ex ort VAR , which is ex editor and 2 files to open which is ort and VAR
2 files to edit
"ort" [New File]
Enter Ex mode. Type "visual" to go to Normal Mode.
:q
E173: 1 more file to edit
:q
$

if we unset the IFS, we restore to the state of that space and tab are the only field seperators

NOTE: Several Space or Tab, accounts for as being 1 seperator (so 3 space in a row is still 1 seperator), whilst seperator cahrs that are specified by the IFS,
still surmount to 1 seperation per character (so 3 seperation chars is 3 seperations)

We can expand the range of cd, by setting a Variable by the name of CDPATH (cdpath in tcsh). Normally, when we call CD, it checks in subdirectories of the cd and if it does not
find anything, it gives up.

If we assign values to the CDPATH variable (a list), we will find that the CD will search the CDPATH directories as well, and if it finds a match, it will change
to that directory to be the cd.

To set the CDPATH or cdpath variable, we must access  ~/.bash_profile or ~/.tcshrc and add a command line as follows:

export CDPATH=$HOME:$HOME/literature //BASH

setenv cdpath $HOME\:$HOME/literature //tcsh

if we wish to have it search the working directory first, we must supply an empty string in the beginning of the line added to CDPATH or cdpath:

export CDPATH=::$HOME:$HOME/literature

NOTE: If the argument to the cd command begins with a / (as in being a absolute path), it does not consult CDPATH or cdpath

What follows, is a list of Keywords for BASH:

BASH_ENV : The pathname of the startup file for noninteractive shells

CDPATH : the cd search path

COLUMNS : The width of the display used by select

FCEDIT : The name of the editor that fc uses by default

HISTFILE : The pathname of the file that holds the history list (default: ~/.bash_history)

HISTFILESIZE : The maximum number of entries saved in HISTFILE

HISTSIZE : The maximum number of entries saved in the history list

HOME : the pathname of the user's home directory, used as the default argument for cd and in ~ expansions

IFS : internal field seperator, the list that defines what chars accounts for as being a seperator (accounts for being a space)

INPUTRC : The pathname of the Readline startup file (default: ~/.inputrc)

LANG : The locale category when that category is not specifically set with an LC_* variable

LC_* : A group of variables that specify locale categories including LC_COLLATE, LC_CTYPE, LC_MESSAGES, and LC_NUMERIC , use the locale builtin to display a complete list
with values

LINES : The height of the display used by select

MAIL : The pathname of the file that holds the user's mail

MAILCHECK : How often, in seconds, it will check the MAIL 

MAILPATH : A colon-seperated list of file pathnames that bash checks for mail in 

PATH : A colon-seperated list of directory pathanmes that bash looks for commands in

PROMPT_COMMAND : A command that bash executes just before it displays the primary prompt

PS1 : Prompt String 1 : The primary prompt

PS2 : Prompt String 2 : The secondary prompt, defaults to '>'

PS3 : The prompt issued by select

PS4 : The BASH debugging prompt

REPLY : Holds the line that read accepts, also used by select


What follows, is a list of special characters that applies to Bash and tcsh Shell's:

NEWLINE : Initiates execution of a command 

; : 	Seperaets commands

() : Groups commands for executiuon of subshells or identifies a function

(()) : Expands an arithmetic expression

& : Executes a command in the background

| : A pipe which redirects standard output to the following command's standard input

> : Redirects standard output

>> : appends standard output

< : Redirects standard input

<< : Here document 

* : Any string of zero or more characters in an ambigious file reference 

? : Any single character in a ambigious file reference

\ : Quotes the following character

' : Quotes a string, prevents substitution

" : Quotes a string, allowing only variable and command substitution

`...` : Performs command substitution

[] : Character class in an ambigious file reference

$ : References a variable

. : Executes a command

# : begins a comment

{} : Surrounds the contents of a function

: : Returns true

&& : Executes command on right only if command on left succeeds (returns a zero on exit status, basically a and)

|| : Executes command on right only if command on left fails (returns a nonzero exit status, an or statement)

! : Reveres exit status of a command

$() : Performs command substitution //DOES NOT WORK IN TCSH 

[] : Evalutes an arithmetic expression

A process is started when we run commands that spawn a subshells or is run in the current shell. A process is NOT spawned, if it is run by a shell builtin, such as 
cd.

Each process forks into another set of commands, creating a tree structure of 2 directions on each node.

When a linux system launches, it starts with a init in a single process called a spontaneous process, which it gives the process ID of 1.
Which means that this process is the main process of all other processes (i.e, the highest in the hierarchy of all processes)

To get the login, the spontaneous process runs a getty or a mingetty process which displays login, and after we give the info and return it to the Login,
the getty hands over control to the Login process. after we have logged in, the login process becomes the user's shell process

When a process is spawned, it's handed a process id. As long as said process runs, it keeps the same Id.

What follows is an example of some of the interactions between said things:

$ sleep 10 &
[1] 22789
$ ps -f
UID 	PID  PPID C STIME TTY 		TIME CMD
max    21341 21340 0 10:42 pts/16 00:00:00 bash
max    22789 21341 0 17:30 pts/16 00:00:00 sleep 10
max    22790 21341 0 17:30 pts/16 00:00:00 ps -f


if we re-run the same command, we'll see dupplication in the processers ID (PPID)

$ sleep 10 &
[1] 22791
$ ps -f
UID 	PID 	PPID 	C 		STIM 		TTY 		TIME 	CMD
max   21341    21340    0      10:42 		pts/16 		00:00:00 bash
max   22791    21341    0      17:31 		pts/16 		00:00:00 sleep 10
max   22792    21341    0 	   17:31 		pts/16 		00:00:00 ps -f

if we wish to see a tree of the processes doing what and what forks to what, we can use the command of pstree -p, as follows:

$ pstree -p
init(1)-+-acpid(1395)
		|-atd(1758)
		|-crond(1702)
		...
		|-kdeinit(2223)-+-firefox(8914)---run-mozilla.sh(8920)---firefox-bin(8925)
		|				|-gaim(2306)
		|				|-gqview(14062)
		|				|-kdeinit(2228)
		|				|-kdeinit(2294)
		|				|-kdeinit(2314)-+-bash(2329)---ssh(2561)
		|				|				|-bash(2339)
		|				|				'-bash(15821)--bash(16778)
		|				|-kdeinit(16448)
		|				|-kdeinit(20888)
		|				|-oclock(2317)
		|				'-pam-panel-icon(2305)---pam_timestamp_c(2307)
...
|-login(1823)---bash(20986)-+-pstree(21028)
|							'-sleep(21026)
...

The output is abbreviated (shortened down), because it would give a lot more as output

the kdeinit runs a GUI user followed by some programs such as firefox and oclock etc.

the login process runs a text user that is running a sleep command in the background and pstree in the foreground.

When we execute a command, we spawn a subshell that runs the command, and this puts the parent process to sleep whilst
the child process processes. When the child process completes, the parent process wakes up.

When we prompt a process to go to the background to process, the main process which forks the other one, DOES NOT GO TO SLEEP.
This is the upside of sending processes that we do not wish to currently compute on or to run in the foreground, has.

When we run builtin processes, they do not fork a process and thus, do not spawn a sub-shell to process the command.

Within a given process, we can initalize, change, read and declare variables. By default, the variables are local to the process.
When we fork processes, attributes are not inherited to the child process. We can make variables avaialble to child processes by virtue of 
making them global with export (BASH) or setenv (TCSH)

If we wish to get a list of the previous commands that we have run, and to access them/re-run them etc, we can gain access to them by virtue of
calling the history utility (Works in Bash and TCSH)

Albeit similar, the TCSH history uses slightly different variables and some other functions compared to the Bash version.
There is a Variable within the History utility, called HISTSIZE, that defines how many of the previous commands we save.

The usual range, is 100 to 1001.

When we exit the Shell, we can find the last run commands in a file, called HISTFILE, located in ~/.bash_history (this is the default)

This file, can save HISTFILESIZE amount of commands between sessions, and is reinitialized during bootup of the Shell. 
The default for this file, is 500.

Bash applies a sequential (auto-increment) identifying number to each process in the history list, which allows us to see what number of the
command we ran, was (as in, what ordering it occured in)

To for instance just make Bash save the last 100 command, we can give the following command:

$ HISTSIZE=100

To define how many commands we save across sessions, we can do as follows:

$ HISTFILESIZE=100

The history list, when called upon, is sorted so that the oldest commands are going upwards (as in, oldest is at the top of the list)
In TCSH, a time of execution is included, next to the command, to showcase at what timepoint that command was given:

$ history | tail
23 	PS1="\! bash$
24  ls -l
25 	cat temp
26  rs temp
27 	vim memo
28 	lpr memo
29 	vim memo
30 	lpr memo
31 	rm memo
32 history | tail

To make commands readable, as the list gets longer (i.e, sort through the result), we can call history and pipe it through to a less command

We can also create "aliases", if we wish, which is basically assigning commands to keybindings that we decide, as follows:

$ alias "h=history | tail" //binds the h key to display the tail of history
$ alias "hg=history | grep" //binds the command of hg to pipe through the contents of history to a grep (regex), which basically means that we can call
hg with the command of a string to find all related strings from the history piped to the grep

To display the history list in a modifiable state, in BASH (Does not account for tcsh), we can use the fc command.
An example follows:

$ fc -l
1024 	cd
1025 	view calendar
1026 	vim letter.adams01
1027 	aspell -c letter.adams01
1028 	vim letter.adams01
1029 	lpr letter.adams01
1030 	cd ../memos
1031 	ls
1032	rm *0405
1033 	fc -l
1034 	cd
1035	whereis aspell
1036 	man aspell
1037 	cd usr/shared/doc/*aspell*
1038 	pwd
1039	ls
1040	ls man-html

fc -l can take two arguments, according to this format: fc -l <first argument> <second argument>
The first argument, can be a number (number of process), a string that is the start of the command, or a negative number, that allows it to sort out
the X last commands. (So, -10 would be the 10 last commands)

The second argument, is the upperbound of the range. This can be a number (same as lower for result of 1 specific number), a string of the command (which finds
first argument of command and up to second argument of command)

fc -l can also run without the second argument, in which case we end up in a situation where it takes EVERYTHING from first argument to the current elementl.
if we run it without first and last, we get the entire list.

Here are some examples of the lists interactions:

$ fc -l 1030 1035
1030 	cd ../memos
1031 	ls
1032 	rm *0405
1033 	fc -l
1034 	cd
1035 	whereis aspell

An example of string interactions:

$ fc -l view whereis
1025 	view calendar
1026 	vim letter.adams01
1027 	aspell -c letter.adams01
1028 	vim letter.adams01
1029 	lpr letter.adams01
1030 	cd ../memos
1031 	ls
1032	rm *0405
1033 	fc -l
1034 	cd
1035 	whereis aspell

and an example of getting a single element:
$ fc -l 1027 1027
1027 	aspell -c letter.adams01

We can edit commands, using fc, by appending the -e to the command line when we call upon fc. fc can run with different text editors that are installed,
and when we provide -e and the name of an editor, we can edit a command. If we do not provide first and last arguments to this, it will simply
assume to edit the last command that we entered. 

The default editor that fc resorts to, is nano. 

An example of a call:

$ fc -e vi //opens the most recent command given in the vim editor

We can set the default variable to which fc opens in, as follows:

$ export FCEDIT=/usr/bin/emacs //This causes fc to edit in emacs, by default. emacs are not by default installed
$ fc

another example:

$ fc 1029 //edits process 1029 in the work buffer with default editor

Another example, this time with a range:

$ fc vim 1030 //Puts in all the elements from thoose that begin with vim, to the process number 1030

NOTE: when we execute a fc command, it puts the inputs into a editor buffer, when we are done, we should ALWAYS clear out the editor buffer

We can also reexecute commands without calling an editor, with the -s command, as follows:

$ fc -s 1029
lpr letter.adams01

the next reexecutes this command

$ fc -s

When we re-execute commands, we can substitute a string with another one, in terms of the command : as follows:

$ fc -s adams=john 1029 //runs the 1029 command, but with adams replaced by john
lpr letter.john01

We can also re-execute commands from history with the ! command, as follows:

!! : This re-executes the previous command from history list

!$ : this token gets replaced with the last word of the last command, thus, if the last command was lpr test, we'd replace !$ with test

What follows are two examples of interactions of references:

$ ls -l text
-rw-rw-r-- 	1 	max 	group 45 Apr 30 14:53 text
$ !!
ls -l text
-rw-rw-r--  1 	max 	group 45 Apr 30 14:53 text

Following example is with reference of number event: //NOTE: When we call commands such as !-5 or !55 etc., it adds one element to the history of called commands, thus the command
we just executed becomes the last to have been run and the previous call (55) will become 56:

51 $ !44
ls -l text
-rw-rw-r-- 	1 	max 	group 45 Apr 30 14:53 text
52 $ !-8 //This will rerun event 44, because we currently have 52 events in the history log
ls -l text
-rw-rw-r-- 	1 	max 	group 45 Apr 30 14:53 text

if we wish to have a prompt of the last command that was run, by the prompt, we can do as follows:

export PS1='last cmd num: $((\# -1)), this num: \# ' //we make the PS1 (first prompt) assigned to be last cmd num followed by the reference of the previous command
by virtue of ((\# -1)) (escaping the arithmetic expression), creating $number, basically, whilst this num: \# is reference to current command number.

To find the command number for a command, we can run the \# command, whilst if we want the HISTORY number, we can get the \! command.

The difference is that a command number, is the ordering of which when the number occured this session, whilst the history number, is based on the history index of a 
command in the history list (This might lead to confusion, due to copying of elements in the history list, and thus making it so that you might end up with a 
longer list of actual elements in the history list compared to the command list)

if we run a ! command in combination with a letter, we will find the first command in the history that begins with thta element, as such:

$ history 10
//some elements

69 $ !l //begins with l
ls -l
...
70 $ !lpr //begins with lpr
lpr memo
71 $ !?letter? //contains letter
cat letter
...

What follows is a short list containing some of the commands related to the ! command:

! : starts a history event unless followed immedeatly by a SPACE, NEWLINE, = or (

!! : previous command

!n : Command number n in the history list

!-n : The n:th preceeding argument (so, n elements back)

!<string> : The most recent commend that starts with <string>

!?<string>? : The last command that contains <string>, the last ? is optional

!# : The current command, as you have typed it, thus far

!{event} : a isolated event desgination that can be followed by other things. an example: !{-3}3 would be three elements back, followed by a 3

What follows are some examples in terms of word desginations:

72 $ echo apple grape orange pear
apple grape orange pear
73 $ echo !72:2
echo grape
grape
74 $ echo !72:^ //^ is the first word
echo apple
apple
75 $ !72:0 !72:$ //This causes 72:0 to perform the echo command, and the $ is the latest word, that you wrote on that commandline
echo pear
pear
76 $ echo !72:2-4 //Echo from the 2:nd element to the 4:th echo
echo grape orange pear
grape orange pear
77 $ !72:0-$ //Go from the 0:th element (incluiding echo), to the last word
echo apple grape orange pear
apple grape orange pear

An example of the selection of last word by !$:

$ cat report.718
...
$ vim !$
vim report.718

If a statement contains several commands, the ordering becomes a bit different, as each argument accounts for as being 1 element, whilst seperate characters
are considered 1 argument each, as we can see:

78 $ 72 ; echo helen zach barbara //When we have seperation of commands, they run sequentially, so first it runs the statement of line 72, which is echo apple grape orange pear
echo apple grape orange pear
apple grape orange pear

helen zach barbara //It then echoes helen zach and barbara, due to echo helen zach barbara

79 $ echo !78:7 //since the whole line we would have is as follows: echo apple grape orange pear ; echo helen zach barbara
//then we get a total of echo helen, because echo = 0, apple = 1, grape = 2, orange = 3, pear = 4, ; = 5, echo = 6, helen = 7
//thus, !78:7 is helen
echo helen 
helen

//echo apple grape orange pear ; echo helen zach barbara 

80 $ echo !78:4-7 //we get, pear ; echo helen (pear = 4 ; = 5 echo = 6 helen = 7), which we have echo added in front of, so we get echo epar ; echo helen

echo pear ; echo helen
pear
helen

What follows is a list of some of the modifiers when it comes to the ! command:

n : this is just the n:th word, basically just index

^ : the first word

$ : the last word

m-n : all words from m to n, m default is 0, thus if we omit it, we get up to n

n* : from n to end, * meaning all words past n

* : all words except the command name

% : The word matched by the most recent ?string? search

If we write wrong, we can do substitute by the s command, what folows is an example of how to use it in pure syntax :

$ car /home/zach/memo.0507 /home/max/letter.0507
bash: car: command not found
$ !!:s/car/cat //Repeat last command but substitute car with cat, each substitution must consist of :s/<old>/<new>/
cat /home/zach/memo.0507 /home/max/letter.0507

This substitutes the first element it finds, to substitute all, we can do a g before the s, causing it to global

We don't have to use / to seperate, we can use any char that is not in the word. Also, the last / is optional, if a return statement
follows is directly after, we find that it's not needed

Old may not be a regex. If we want, there is a abbreviated form of substitution, as we can see here:

$ ^old^new^ 

this is basically the same as:

$ !!:s/old/new/

Thus, if we wanted to write the $ !!:s/car/cat replacement, we could simply write:

^car^cat //We can omit the final caret if a return statement immedeatly follows (as in, the command completes there)

There are other modifiers as well, that we can apply and replace as follows:

$ ls /var/log/messages
/var/log/messages
$ !!:p //Run last command with :p modifier, which is do not run it
ls /var/log/messages
$ !!:h:p //run last command, remove last word, and do not ru nit
ls /var/log

What follows is a list of modifiers on the !! operations:

e - Removes all but filenam extensions

h - Removes last part of pathname

p - do not print

q - quote a substitution to prevent further substitutions on it

r - removes the filename extensions

t - removes all elements of a pathname, except for last

x - like q, but quotes each word in substituion individually

To have commandline editing, we use something called the readline library. any application written in C allows for this.
Programs that use Readline Library, including bash, read ~/.inputrc for key binding info and configs.
if we do not wish to allow for command editing, we can give the command of --noediting

We can choose two modes in commandline editing, either vi(m) or emacs. By default, it's set to Emacs.
Commandline editing, allows for many of the integrated commands in respective editor to be used.

It also activates so taht you can use arrow keys, up and down go through history list.
It also provides a lot of autocomplete options.

to switch to vi or emacs, we can do as follows:

$ set -o vi //sets to vi mode

$ set -o emacs //sets to emacs mode

When we are in vi editing mode, if we wish to enter command mode, we can press ESC. This causes all of the vi features to activate specifically for 
editing the command line, then.

The VIM editor starts in command mode, whilst the command-line version starts in input mode.

We can search backwards after commands in our history, with ? followed by a string of the command

If we wish to search foward we can use the / command followed by a text.

These texts in command-line mode, unlike vim, cannot contain Regex commands. Albeit, we can use the ^ command followed by a text, to force the shell
to locate a command that begins with <input string>

Just as in vim, if we wish to scroll to the next "hit", we must press n

We can also use event numbers to access elements in the history list, if we are in command mode we can give a numeric command followed by, as follows:
55G, for instance, which gives us the element in the history list that has that co-responding event number

When we are in command mode, we can use the following commands to edit the command:

x (delete char)

r (replace char)

~ (change case)

. (repeat last change)

To change back to input mode, we can give one of the following commands : 

i/I - Insert

a/A - Append

R - Replace

c/C - Change 

As per usual, Emacs doesn't have multiple modes. Thus, we can edit commands on the line, without having to change between modes.

In the emacs mode, we can move the cursor by providing both ctrl commands and escape commands.

To move the cursor one step backwards, we can use the ctrl+B and for forward, we can use ESC-f. 
To move several words forward, we can press ESC and then follow up with a number to define how many words we'd like to forward and a escape sequence.

To move to the beginning of the line, we can do CTRL+A.
For end of line : CTRL+E 
next instance of a specific char: CTRl+X CTRl+F <specific char>

NOTE: If you press CTRL+D at the beginning of a sequence that contains no chars, it may terminate your shell session.

If we wish to delete from cursor to end of line, we can do CTRL+K

If we wish to use autocomplete of commands, we can do so both in vim end Emacs mode, with the tab char.

If there is one command that matches it, you can press tab and get a completion of that. 
If there are more matches, in vi mode it does nothing, and in emacs mode it beeps.

if we press tab again, we get a list of the commands of which matches the input we have given.
This still allows you to keep typing the command.

What follows is an example of the interaction:

$ bz -->TAB (beep) -->TAB //Beep because of emacs mode
bzcat 		bzdiff 		bzip2 			bzless
bzcmp 		bzgrep 		bzip2recover 	bzmore
$ bz

$ bzc -->TAB (beep) -->TAB
bzcat 	bzcmp
$ bzca -->TAB --> t //Since bzca only matches one command, bash completes the command by virtue of tabbing

We can autocomplete pathnames as well, with the same logic, where emacs beeps and vi does nothing.
If there is a unique path that it can relate the path written so far to, it will change to that, if you tab to it.
However, if there are several ones, tab will only bring you to the most common ancestor and allow you to keep typing.

$ cat films/dar -->TAB(beep) cat films/dark_ //beeps, and stops, because every file past this point all begin on dark_

To review the choices at our disposal, we can double click tab to find out what the list of commands are.

When there is no more ambiguity it appends a space, and afterwards, we can just press Enter to finish the command line, since there
is no ambiguity to what it could be.

When we type variable names, we have pretty much the same functioning, as follows:

$ echo $HO -> TAB -> TAB
$HOME 	$HOSTNAME 	$HOSTTYPE
$ echo $HOM -> TAB ->E

if we press return, we execute the command.

BASH and other utilties that uses the Readline Library, read the file specified by the INPUTRC environment variable to obtain
initialization information.

if INPUTRC is not set, these programs read the ~/.inputrc file. They ignore lines of .inputrc file that are blank or start with # (a commant)

We can set the variables in .inputrc to control the behaviour of the Readline utility, using the syntax of:

set <variable> <value>

To see a complete list of variables modifyable, we can access the Readline Variables in the Bash man page or info page.

What follows, is a shorter list:

editing-mode : similar to the set -o vi or set -o emacs command, we can set editing-mode with vi or emacs to define that we wish to have that
specific kind of editing for readline mode

horizontal-scroll-mode : Set to on, to cause long lines to extend off the right edge of the screen.  Default is off.

mark-directories : set to off to cause autocomplete to not place a / on the end of path directories in terms of autocomplete. Is set to on by default

mark-modified-lines : Set to on, to cause the readline utility to mark modified lines with a * in the history listing. Default is off

If we have unbound keys, we can map the keys to commands in the Readline utility, and to do so, we can follow the following format of setting commands:

keyname: command_name
"keystroke_sequence": command_name

In the first form, we have to spell out the command name, so for instance CTRL+u would be control-u

In the second form, we must specify a string that describes the sequence which we wish to bind. We can use the emacs form of escape sequence to
quote special characters, as follows:

\C (CTRL)

\M (alt)

\e (escape)

Any character being quoted, can be escaped by a single escape character of the \ notation

For instance, if we wish to bind the kill-whole-line command, which exists in emacs only, we can do as follows:

control-r: kill-whole-line

To see all the commands of the readline utility we can run the bind-P command.

Commands that begin with vi in this mode, are exclusive to vi editing, whilst if they do not begin with vi, they generally can be used in emacs mode.

To see all the currently bound buttons of readline utility, we can give the command of bind -q <command name>, as follows:

$ bind -q kill-whole-line
kill-whole-line can be invoked via "\C-r".

We can also bind text by enclosing it by double quotes, as follows:

"QQ" = "The Linux Opreating System"

This makes the system insert the given string, when we give the given bound command. (in this case, QQ gives The Linux Operating System)

We can conditionally select parts of the .inputrc file using the $if directive (if statement), as follows:

$if test[=value]
		commands
	[$else
		commands]
$endif

Where test is mode, term or Bash. if test equals value (or if test is true when value is not specified) it executes the first set of commands.
if it does not equal the value or if the if test is false when value is not specified, then it executes the second command if they are present,
or exists if they are not present.

an $if statement can test three distinct types of tests:

$if mode=vi //This will result true if the current mode is vi for the editing in commandline, otherwise false, we can test for vi or emacs

$if term=xterm //Checks the value of the TERM variable. We can test for any value of TERM.

$if bash //Checks for what program is running the command. If it's bash, this gives true. False, otherwise. We can test for any app name.

These commands allow for some great customization of the Readline utility. We can change specific things based on the factor of what Shell it is,
or arbitrary things, etc.

What follows, is an example of a command that makes it so that regardless of what editing mode we are in, that we can give the CTRL + y command
to move the cursor to the beginning of the next word, regardless of Editor mode we are in.

if we wish to find more info about the readpage utility, we can open the BASH man page and give the command of /^READLINE

If readline commands are not processing, we need to log in and out. as it reads from ~/.inputrc after we log in.

Alises are shorthand commands for other values, as follows:

alias [name[=value]] //BASH

alias [name[ value]] //TCSH

Unlike alises in TCSH, Bash does not accept an argument from the command line in value.
We have to use a bash function to be able to use arguments.

Aliases do not replace themselves, so there cannot be any recursive function calls, when handling them, as follows:

$ alias ls='ls -F' //cannot cause a recursive call, even if involing ls in a ls assignment

We can nest alises, and alises are disabled for non-interactive shells (such as shell scripts).

To see a list of all the current alises, we can give the alias command.
If we wish to find a specific name, we have to type: alias <alias name> to find what that alias is bound to

To remove an alias, we can use the unalias command.

What follows, is an example of the alias command:

$ alias
alias ll='ls -l'
alias l='ls -ltr'
alias ls='ls -F'
alias zap='rm -i'

Most linux variants define some alises, and we can find them by running the alias command.

If we create an alias with simple qutations marks, as in a ' , we find that values are only expanded upon use.
When we enclose with "'s, we find that we expand upon creation of the variable. What follows, are some exmaples to showcase the difference:

$ echo $PWD
/home/max
$ alias dirA="echo Working directory is $PWD"
$ alias dirA
alias dirA='echo Working directory is /home/max' //Variable is expanded upon creation

$ alias dirB='echo Working directory is $PWD'
$ alias dirB
alias dirB='echo Working directory is $PWD' //Variable is only expanded upon use

following, we will see the difference in action:

$ cd cars
$ dirA
Working directory is /home/max //dirA has a earlier value set, and is not updated when we change cwd
$dirB
Working directory is /home/max/cars //dirB has a unexpanded value, and is only set when used. Thus, it always remains relative to it's call

To give a command as the command name, regardless of alias, we can either do:

the absolute pathname of the command

or

./<command>

What follows, is a list of all the related modifiers for the ls command:

//Remember, in linux, commands chain. Each letter or a combination of letters can be a command, so when we see ltr or the likes, it's 3 commands : l t and r.

-a or --all // Do not hide entries starting with . 

-A or --almost-all // do not list implied . and ..

--author //print the author of each file

-b or --escape // print octal escapes for nongraphic characters

--block-size=SIZE // use size-byte blocks

-B or --ignore-backups // do not list implied entries ending with ~

-c //with lt: sort by, and show, ctime (time of the last modification of this files status information), with -l: show ctime and sort by name. otherwise sort by ctime

-C //list entries by columns

--color[=WHEN] //Control wether color is used to distinguish file types. WHEN may be 'never', 'always' or 'auto'

-d or --directory //list directory entries instead of contents, and do not dereference symbolic links

-D or --dired //generate output designed for Emac's dired mode

-f //do not sort, enable -aU, disable -lst

-F or --clasify //append indicator (one of */=@|) to entries

--format=WORD //across -x, commas -m, horizontal -x, long -l, single-column -1, verbose -l, vertical -C

--full-time //like -l --time-style-full-iso

-g //like -l, but do not list the owner

-G or --no-group //inhibit display of group information

-h or --human-readable //print sizes in human readable format (1k, 234M, 2g etc.)

--si //likewise, but use powers of 1000, not 1024

-H or --dereference-command-line //follow symbolic links listed on the command line

--derefernce-command-line-symlink-to-dir //follow each command line symbolic link that points to a directory

--indicator-style=WORD //append indicator with style WORD to entry names: none (default), classify (-F), file-type (-p)

-i or --inode //print index number of each file

-I or --ignore=PATTERN //do not list implied entries matching shell PATTERN

-k //like --block-size=1K

-l // use a long listing format

-L or --dereference //when showing file information for a symbolic link, show information for the file the link references rather than for the link itself

-m //fill width with a comma seperated list of entries

-n or --numeric-uid-gid //like -l, but list numeric UIDs and GIDs

-N or --literal //print raw entry names (don't treat e.g. control characters specially)

-o //like -l but do not list group information

-p or --file-type //append indicator (one of /=@|) to entries

-q or --hide-control-chars //print ? instead of non graphic characters

--show-control-chars //show non graphic characters as-is (default unless program is 'ls' and output is a terminal)

-Q or --quote-name //enclose entry name in double quotes

--quoting-style=WORD //use quoting style WORD for entry names: literal, locale, shell, shell-always, c, escape

-r or --reverse //reverse order while sorting

-R or --recursive //list subdirectories recursively

-s or --size //print size of each file, in blocks

-S //sort by file size

--sort=WORD //extension -X, none -U, size -S, time -t, version -v, status -c, time -t, atime -u, access -u, use-u

--time=WORD //Show time as word instead of modification time: atime, access, use, ctime or status; use specified time as sort key if --sort=time

--time-style=STYLE //show times using style STYLE: full-iso, long-iso, iso, locale, +FORMAT

Format is interpeted like 'date': if format is format1<new-line>format2, format1 applies to non-recent files and format2 to recent files: if STYLE is 
prefixed with 'posix-', STYLE takes effect only outside the POSIX locale

-t //sort by modification time

-T or tablesize=COLS //assume tab stops at each COLS instead of 8

-u //with -lt: sort by and show, access time. with -l: show access time and sort by name. otherwise: sort by access time

-U //do not sort; list entries in directory order

-v //sort by version

-w or --width //assume screen width instead of current value

-x //list entries by lines instead of by columns

-X //sort aplhabetically

-1 //list one file per line

Here are some SELinux options as well:

--lcontext //Display security context. Enable -1. Lines will probably be too wide for most displays.

-Z or --context //Display security context so it fits on most displays. Displays only mode, user, group, security context and file name.

--scontext //Display only security context and file name

--help //display this help and exit

--version //output version information and exit



What follows, are some examples of different alises:

$ alias r='fc -s' //This allows you to type r, to repeat the previvious command or r abc to repeat the last command that began with abc
//fc is for the history list, the -s modifier repeats the last command with the fc command

$ alias l='ls -ltr'
$ l
total 41
-rw-r--r-- 	1 max 	group 	30015 Mar 1 2008 flute.ps
-rw-r----- 	1 max 	group 	3089  Feb 11 2009 XTerm.ad
-rw-r--r-- 	1 max 	group 	641   Apr 1  2009 fixtax.icn
-rw-r--r-- 	1 max 	group 	484   Apr 9  2009 maptax.icn
drwxrwxr-x 	2 max 	group 	1024  Aug 9  17:41 Tiger
drwxrwxr-x  2 max 	group 	1024  Sep 10 11:32 testdir
-rwxr-xr-x  1 max 	group 	485   Oct 21 08:03 floor
drwxrwxr-x  2 max 	group 	1024  Oct 27 20:19 Test_Emacs

What follows is an example of putting in rm with interactive mode into an alias:

$ alias zap='rm -i'
$ zap f* //rm -i on every file that begins with f
rm: remove 'fixtax.icn'? n
rm: remove 'flute.ps'? n
rm: remove 'floor'? n

We could also write the command as follows: 

alias rm='rm -i' //just makes rm take the alias of rm -i

What follows are two aliasings of ls and ll:

$ alias ls='ls -F'
$ alias ll='ls -l'
$ ll
total 41
drwxrwxr-x 	2 max 	group 1024 	Oct 27 20:19 	Test_Emacs/
drwxrwxr-x 	2 max 	group 1024 	Aug 9  17:41 	Tiger/
-rw-r----- 	1 max 	group 3089 	Feb 11 2009 	XTerm.ad
etc.

In the above example, we see that ls has an alias, and ll is an alias that gets assigned this alias and -l, which simply leads to that ls is expanded once, which leads to the final
product of:

ls -F -l (because ls is alias for ls -F, so (ls -F -l) is the final result)

The -F option causes ls to print a / at the end of directories and * at the end of executable files.

if we run a alias command followed by simply variables, without actual connection in terms of =, it will just name thoose aliases meanings as follows:

$ alias ll l ls zap wx
alias ll='ls -l'
alias l='ls -ltr'
alias ls='ls -F'
alias zap='rm -i'
bash: alias: wx: not found

To avoid alias substitution (i.e, do the literal command), we can proceed a command with a \, as follows:

$ \ls
Test_Emacs Xterm.ad 	flute.ps 	maptax.icn
Tiger 		fixtax.icn 	floor 		testdir

Since alias names don't change other parts of the command line, we can still feed arguments to alias arguments, as follows:

$ ll f* //will run ls -l f* , so list long all the files beginning with f

An example of unaliasing

$ unalias zap
$ alias
alias ll='ls -l'
alias l='ls -ltr'
alias ls='ls -F'
$ zap maptax.icn
bash: zap: command not found

BASH FUNCTIONS:

NOTE: Only BASH has functions, TCSH does not have functions.

A shell function (BASH function), is stored in RAM instead of on the hard disk, so it leads to faster bootup time and pre-processing of the function, compared to a 
shell script.

The shell runs a Function in the same shell as it's run from. 

If we define too many functions, the overhead (performance loss) can become too large, due to storing too many of the elements in RAM and loss of performance coming from
access of large variance in RAM.

We can declare a bash function in ~/.bash_profile or in the script that uses it or directly from the command line. You can remove functions with the unset command.
Functions are not saved between sessions (logging out removes em)

If we have a shell variable and a function called the same thing, variable takes predecence of being removed, so to remove a function with the same name as a 
variable, we need to run the removal of said name twice.

The syntax that declares a Bash function, is as follows:

[function] function-name()
{
	commands
}

function is optional, function name is the name of the function and commands are what the function is supposed to execute

commands can be anything, as per usual.

The { can be on the same line as the function name. alises and variables are expanded in the function when it's read, NOT when it's executed.

We can use the break statement to stop execution of the function, if we wish.

An example of a function:

start_process(){
process > .process.out 2>&1 & //runs the standard output of process to process.out and send the process to the background
}

The next example is a simple version of how to do the whoson script manually:

$ function whoson()
{
	date
	echo "Users currently Logged On"
	who
}

$ whoson
Sun Aug 9 15:44:58 PDT 	2009
Users Currently Logged On
hls 	console 	Aug 8 	08:59 	(:0)
max 	pts/4 		Aug 8 	09:33 	(0.0)
zach 	pts/7 		Aug 8 	09:23 	(bravo.example.com)

if we wish to always have a function available, not having to rewrite it every time we enter the shell, we can have it written in the profile, as follows:

$ cat ~/.bash_profile
export TERM=vt100
stty kill '^u'
whoson ()
{
	date
	echo "Users Currently Logged On"
	who
}
$ . ~/.bash_profile

We can specify arguments when we call upon a function, and we can access them by virtue of positional arguments, as follows:

$ arg1( ) {
	echo "$1" //This will echo the first argument that we feed into the function
}
$ arg1 first_arg
first_arg

What follows, is an example of a function that allows you to export variables using tcsh syntax:

Note: The env builtin lists all environmental variables and ensures that set env worked correctly

$ cat .bash_profile
....
#setenv - keep tcsh users happy
function setenv()
{
	if[ $# -eq 2 ] //The $# variable, is a special value that is the amount of commandline arguments, and we are looking to have 2 arguments
		then
			eval $1=$2 //eval forces Bash to scan this line twice, because variable expansion does not occur on the first run, as such, when it has occured when
			//we run it a second time, we get the full extent of which it is to run it with. 

			//if we would run it once, it would give us a grand total of TCL_LIBRARY=/usr/local/lib/tcl which is wrong, and the reason it does this is because it begins with
			//a $ before substituion ($1=$2), so it thinks it's a command, which it's not, it's a list of arguments for a command, so scanning it a second time, it correctly
			//splits up all of the things into 3 parts, instead of 1 "entire string", thus giving us the correct format
			export $
		else
			echo "Usage: setenv NAME VALUE" 1>&2
	fi //fi closes the if statement in BASH
}
$ . ~/.bash_profile
$ setenv TCL_LIBRARY /usr/local/lib/tcl
$ env | grep TCL_LIBRARY
TCL_LIBRARY=/usr/local/lib/tcl

There are two kinds of commands, in Bash, short and long. The short version, uses only 1 - and the long version ,uses 2, as --.

What follows, is a list of some common commands used:

--help //help

--noediting //Prevents users from using the Readline Library to edit commands in an interactive shell

--noprofile //prevents reading of startup files: /etc/profile ~/.bash_profile ~/.bash_login and ~/.profile

--norc //Prevents reading the ~/.bashrc startup file. This option is on by default if the shell is called as sh

--posix //Runs bash in POSIX mode

--version //Displays bash version information and exits

--l //Runs bash as if it was a login shell

shopt +o/-o //allows you to turn on set functions, + means on, - means off

-- //end of options on commandline, every command after this is treated as if it begins with -

In Bash, we can turn on and off many options, albeit, set turns on/off one set of commands, whilst shopt turns on/off another set of commands.

An example of BASH's set function (tcsh has one as well, albeit it works differently):

$ set -o noclobber //turns on noclobber

$ set +o noclobber //turns off noclobber

If we give set -o without any arguments, we get a list of all the features that are on/off and controlled by set.
If we give set +o without any arguments, we get them in a way that we can give them as input into the shell.

Some examples of shopt follows:

$ shopt -s dotglob //-s stands for set, dotglob includes filenames with . beginnings when expanding ambigious file references
$ shopt -u dotglob //-u stands for unset
$ shopt dotglob //will show the status of dotglob
dotglob 	off 

if we give shopt -s without any argument, we get a list of all the features that are on and handeled by the shell
if we give shopt -u without any argument, we get a list of all the features that are off and handeled by the shell

NOTE: shopt has higher predecense than set, thus if you include shopt and relevant syntax before the set command, it works, as follows:

$ shopt -o -s noclobber //this turns on the noclobber feature

What follows, is a list of bash features:

set -o allexport or set -a //Automatically exports all variables and functions you create or modify after giving this command.

set -o braceexpand or set -B //causes bash to perform brace expansions

shopt -s cdspell //Corrects minor spellings errors in directory names used as arguments to cd

shopt -s cmdhist //Saves all lines of a multiline command in the same history entry, adding semicolons as needed

What follows, is a list of Bash features:

set -o allexport or set -a // Automatically exports all variables and functions that we make or modify, after having given this command.

set -o braceexpand or set -B // Causes bash to perform brace expansion

shopt -s cdspell //Corrects minor spelling errors in terms of calling path for cd

shopt -s chmdhist //Saves all lines of a multi-line command in a single entry, adding ;'s as needed

shopt -s dotglob //Causes wildcar characters to be able to match . and .. (? can thus match . or ..)

set -o emacs //Specifies emacs editing mode for command-line editing

set -o errxit or set -e //Causes bash to exit when a simple command (not a control structure) fails

shopt -s execfail //Causes a shell script to keep running, even if it cannot find the specified file from the arguments list. The default behaviour is that
it quits execution, if it cannot find the specified file

shopt -s expand_alias //Causes alises to be expanded (Default for interactive shells is on, default for non-interactive shells is off)

set -o hashall or set -h //Causes bash to remember where commands it has found using PATH are located. (default is on)

shopt -s histappend //Causes bash to append the history list to the file named by HISTFILE when the shell exits. By default, Bash overwrites this file.

set -o histexpand or set -H //Turns on history mechanism (Which uses ! by default). Turn this off to turn off history expanding

set -o history //Enables command history

shopt -s huponexit //Sends a SIGHUP signal to all jobs when an interactive shell exits //a SIGHUP signal is a termination signal send to processes

set -o ignoreeof //Specifies that bash must recieve 10 EoF chars before it exits (useful on systems that cause a lot of EoF but is not intended for quit on 1)

set -o monitor or set -m //Enables job control (default is on)

shopt -s nocaseglob //causes ambigious file references to match regarding casing (default is off)

set -o noclobber or set -C //helps preventing overwriting files (off by default)

set -o noglob or -set f //Disables pathname expansion (default is off)

set -o notify or set -b //Causes background jobs to immedeatly report when terminated (Standard is to report on next prompt)

set -o nounset or set-u //Displays an error and exits, if you unset a variable in a interactive shell. The default is to display a null value
for an unset variable

shopt -s nullglob //Causes bash to expand filenames that do not match into a null string (default is to simply pass them by)

set -o posix //Runs Bash in POSIX mode 

What follows, is a list of the changes of interactions in POSIX mode:

When a command in the hash table no longer exists, Bash will re-search $PATH to find the new location. This is also avaiable with 'shopt -s checkhash'

The message printed by job control code and builtins when a job exits with a non-zero status is 'Done(status)'

The message printed by the job control code and builtins when a job is stopped is 'Stopped(signature)', where signature, is for example, SIGSTP.

Alias expansion is always enabled, even in non-interactive shells.

Reserved words appearing in a context where reserved words are recognized do not undergo alias expansion.

The POSIX Ps1 and Ps2 expansions of '!' to the history number and '!!' to '!' are enabled, and parameter expansion is performed on the values of PS1 and PS2,
regardless of the setting of the promptvars option.

Tilde expansion is only performed on assignments preceeding a command name, rather than on all assignment statements on the line.

The default history file is ~/.sh_history (This is the default value of $HISTFILE)

Redirection operators do not perform filename expansion on the word in the redirection unless the shell is interactive.

Redirection operators do not perform word splitting on the word in the redirection.

Function names must be valid shell names. That is, they may not contain characters other than letters, digits and underscores, and may not start with a digit.
Declaring a function with a invalid name, causes a fatal syntax error in noninteractive shells.

POSIX special builtins are found before shell functions during command lookup.

When printing shell function definitions(e.g by type), Bash does not print the function keyword

Literal tildes that appear as the first character in element of the PATH variable are not expanded as described under Tilde Expansion

The time reserved word may be used by itself as a command. When used in this way, it displays timing statistics for the shell and its completed children.
The TIMEFORMAT variable controls the format of the timing information.

When parsing and expanding a ${...} expansion that appears within double quotes, single quotes are no longer special and cannot be used to quote a closing
brace or other special character, unless the operator is one of thoose defined to perform pattern removal. In this case, they do not have to appear as
matched pairs.

The parses does not recognize time as a reserved word if the next token begins with a '-'

The '!' character does not introduce history expansion within a double-quoted string, even if the histexpand option is enabled.

If a POSIX special builtin returns an error status, a non-interactive shell exits. The fatal errors are those listed in the POSIX standard, and include things
like passing incorrect options, redirections errors, variable assignment errors for assignments preceeding the command name, and so on.

A non-interactive shell exits with an error if a variable assignment occurs when no command name follows the assignment statements. A variable assignment error
occurs, for example, when trying to assign a value to a readonly variable.

A non-interactive shell exits with an error status if the iteration variable in a for statement or the selection variable in a select statement is a readonly
variable.

Non-interactive shells exit if <filename> in . <filename> is not found

Non-interactive shells exit if a parameter expansion error occurs

Non-interactive shells exit if there is a syntax error in a script read with the . or source builtins, or in a string processed by the eval builtin.

Process substitution is not available.

While variable indirection is available, it may not be aplied to '#' and '?' special parameters.

When expanding the '*' special parameter in a pattern context where the expansion is double-quoted does not treat the $* as if it were double-quoted

Assignment statements preceeding POSIX special builtins persist in the shell environment after the builtin completes.

Assignment statements preceeding shell function calls presist in the shell environment after the function returns, as if a POSIX special builtin
command had been executed

The command builtin does not prevent builtins that take assignemnt statements as arguments from expanding as assignment statements, when not in POSIX mode,
assignment builtin loses their assignment statement expansion properties when preceeded by a command.

The bg built uses the required format to describe each job placed in the background, which does not include an indication of wether the job is the current job
or previous job.

The output of 'kill -l' prints all the signal names on a single line, seperated by spaces, without the 'SIG' prefix.

The 'kill' builtin, does not accept signals names with a prefix of 'SIG'.

The export and readonly builtin commands display their output in the format required by POSIX.

The trap builtin displays signal names without the leading SIG.

The trap builtin doesn't check the first argument for a possible signal specifications and revert the signal handling to the original disposition it is, unless the
argument solely consists of digits and is a valid singal number. If users want to reset the handler for a given signal to the original disposition,
they should use '-' as the first argument.

The . and source builtins do not search the current directory for the filename argument if it is not found by searching PATH.

Enabling POSIX mode has the effect of setting the inherit_errexit option, so subshells spawned to execute command substitution inherits the value of the -e options from
the parent shell. When the inherit_errexit option is not enabled, Bash clears the -e option in such subshells.

When the alias builtin display alias definition, it does not display them with a leading 'alias' unless the -p option is supplied.

When the set builtin is invoked without options, it does not display the shell function names and definitions.

When the set builtin is invoked without options, it displays variable values without quotes, unless they contain shell metacharacters, even if the result contains
nonprinting characters.

When the cd builtin is invoked in logical mode, and the pathname constructed from $PWD and the directory name supplied as an argument does not refer to an existing 
directory, cd will fail instead of falling back to physical mode

NOTES ON MODES: 

Modes, are the adresses that a x86 CPU can process, as stands - there are 3 different "kinds" of adresses and modes it can go into:

Physical is the actual adresses used to select the byte in memory. Used for things like segment base adresses, interuppt descriptor table, global
descriptor table.

Logical these are the adresses used most of the time when paging is disabled. They're converted to physical adresses by adding the respective segments (physical)
base address.

Virtual these are the adresses used by most instructions when paging is enabled. These are translated into logical adresses using page directories and tables.


the pwd builtin verifies that the value it prints is the same as the current directory, even if it is not asked to check the file system with the -P option.

When listing the history, the fc builtin does not include an indication of wether or not a history entry has been modified.

The default editor used by fc is ed.

The type and command builtints will not report a non-executable file as having been found, though the shell will attempt to execute such a file if it is
the only so-named file found in $PATH

The vi editing mode will invoke the vi editor directly when the 'v' command is run, instead of checking $VISUAL and $EDITOR

NOTES on $VISUAL and $EDITOR: $VISUAL is a variable access to attempt to run a Graphical mode, where as of if it fails, $EDITOR is run instead, then.
Some commands demand the $EDITOR variable, such as the fc command.

When the xpg_echo option is enabled, Bash does not attempt to interpet any arguments to echo as options. Each argument is displayed, after escape
characters are converted.

The ulimit builtint uses a block size of 512 bytes for the -c and -f options.

The arrival of SIGCHLD when a trap is set on SIGCHLD does not interuppt the wait builtin and causes it to return immediately. The trap command is run
once for each child that exits.

NOTE ON SIGCHLD: When a child process is terminated or cancelled, it sends a SIGCHLD signal, which is by default ignored. 

What follows is an example of how to constructor a catcher for SIGCHLD signals:

#include <stdio.h>
#include <signal.h>
#include <sys/wait.h>
#include <sys/resource.h>

void proc_exit()
{
	int wstat;
	union wait wstat;
	pid_t 	pid;

	while(TRUE){
				pid = wait3 (&wstat, WNOHANG, (struct rusage *)NULL);
				if(pid == 0)
						return;
				else if (pid == 1)
						return;
				else
						printf ("Return code: %d\n", wstat.w_retcode);
	}
}

main()
{
	signal (SIGCHLD, proc_exit);
	switch(fork()){
			case -1:
					perror ("main: fork");
					exit(0);
			case 0:
					printf ("I'm alive (temporarily)\n");
					exit (rand());
			default:
					pause();
	}
}

The read built in may be interuppted by a signal for which a trap has been set. If Bash recieves a trapped signal while executing read, the trap
handler executes and read returns an exit status greater than 128.

A short list of error codes:

1 : This is a Catchall for general errors code, which means arithmetic errors and impremisable errors (error of operations) etc.

2 : Misuse of Shell builtins (according to Bash documentation), which basically means missing of Keywords or commands, or a permission problem. 

126 : Command invoked cannot execute, this is a permission problem or a command that is unexecutable 

127 : Commant not found, the 404 of the bash error lineup, possible problem with $PATH or a typo

128 : Invalid argument to exit, occurs if a errornous exit numeral was given, was exit only accepts positive integers from 0-255

128+n : Fatal error signal "n", $? returns 137 (128 + 9)

130 : Script terminated by CTRL+C, Ctrl+C is fatal error signal 2, as per 128+2

> 255 or < 0 : Caused when we try to exit with a invalid numeric range command

For praxis sake, we should restrict ourselves to integers between 64 and 113 upon errors, and 0 upon success.
This is to better confirm with C and C++

Bash removes an exited background process's status from the list of such statuses after the wait builtin is used to obtain it.

set -o verbose or set -v //Displays command lines as Bash reads them

set -o vi //Specifies vi editing mode for commandline editing

shopt -s xpg_echo //Causes echo to expand backslashes escape sequences without the need for the -e option

set -o xtrace //Turns on shell debugging

When we run things in a non-interactive shell (i.e run shell scripts) or we work interactively (we write commands)
bash needs to read a line before it can start processing it, if there are several lines, Bash will automatically
recognize that you are doing multilines, until it detects a EoL notation, as follows:

$ echo 'hi
> end' //the > is the PS2 prompt here, as in, showing for secondary prompts or trailing lines of where you have multiline commands etc.

$ function hello(){
>echo hello there //A function spans several lines, because of the braces declarations
>}
$

When the shell analyzes the commandline for execution, it parses the parts on the commandline into tokens (a token is a word)

This expansion is accounting for BASH expansion. Only brace expansions, word splitting and pathname expansions can change the number of words
in a command(Except for the variable "$@" (covered later)).

Expansion ordering is as follows:

1. Brace expansion
2. Tilde expansion (~)
3. Parameter and Variable expansion
4. Arithmetic Expansion
5. Command Substitution
6. Word splitting
7. Pathname expansion
8. Process substitution

It then proceeds to remove 's, "'s and /'s that are not result of an expansion

NOTE: Shell recognizes input and output redirection BEFORE evaluating variables, as follows:

$ SENDIT="> /tmp/saveit"
$ echo xxxx $SENDIT
xxxx > /tmp/saveit
$ cat /tmp/saveit
cat: /tmpt/saveit: No such file or directory

Due to the ordering, it checks for redirection, finds none, evalutes SENDIT, replaces, runs the echo command.
Thus, a file is never created, due to ordering of processing.

Single quotation surpresses all types of expansions, whilst " quotation marks permit expansion and variable expansion but does not allow any other.

Brace expansion works akin to a placed and controlled form of echo, as follows:

$ ls //gets no results, because there are no files in the cd
$ echo chap_{one,two,three}.txt
chap_one.txt, chap_two.txt, chap_three.txt

Since there are no files in the directory, the echo does not match against any filenames.


Both the preamble and the postscript are optional, in terms of a {} notation. In this case, the preamble is the chap_ part and postscript is the .txt part

For the shell to reat treat left and right braces specially and for brace expansion to happen, at least one comma and no unqoted whitespace char must be inside the
{}. You can also next {} expansions

What follows is an example of how to copy 4 files located in the same directory:

$ cp /usr/local/src/C/{main,f1,f2,tmp}.c

We can also use it to create directories with related names:

$ ls -F
file1 file2 file3
$ mkdir vrs{A,B,C,D,E}
$ ls -F
file1 file2 file3 vrsA/ vrsB/ vrsC/ vrsD/ vrsE/

the -F option to ls causes to display / on dirs and * on exe's

Note: Using amibigous file references, instead of curly brace notations, causes errors, as follows:

$ rmdir vrs* //remove all dirs that begin with vrs
$ mkdir vrs[A-E] //makes one called vrs[A-E] cause it did not find a dir called that already
$ ls -F
file1 file2 file3 vrs[A-E]

When we have the ~ on the command line, the Shell interpets it as being a value of the home directory.
If we have ~<username> on a command line, it replaces the value of the home variable user name with the one given.
What follows are some examples of interaction with tilde ~:

$ echo $HOME
/home/max
$ echo ~
/home/max
$ echo ~/letter
/home/max/letter
$ cp ~/letter . //copes the file letter from home dir to cd

$  echo ~zach
/home/zach
$ echo ~root
/root
$ echo ~xx
~xx

If the string following the ~ character is not a valid username and not null, it performs on substitution

Tildes are used in directory stack manipulation.

~+ is a synyonym for PWD (name of the cwd)

~- is a synonym for OLDPWD(previous wd)

Whenever we write $ and do not follow it with ( , allows for substitution on parameters and variables.
Parameters are as follows: command-line and positional parameters, special parameters. 
Variables includes: user-created variables and keyword variables.

If the $ is escaped with \ , or if its quoted literaly with ' , it will not substitute.
If it's quoted loosely, with ", it still allows for expansion of parameters and variables.

In bash, if we wish to make a arithmetic expression, we must encase it with $((<expression>)), where it replaces the value within the (()) with the result of the
expression.

we can use the $((<expression>)) in place of any place where we would want a number.

Arithmetic expressions uses Integers, in Bash. If we do not use integers, such as strings, we must convert them to integer first.

We do not need to enclose a expression in "s, because it does not perform file expansions on it.

What follows, is a example of interaction in arithmetic expressions:

$ cat age_check
#!/bin/bash
echo -n "How old are you? " //ask how old the user is
read age //reads input and stores it in the age variable
echo "Wow, in $((60-age)) years, you'll be 60!" //echo output

$./age_check
How old are you? 55
Wow, in 5 years you'll be 60!

We can also do multiplication in Arithmetics, as follows:

$ echo There are $((60*60*24*365)) seconds in a non-leap year.
There are 31536000 seconds in a Non-leap year.

What follows, is an example of how many lines there are in a file
The commands it uses, are as follows:

wc -l (word count) //leads to how many lines there are in the file, in the colums 1 through 4
cut -c1-4 //Extracts the 4 first columns 

$ wc -l letter.txt
351 letter.txt

$ wc -l letter.txt | cut -c1-4 //4 first columns of command gets fed out
351 


what follows is an example of how to calculate how many pages this would fill, and by adding 1, we fill up the small margin of integer division (due to remainders being
destroyed):

$ echo $(( $(wc -l letter.txt | cut -c-1-4)/66 + 1)) //we get 351/66 + 1, remainder is stripped from 6.2222 so we get 6
6

when we do a arithmetic expression with substitution in terms of $x and $y inside the (()) part, the $ for the x and y are optional:

$ x=23 y=37
$ echo $((2*$x + 3*$y))
157
$ echo $((2*x + 3*y))
157 //same result as above, because the $ inside of the ()'s are optional

Another way to skip the naming of the file, is by directing the result into wc instead, as follows:

$ wc -l < letter.txt
 351 //no file name


example of assignment of a variable:

$ numpages=$(($(wc -l < letter.txt)/66 + 1))

In BASH, we can also use the key command of let, to do arithmetic expressions as follows:

$ let "numpages=$(wc -l < letter.txt)/66 + 1" // this is the same as the ones above, the "'s prevent word splitting 

The last expressions evalutes the result of the last expression, if the value is 0, the exit status of let is 1, otherwise it's 0

we can supply let with multiple commands on a single line, as follows:

$ let a=5+3 b=7+2
$ echo $a $b
8 9

when we do arithmetics with let, we can optionally omit the $ albeit it is good practice to not omit them.

In bash, we can refer command substituion with the $ sign, as follows:

$<command>

or

`<command>` //This is a older syntax for Bash, and the only legitimate one for TCSH.

Whenever we run command substitution we substitute the command and run it in a seperate spawned shell

an example of this follows, where we run substitutioin on a command where it purges the result from punctuation etc. and return it as a 
argument for another command:

$ echo $(pwd)
/home/max

The next example, assigns the output of pwd to where and displays the contents:

$ cat where
where=$(pwd)
echo "You are using the $where directory"
$ ./where
You are using the /home/zach directory

The following command, uses find to locate files with the name README in the directory tree rooted at the working directory.
This list of files is standard output of find and becomes the list argument to ls

$ ls -l $(find . -name README -print)

or with the older syntax:

$ ls -l `find . -name README -print`


What follows is a nested example of calling with command substitution, which gives us a list of all the README files that exceeds the size of the
./README file:

$ ls -l $(find . -name README -size +$(echo $(cat ./README | wc -c )c) -print)

If IFS is set to null, bash does not split words.

The following type of commands, splits words:

parameter and variable expansion

command substitution

arithmetic expansion

When we do expansion on pathname, unless we have noglob set, it does this whenever we use a wildcare character or use a ambigious reference.

The following characters account for making a pathname ambigious:

* [ ] ? 

If Bash cannot find any file that matches the specified token, it leaves the token alone and passes it as is, to the program.

This is only true for BASH, the TCSH raises an error, if we do this.

What follows is an example of where a command goes unexpanded, because it finds no matches:

$ ls
tmp1 tmp2 tmp3
$ echo tmp*
tmp1 tmp2 tmp3
$ rm tmp* //remove all tmp beginning files
$ echo tmp*
tmp* //since it found no match, it literally performs echo on the command regardless of *

in tcsh, it would simply raise an error:

tcsh $ echo tmp*
echo: No match

a . that starts a pathname or follows a / in a pathname, must be matched explicitly unless we have set dotglob.
We can make ambigious statements match regardless of casing, with the nocaseglob feature

What follows is an example of expansion:

$ echo tmp* $max //normal expansion occurs
tmp1 tmp2 tmp3 sonar 

$ echo "tmp* $max" //only the variable can expand, due to limitation of the "" quoting
tmp* sonar

$ echo 'tmp* $max' //'' prevents all expansion of all kinds, so the quote becomes literal
tmp* $max

if the value of a variable happens to contain a ambigious file reference, it does not expand it, in context of "" quoting or '' quoting.

Since expansion occurs upon reference of a variable, what follows, is an illustration of context not expanding the ambigious reference:


This shows the three levels of expansions in effect:

$ ls letter*
letter1 letter2 letter3
$ var=letter* 
$set | grep var
var='letter*' 
$ echo '$var' //performs no expanion due to ''
$var
$ echo "$var" //no pathname expansion occurs, due to "", thus only variable expansion occurs
letter*
$ echo $var //performs variable expansion first, then pathname expansion
letter1 letter2 letter3

A special thing in BASH, is that we can replace filename arguments with processes. 
An argument with the syntax of <(command) causes command to be executed and output is piped through a named pipe.(FIFO (first in, first out)).
The shell replaces that argument with the name of the pipe.

If this argument is then used, as the name of an input file during processing, the output of the command is read.
The same, goes, but inverse, for the >(command) (same principle, except for output)

The following example, mixes sort and -m (merge, works only with lists that are already sorted), take output of two files, and put em in one list.
The output is sorted through a regex, as follows:

$ sort -m -f <(grep "[^A-Z]..$" memo1 | sort) <(grep ".*aba.*" memo2 |sort)

We need some explonations on the terms of the grep here, as it is a special case of interaction with grep:

* means 0 or more matches against the PRECEEDING character in the text search. It's a greedy quantifier.
.* means 0 ore more matches against anything, meaning that ".*aba.*" matches against any word that contains or ends or starts (so just contains at all),
the word "aba".

So, to simplify the second part, we find <(grep (ANYTHING CONTAINING "aba") memo2 |sort), so we find anything that contains aba, in memo2, which has been
piped through sorting, so it's sorted.

We then pipe this, back to the file that we merge together with the first part, which is as follows:

(grep "[^A-Z]..$" memo1 | sort) //This basically means, (grep "ANYTHING NOT BEGINNING WITH A LETTER" in the parent directory that begins with $ memo1 |sort)
So, basically we grep variables to be expanded from memo1, from the sorted list of memo1

-f commits folding for sake of comparison

THE TC SHELL:

The TC shell (also spelled tcsh), is a C based-shell that uses different functions from Bash. They do have a lot in common, but some things
differs tcsh from Bash.

The tcsh is a interactive command interpeter as well as a high-level programming language.

To set variables in tcsh, we must commmit a slightly different syntax, compared to Bash:

set variable = value 

If we reference a variable that has no declared value, tcsh signifies an error (BASH does not)

Note: Do not use Tcsh as a programming language, as Bash is better for programming.

if the first character in a script is # and the next char is NOT !, it runs the script in tcsh.
Otherwise, it calls on Bash or Dash through a sh link and runs it through them instead.

To get rid of the return statement after a echo call result, we can call echo with -n or \c to get rid of it.
in Bash, only -n is allowed.

if we run a file which's script DOES NOT begin with #, even if we are running the tcsh shell currently, we end up running the
bash shell on it, then.

Since set works differently, what is showcased in the following example, is that of a shell script being run by Bash and then
malfunctioning on that basis:

tcsh $ cat user_in //showcase the contents of the script
echo -n "Enter input: "
set input_line = "$<" //will fail in bash, due to space
echo $input_line

tcsh $ user_in //When run in bash, without explicit orders of being run to tcsh, it malfunctions and does not wait for a prompt
Enter input:

tcsh $ tcsh user_in //explicit call to run it with tcsh due to tcsh declaration before script specified
Enter input: here is some input //Now works correctly
here is some input

If we wish to find out what shell we are running, we could do a number of things:

we could finger ourselves (as in, run finger on the user of which we are currently running with)

run the ps command //will display tcsh, bash, sh(linked to bash), or perhaps another shell

The name of our login shell is also stored in the /etc/passwd file

If we wish to change shell, we can use the chsh (change shell) utility.

What follows is an example of where we change from bash to tcsh:

bash $ chsh
Changing shell for <user>.
Password:
new shell [/bin/bash]: /bin/tcsh //we are prompted to write in what shell we want to change to, here we change to the tcsh shell
Shell changed.
bash $

This stays in effect until the next time you change your login shell.

if we are unsure of how to exit tcsh, we can press CTRL+D on an empty line.

if ignoreof is not set and has not been set in the startup file, then we can exit from any shell spawned with CTRl+D

if ignoreof is set, we can exit by a exit command. Or we can give the logout command, which causes us to log out from the login shell.
(This only logs us out from the login shell, not any other shell)

If ignoreof is set and we try to exit by CTRL+D, we get a message on how to exit.

To setup systemwide defaults for tcsh users, we can go root privlege and set the files of:

/etc/csh.cshrc 

and

/etc/csh.login

Both of these files, contains systemwide defining information, such as the settings of path, location to check for mail etc.

To establish variables that are local to our shell, we can access the following:

~/.tcshrc 

or, if ~/.tcshrc does not exist, it looks for ~/.cshrc

~/.cshrc

What follows, is an example of a ~/.tcshrc start up file that is customized to set several shell variables, put up two alises and adds two directories 
to path - one at the beginning of the list, the second one at the end of the list:

tcsh $ cat ~/.tcshrc
set noclobber
set dunique
set ignoreof
set history=256
set path= (~/bin $path /usr/games)
alias h history
alias ll ls -l

in ~/.history , the tcsh retrieves the history list. If the histfile variable exists, tcsh uses the file that histfile points to, instead of
the ~/.history file

login shells read and execute commands in ~/.login , the file contains commands that you want to execute once, at the beginning of each session.
We can use the setenv command to set some global variables in this file, and we can also define some terminal characteristics, in this file, if we wish:

tcsh $ cat ~/.login
setenv history 200
setenv mail /var/spool/mail/$user
if( -z $DISPLAY ) then
		setenv TERM vt100
	else
		setenv TERM xtrem
endif
stty erase '^h' kill '^u' -lcase tab3
date '+Login on %A %B %d at %I:%M %p'

This login file, checks to see if we are running a graphical version or a text version, if we are running a graphical one, 
it tells us what TERM we should use, otherwise it defaults to another one

it then runs stty to define what erase characteristics there should be
and to display the time of which when we logged in

When we logout from tcsh, it runs the /etc/csh.logout and ~/.logout files, what follows is an example of having the ~/.logout file setup so that 
it puts the system to sleep a short while and then pops up the date and time for logging out:

tcsh $ cat ~/.logout
date '+Logout on %A %B %d at %I:%M %p'
sleep 5

The features that both TCSH and the BASH shell, has in common, is as follows:

Command-line expansion (a.k.a substitution)

History

Alises

Job Control

Filename substitution

Directory stack manipulation

Command substitution

The order of which Tcsh scans for substitution or expansion, is as follows:

History substitution

Alias substitution

Variable substitution

Command substitution

Filename substitution

Directory stack substitution

The history function works as per usual in tcsh, with the history command showing a list of event ordering occuring.

If we wish to activate the prompt to make it so that it can access historical relevance in terms of occurance, we can give the following command:

set prompt = "! $ "

History expansion works as per Bash, in the following matter:

!! //the last command given

!328 //Execute event number 328

!?txt? //Executes most recent event containing <txt>

The following functions do not exist in bash:

u //Converts the first lowercase letter into uppercase

l //Converts the first uppercase letter into lowercase

a //Applies the next modifier globally within a single word

What follows, is a demonstration of chaining commands in tcsh:

tcsh $ echo $VERSION //Does not recognize
VERSION: Undefined variable.
tcsh $ echo !!:l:al //repeat last command, make first character lower letter, applies to all letters, since a is a global modifier
echo $version
tcsh 6.14.00 (Astron) 2005-03-25 (i486-intel-linux) options wide, nls, dl....

in addition to event designations (!<number>), you can acess history with the commandline editor, to access, modify and execute previous commands.

In BASH, when we talk about the history list, we talk about having the Variables of HISTSIZE and HISTFILESIZE
but in TCSH, we have the names of : history and savehist, instead.

history //Default history command for list of history in Tcsh, defaults to 100 elements

histfile // This is the location of the history file, ~./history is it's default value

savehist // This variable is the maximum amount of events saved between sessions. Default: not set

When we exit from the shell, the most recent commands are saved in your ~/.history file.
The savehist defines how many lines we save in the .history file, which is not nessecarily
the same as the history variable

if savehist is not set, tcsh does not save history information between sessions

The history and savehist variables, must be locally set with the set command, not setenv (setenv is a global modifier, set is a local modifier)

NOTE: the history variable allocates memory to save things, thus we should not keep this variable too high.

An example of how to set the startup value of history to be 500, is as follows:

go into the ~/.tcshrc startup file

tcsh $ set history = 500 //A history list of the 500 most recent commands

or to put a variable that is saved across sessions:

tcsh $ set savehist = 200 //saves across sessions, also put in the ~/.tcshrc startup file

If we wish, we can combine the two commands, into one line that does both:

tsch $ set history=500 savehist=200

After we have set the savehist, we can log in and log out again, to have the changes take effect in terms of the ones allocated in the
~/.tcshrc directory.

if we set the variable histlit, the history list will display the commands just as they were typed in, without any shell expansion
of the commands.

What follows, is a showcase of this:

tcsh $ cat /etc/csh.cshrc
...
tcsh $ cp !!:1 ~
cp /etc/csh.cshrc ~
tcsh $ set histlit
tcsh $ history
...
	31 9:35 	cat /etc/csh.cshrc
	32 9:35 	cp !!:1 ~ //copy with the [1:th] element from the last command with the destination dir of home
	33 9:35 	set histlist
	34 9:35 	history
tcsh $ unset histlit
tcsh $ history
...
	31 9:35 	cat /etc/csh.cshrc
	32 9:35 	cp /etc/csh.cshrc ~ //copy etc/csh.cshrc to home dir
	33 9:35 	set histlit
	34 9:35 	history
	35 9:35 	unset histlit
	36 9:36 	history

A difference between BASH and TCSH, is as follows:

!250w //Bash understands this as the 250:th command in history, where as of tcsh believes this is a string that begins with 250w
//if we were to rewrite it as 

!250 w //then tcsh understands that it's the 250th command, but it's a illegal operation in bash due to the space.
//we can also seperate the two, with braces, in both bash and tcsh

!{250}w //this will literally append w to the 250th command

Alises works like in bash, except the syntax for it, is slightly different:

alias name value //This is the general syntax of the alias in tcsh

The following creates an alias for ls:

tcsh $ alias ls "ls -lF" //creates an alias for ls

the tcsh allows us to expand commandline arguments, as well:

$ alias nam "echo Hello, \!^ is my name" // the \!^ is the part of which represents the first argument of the command
$ nam Sam
Hello, Sam is my name

$ alias sortprint "sort \!* | lpr" //the \!* expands all commandline arguments in the command

$ alias n2 "echo \!:2" //access the second argument of the command

To see a list of all current alises, we can give the following command:

alias //gives a current list of all the current alises active

alias <name> //to view a specific alias by the name designation

Some aliases in tcsh, has special meaning, and what follows, is a list of thoose commands and what they do:

			Is executed
beepcmd : Whenever this shell would normally ring the terminal bell. Allows for customization of different sounds in terms of the beep

cwdcmd  : Whenever we change to another working directory

periodic : Periodically, as determined by the number of minutes in the tperiod variable, if tperiod is unset or has the value of 0,
periodic has no meaning.

precmd : Just before the shell displays a prompt

shell : Gives the absolute pathname of the shell that you want to use to run scripts that do not start with #!

What follows, are again, some examples of aliases:

$ alias last echo \!:$ //the ! is quoted so that it's not interpeted when it's building the alias //This is an alias for displaying the last argument
$ last this is just a test
test
$ alias fn2 echo \!:2:t //displays the simple filename of the second command in the commandline
$ fn2 /home/sam/test /home/zach/temp /home/barbara/new
temp

Job control in tcsh is pretty much the same as in bash, except that it will display all the seperate job number for all seperate commands that belong
to a job, as follows:

tcsh $ find . -print | sort | lpr & grep -l zach /tmp/* > zachfiles &
[1] 18839 	18840 	18841
[2] 18876

As far as filename substitution goes, tcsh works identical to Bash, in terms of expansions:

* matches a string of 0-n length

? matches any single character

[] is a set of characters that can individually be matched, every char in that list is matched on a single char basis

~ is just home dir, as per in bash. NOTE: ~+ and ~- IS NOT SUPPORTED IN TCSH.

In tcsh, brace expansion {} is considered a filename substitution, even if the {} can expand into things that are not filenames

Matching against patterns of text in Tcsh, is refered to as globbing with a globbing pattern, if tcsh is unable to find one or more
of the occurences of the pattern, it reports an error. (unless said pattern is in {}) We can surpress this, with the setting of noglob

Directory stacks works the same as in Bash

Command substituion of $(...) does not exist in Tcsh, you must use the older syntax of  `...`

Redirection in terms of standard output and standard error is slightly different in tcsh, as you cannot redirect error with 2> , instead we have to use
the >& command which redirects standard output and standard error output:

tcsh $ cat x
cat: x : no such file or directory

tcsh $ cat y
This is y.

tcsh $ cat x y >& hold //bind x's and y's output to standard error and standard output respectively
tsch $ cat hold
cat: x : no such file or directory //standard error
This is y. //Standard output

in tcsh we cannot seperate redirection of standard output and standard error in an easy manner, thus, what follows is a proposed solution:

tcsh $ (cat x y > outfile) >& errfile
tcsh $ cat outfile
This is y.
tcsh $ cat errfile
cat: x: No such file or directory

What follows, is a process that takes a lot of time to complete (find) that we relegate to the background and and send its output to findout
//NOTE: This allows for files that we do not have permissions to read either, and since we redirected both standard error and standard output
//to the findout file, we can do as follows:

tcsh $ find / -name "*biblio*" -print >& findout & //find directories that have biblio in their name, print it, redirect the output and standard err into findout,
//relegate process to background

if we wish to see the files as they are added, we can give the tail command for the background process, as follows:

tcsh $ tail -f findout //tail with the -f option causes it to display lines as it goes
//to terminate the command, we must press the interuppt key, which is usually ctrl+C

We can use autocomplete in filenames by using tab in the commandline, as follows:

tcsh $ cat trig1A -> Tab (press tab) -> cat trig1A.302488

if two or more files match the pre-fix, tcsh will require more input to clear out ambiguity, as can seen in the following example:

tcsh $ ls h*
help.hist help.trig01 help.txt
tcsh $ cat h -> Tab -> cat help. (beep) //it cannot find further, since beyond help. it is ambigious

To find a list of matches so far, we can do ctrl+D:

tcsh $ cat help. -> Ctrl+D
help.hist help.trig01 help.txt
tcsh $ cat help. //redraws so we can finish typing

if the ~ character is the first of a word, tab attempts to expand the word to a username, as follows:

tcsh $ cd ~za -> Tab -> cd ~zach/  //Tab autocompleted the ~za to zach
tcsh $ pwd
/home/zach

We can use tab on command completion as well, albeit if tcsh fails to find a command in this manner, it will attempt to complete the command by checking the
path variable

An example of such interaction:

tcsh $ up -> TAB(beep) -> CTRL+D
up2date 	updateb 		uptime
up2date-config 	update-mime-database
up2date-nox 	updmap
tcsh $ up -> tab -> uptime -> RETURN

If we set the variable of autolist, we do not need to press CTRL+D to find the listings of current matches:

tcsh $ set autolist
tcsh $ up -> Tab(beep)
up2date 	updateb 	uptime
up2date-config 	update-mime-database
up2date-nox 	updmap
tcsh $ up -> t -> tab -> uptime -> RETURN

We can also set the autolist to ambigious, which autocompletes a variable to the longest prefix it can find that is a cohesive with the other ones:

tcsh $ set autolist=ambiguous
tcsh $ echo $h -> tab(beep)
histfile history home
tcsh $ echo $h -> i -> tab -> echo $hist -> Tab
histfile history
tcsh $ echo $hist -> o -> tab -> echo $history -> return
1000 

Whenever we speak about the factor of what the SHell recognized what to be what, it depends on context. As follows, we will see an example of this:

tcsh $ ls up*
updates
tcsh $ which updatedb ups uptime
/usr/bin/updatedb
/usr/local/bin/ups
/urs/bin/uptime
tcsh $ which up -> tab ->which updates //it autocompletes to updates, which is a non executable file
updates: Command not found. //Since it autocompleted to a file directory which is not executable, it returns an error.

When we are talking about editing commandline mode in terms of emacs and vim, we can use the following commands to change:

bindkey -v 

or

bindkey -e

if we give a empty bindkey command with no argument, we get a listing of all the current keybindings, as follows:

tcsh $ bindkey
Standard key bindings
"^@" 	-> set-mark-command
"^A" 	-> beginning-of-line
"^B" 	-> backward-char
"^C" 	-> tty-sigintr
"^D" 	-> delete-char-or-list-or-eof
...
Multi-character bindings
"^[[A" 	-> up-history /
"^[[B" 	-> down-history 
"^[[C" 	-> forward-char 
"^[[D" 	-> backward-char 
...
Arrow key bindings
down 	-> down-history
up 		-> up-history
left 	-> backward-char
right 	-> forward-char
home 	-> beginning-of-line
end 	-> end-of-line

IMPORTANT NOTE: ^ is a Ctrl (so ^B is ctrl+B)
the ^[ is alt (so ^[ is alt+B)

To produce a ^[[F, we would press: alt + [ + F

The above example, was for Emacs. To see the complete list of vi(m), we'd need to change bindkey with the v command and then rerun the bindkey command, as follows:

bindkey -v
bindkey //will display for vim, since vim mode has been set

There are two different isntances of correction of spelling : Before you press return and after you press return.

BEFORE RETURN:

When we run the correction for spelling, we can do several different commands, as follows: 

alt+$ //Attempts to spellcorrect the entire line
alt+S //attempts to spell the words to the right of the cursor
alt+s //Attempts to spell the words to the left of the cursor

What follows is an example of interaction in terms of spelling correction:

tcsh $ ls
bigfile.gz
tcsh $ gunzipp -> alt+s ->gunzip bigfele.gz -> alt+s -> gunzip bigfile.gz
tcsh $ gunzip bigfele.gz -> Alt+$ -> gunzip bigfile.gz
tcsh $ ecno $usfr -> Alt+$ -> echo $user

AFTER RETURN:

The variable called correct controls what tcsh attempts to correct or complete AFTER you press return and before it
passes the command line to the command being called. If correct is not set, tcsh does not correct anything.

tcsh $ unset correct
tcsh $ ls morning
morning
tcsh $ ecno $usfr morbing
usfr: undefined variable. //we get an error in the variable and not the command, because it expands variable name before running the command.

We can change the correct variable to define what kinds of things it should correct.
If we set it to cmd, it will only correct commands. If we set it to all, it corrects commands, variables and filenames. If we wish for it to complete
commands, we just give it the value of complete.

tcsh $ set correct = cmd
tcsh $ ecno $usfr morbing //Knowing errors

CORRECT>echo $usfr morbing (y|n|e|a)? y //only corrects the echo command, due to correct being set to cmd
usfr: Undefined variable

tcsh $ set correct = all
tcsh $ echo $usfr morbing

CORRECT>echo $user morning (y|n|e|a)? y //will correct everything, due to correct being set to all
zach morning

Note on (y|n|e|a) : y is for yes, n for no, e for edit, a for abort

An example of correction of cmd follows:

tcsh $ set correct=cmd
tcsh $ lx -l -> return(beep) //lx is spelled wrong, is meant to be ls
Correct>lex -l (y|n|e|a)? e //Since the wrong command is suggested for autocorrection, we opt for editing it
tcsh $ lx -l

another example:

tcsh $ set correct = complete
tcsh $ up 	RETURN
Ambigious command
tcsh $ up ->tRETURN -> uptime //We can either press tab for a list of commands that up matches against, or just type to seek to complete the command ourselves
4:45pm up 5 days, 9:54, 	5 users, 	load average: 1.62, 0.83, 0.33

tcsh stores variables as strings, but we can still interact with them as if they were numbers.

The @ builtin can evaluate integer arithmetic expressions.

Assignment of variable in tcsh can be done in three formats:

set //local, string

@ //local, "numeric", a number

setenv /global, string

An example of variable declaration:

tcsh $ set name = fred
tcsh $ echo $name
fred
tcsh $ set
argv 	()
cwd 	/home/zach
home 	/home/zach
name 	fred
path 	(/usr/local/bin /bin /usr/bin /usr/X11R6/bin)
prompt  $
shell 	/bin/tcsh
status 	0
term 	vt100
user 	zach

To remove a variable, we simply give the command of unset:

tcsh $ set name
tcsh $ echo $name

tcsh $ unset name
tcsh $ echo $name
name: Undefined Variable

example of using setenv:

tcsh $ setenv SRCDIR /usr/local/src
tcsh $ tcsh
tcsh $ echo $SRCDIR
/usr/local/src
tcsh $ exit

if we wish to find all the global variables in effect, we can give a no argument setenv command

note: Unset can unset both global and local variables, unsetenv can ONLY remove global variables

NOTE: Tcsh runs indexing with the beginning of 1 
Arrays in tcsh:

$ set colors = (red green blue orange yellow)
$ echo $colors //references the entire array
red green blue orange yellow
$ echo $colors[3] //references third element of the array
blue
$ echo $colors[2-4] //references with a range of 2 to 4
green blue orange
$ set shapes = ('' '' '' '') //accounts for being an array of 4 size, 4 nulls
$ echo $shapes
 				//empty due to 4 nulls
$ set shapes[4] = square
$ echo $shapes[4]
square

If we give the @ command without arguments, we find that it tells us of all the current shell variables.

@ variable-name operator expression

operators that are allowed for a @ statement, is as follows: =, +=, -=, *=, /=, %=

When we declare paranthesis in arithmetic operations, we need to include one of the following in the expression:

< > & |

What follows is an example of correct vs incorrect assignment of numeric variables:

tcsh $ @ $answer = 5 + 5

gives

anwswer: undefined variable //variable has not been set due to $answer being registered as a referal of a variable

or if answer is defined:

@: Variable name must begin with a letter

the correct way, is as follows

tcsh $ @ answer = 5 + 5

Rules for expressions:

Any missing argument or null argument is treated as 0

All results are decimal numbers

Except for != and ==, the operators act on numeric arguments

You must seperate each element of an expression from adjacent elements by a space, unless the adjacent element is &, |, <, >, ( or )

Some examples of @ in action:

$ @ count = 0
$ echo $count
0
$ @ count = (10 + 4) / 2
$ echo $count
7
$ @ result = ( $count < 5)
$ echo $result
0 //This accounts for as being a logical 0, i.e a logical false
$ @ count += 5
$ echo $count
12
$ @ count++
$ echo $count
13

post increment and post decerement can only occur in expressions containing 1 variable:

tcsh $ @ count = 0
tcsh $ @ count++
tcsh $ echo $count
1
tcsh $ @ next = $count++ //illegal due to attempt of assignment in regards to post increment or post decerement. Post increment/decerement must occur on a seperate line with 1 variable involved
@: badly formed number

Some interaction in terms of arrays:

$ set ages = (0 0 0 0 0)
$ @ ages[2] = 15
$ @ ages[3] = ($ages[2] + 4)
$ echo $ages[3]
19
$ echo $ages
0 15 19 0
$ set index = 3
$ echo $ages[$index]
19
$ echo $ages[6]
ages: Subscript out of range

braces work as in Bash, to serve as seperators:

$ set bb=abc
$ echo $bbdef //no seperation, hence literal interpetation of bbdef reference
bbdef: Undefined variable.
$ echo ${bb}def //seperation, treats ${bb} as one part, appending the def to the result of the echo

To find how many elements an array has, we can use the $#variable-name command.
Also, if we use the $?variable-name command, we find out if it's been set or not (logical 1 or 0).
Examples:

tcsh $ set days = (mon tues wed thurs fri)
tcsh $ echo $#days //echoes how many elements there are in the days array
5
tcsh $ echo $?days
1 //has been set
tcsh $ unset days
tcsh $ echo $?days
0 //has not been set

We can also read input from a user:

echo -n "Enter input: "
set input_line = "$<" //$< is reference to input line, "" assured literal interpetation instead of expanding to just being the first word of the commandline

Shell variables have different sources: Inherited from the Shell environment setting (global value), set by the shell, defined by the user.

Some variables are simply on and off values, others are strings.

Most of them, are defined in the ~/.login and ~/.tcshrc folders

What follows, is a list of some of the shell variables that exists:

arg : this is the array that accounts for the commands on a commandline. We can reference all by argv[*] or $*
We can write $argv[n] as simply $n where n is the number of the index.
This array has a 0 index, which is the calling program.

NOTE: BASH does not use the fullname, it only allows for the shorthand version

$#argv or $# : holds the amount of elements in argv (the commandline)

autolist : Controls command and variable completion

autologout : Enables automatic logging out if idle mode is detected. Default value for non-GUI envs is 60 mins. Default for gui env is unset

cdpath : The backup Array to cd. if cd does not find it's value in the wd, it searches the cdpath for the value instead.

We can find the cdpath in ~/.login , usually.

correct : cmd for commands, all for all, complete for completion. Operates on the values assigned after RETURN

cwd : The name of the cwd. If you access a directory through a symbolic link, cwd is set to the name of that link.

dirstack : keeps track of pushd, popd and dirs builtins here.

fignore : Array of suffixes that tcsh ignores during filename completion

gid : The shell sets the value of your group ID here. 

SEGMENT FOR GROUPS AND GROUPING:
Groups are the permission levels as defined as per the permission levels (as owner, groups and others dictates)

An example of showcasing ownerships:

$ ls -l /media/
drwxrwx--- 1 root vboxsf 16384 Jan 29 11:02 sf_Shared //owned by root and vboxsf

We can find out more specific terms in terms of grouping with the stat command:

$ stat -c %U /media/sf_Shared/ //ownage level of user

root

$ stat -c %G /media/sf_Shared/ //owning group

vboxsf

$ stat -c %A /media/sf_Shared/ //Access Rights
drwxrwx---

We can use find to find out what group or user owns things, as follows:

# find / -group group

# find / -user user

We can change the owner by the virtue of the chown command (like chmod, but for ownage of user)

A list of some useful files (note: do not change these manually)

/etc/shadow : Secure user account information

/etc/passwd : User acc info

//GSHADOW section

/etc/gshadow : Contains the shadowed information for group accounts

gshadow is a readonly file that requires root privs to get into.

it contains an encrypted password for each group, as well as group membership and administrator information.
Just as in the /etc/group file, each groups information is on a seperated line. 

Generally, the syntax is as follows, in the file:

<group name>:<encrypted password>:<group admins>:<group members>

Encrypted password: If set, allows non-members to type the PW and join the group by providing the newgrp command.
if this field is !, it means that no user can access it with the newgrp command. !! is the same, except it indicates
that a password has never been set. If the value is null, only group members can log into the group.


An example:

general:!!:shelley:juan,bob

A group called general, does not allow entry by non members, shelley is admin, juan and bob are users

//END OF gSHADOW SECTION

/etc/group : defines the groups to which users belong

/etc/sudoers : List of who can run what by sudo

/home/* : home dirs

To see all existing user accounts, we can run the passwd -Sa command as root.

NOTES ON THE PASSWD UTILITY:

Normal user may only change own pw, root can change others. This variable also defines period of password validity for associated pw's.

Password changes:

When a normal uses tries to change their pw, they may try to provide their pw once. Root forgoes this (to enable recovery of forgotten passwords)
if the Pw aging is within range (i.e the pw is not outdated), it is allowed. Otherwise, it exits.

If it passes, the user is prompted twice for a new password.

Each password that is provided to be changed to, must fullfill the complexity criteria, which is as follows:

6 to 8 chars, including one or more

lower case letters

digits 0 to 9

punctuation marks

NOTE: Do not inject system default erase or kill chars

What follows, is a list of commands for the passwd utility:

-a or --all //can only be used with -S and causes show status for all users

-d or --delete //Delete a user's password, setting it to null. Makes a account passwordless.

-e or --expire //Instantly runs the timer out on a users password. Forces making a new one upon next login session.

-h or --help //Displays help messages and exits

-i INACTIVE or --inactive INACTIVE //After a account has had a expired password for INACTIVE days, the account gets disabled.

-k or --keep-tokens //indicate password change should be performed only for expired authentication tokens (passwords). Keeps non-expired tokens.

-l or --lock //Locks a pw by disabling it, by virtue of adding a | to the beginning, making encrypted values impossible to match.
This does not disable the account, and can still be accessed with other authentications (such as SSH keys)
To disable an account, the admin must give the following command:

usermod --expiredate 1

users with a locked pw, cannot change their pw.

-n or --mindays MIN_DAYS //Sets the minimum amount of days between password changes to MIN_DAYS. If it's 0, the user can change their pw at will.

-q or --quiet //quiet mode. passwd utility runs without displaying any output.

-r or --repository REPOSITORY //change pw in REPOSITORY repository

-R or --root CHROOT_DIR //Apply changes in the CHROOT_DIR directory and use the configuration files from the CHROOT_DIR directory.

-S or --status //Display 7 fields about an acc. The 7 fields are as follows:

1 Login name of the user

2 If Pw is locked (L), no Pw (NP), or usable PW (P)

3 date of the last password change

4 minimum age

5 maximum age

6 warning period

7 inactivity period for pw

All of the above values, are expressed in days

-u or --unlock //Unlocks the pw of the named acc, resetting it to the value of the pre -l status (pre lock)

-w or --warndays WARN_DAYS //Number of days of warning before a pw change is required. WARN_DAYS option is nr of days for notice of expiration

-x or --maxdays MAX_DAYS //sets max amount of days of validity to MAX_DAYS

NOTE: users may not be able to change their pw, if the linux is run on a NIS server and they are not logged into it

the exit singals for the passwd utility, is as follows:

0 : success

1 : permission denied

2 : invalid combination of options

3 : unexpected failure, nothing done

4 : unexpected failure, passwd file missing

5 : passwd file busy, try again

6 : invalid argument to option

CHPASSWD <options>:
Reads in a list of user name and passwd's from standard input, and updates for said group.

formatting is as follows:

user_name:password //Note: if age is present, it's updated

by default, passwd's in the Linux is encrypted by PAM, which we can change with the -e -m or -o options for chpasswd.

When not using PAM to encrypt, it updates all passwords in memory, and then commits to system if there was no error.

if PAM is used, it skips every user that it was not able to update, and exits with a error code upon completion.

The following list of commands, applies to the CHPASSWD utility:

-c or --crypt-method METHOD : Use METHOD to encrypt the Passwds. The ones available are : DES, MD5, NONE and SHA256 or SHA512 (if your libc supports it)

//Note on SHA256 or SHA512 - System below Windows XP SP2 or older, do not support SHA256 or SHA512 protocols. SHA256/SHA512 are considered to be the most
secure form of encryption currently integrated.

-e or --encrypted : Supplied passwords are in encrypted form.

-h or --help : Displays help message and exits.

-m or --md5 : use MD5 encryption instead of DES if pws are not encrypted //NOTE: MD5 is not secure. Has proven to be breakable time upon time.

-R or --root CHROOT-DIR : applies changes to CHROOT-DIR and use the specified settings from that file

-s or --sha-rounds ROUNDS : Only works with SHA256 or SHA512. Defines how many rounds you wish to run it with. A minimum of 1k, maximum of 999.999.999.
Default values for min and max in ronds, is defined in SHA_CRYPT_MIN_ROUNDS and SHA_CRYPT_MAX_ROUNDS variables in /etc/login.defs

We can configure the range of the sha-rounds, if we wish, in the /etc/login.defs file:

SHA_CRYPT_MIN_ROUNDS <number> and SHA_CRYPT_MAX_ROUNDS <number> : These define how many rounds we configure for the SHA_CRYPT. 

If the min_rounds > max_rounds, then min_rounds is used. Default is 5000. If only one is defined, that value is used. If no specification
is set, default is used.

The more rounds we set, the harder it is to bruteforce, but the more performance it takes to authenticate users.

Note: Sha_Crypting only applies to Group paswords if it's supported, it does not apply to user pw's, User Pw's use PAM settings.

The files for chpasswd can be found in the following files:

/etc/passwd

/etc/shadow //shadow info about passwds

/etc/login.defs //shadow pw suite config

/etc/pam.d/chpasswd //Pam config for chpasswd

UMASKING:

Umasking is the default value of which can be defined for permissions, and can be set or unset, or shown.

What follows is the syntax for Umasking:

umask [-S] [mask] //[] notation means optional

-S : Accepts a symbolic representation of a mask or return one.

mask : if a valid mask is specified, the umask is set to this value. If no mask is specified, the current umask value is returned.

The umask values are as follows:

4 read
2 write
1 execute

The base umask for files is 0666 and the base umask for dirs is 0777

NOTE: Whenever we run the umask command, it actually sets the files creation umask to 0777, the 0666 for files is just because thoose are the bits it actually
uses. 



The general symbolic values of masks, is as follows:

[user class symbol (s)][permission operator][permission symbol(s)][,]...

A permission symbol is one of the 3: rwx

user class symbols are as follows: u (user), g (group), o (anyone else), a (all, equivilant to ugo)

Permissions operators are as follows:

+ : Allows specified permission, unspecified values untouched

- : Prohibits specified permission, unspecified values untouched

= : same as +, except if a value is unspecified, it's prohibited, so only the value related to = gets enabled, all else gets disabled


An example of a umask command:

umask u+w //Gives user default writing permissions

an example of defining multiple values in a command:

umask u-x, g=r, o+w //Remove default execute permissions for user, allow default group reading and disable rest of permissions, give others default writing rights

an example of prohibiting all access formats, for all users, by default upon file creation:

umask a=

In octal representation, the lineup is always ugo (user, group, other)

The list of octal representation contra symbolic values:

0 =  //note, the actual representation of no right, is just a empty space, in terms of symbolic references
1 = x
2 = w
3 = wx
4 = r
5 = rx
6 = rw
7 = rwx

What follows is a listing of values contra permissions:

Octal Umask        	 file 	 dir
0 					 rw 	 rwx
1 					 rw 	 rwx
2  					 r 		 rx
3 				     r       r
4 					 w 		 wx
5 					 w 		 w
6 					 x 		 x
7 					NONE 	NONE

Examples:

0777 - 0111, 0666 - 0111 //666, rwxrwxrwx for dir, rw-rw-rw for files

0777 - 0666, 0666 - 0666 //111, --x--x--x for dir, 000 --------- for files

Generally, umask is a global setting - Albeit we can bind a certain umask to dirs and one to files, if we wish, by virtue of binding a specific umask to
mkdir calls (assuming we make all of our dirs with mkdir), and then set the specific umask for a respective specific file variant.

We can call upon Python or Perl to access single files with specific umasks, if we wish. (Or write a script to handle umasking of a lot of files)
Note: This method still makes it so that whenever we create things, it's modified by the standard umask, so we have to reset that after we are done.
This approach is for one file at a time, if you want for several, you're going to need to write a script for that.

umask 000 //set umask to 0 to not interfere
python -c 'import os;os.mkdir("mydir",0701)' //call upon Python to call upon C, import os, use os to run mkdir, name the dir, set the octal umask
ls -l mydir

drwx-----x 2 user1 QPLADV 4096 Sep 16 10:28 mydir


python -c 'import os;os.open("myfile", os.O_CREAT,0604)' // for creating files

ls -l myfile

-rw----r-- 1 user1 QPLADV 0 Sep 16 10:32 myfile

Can be done in Perl as well:

perl -e "mkdir mydir, 0701" //make a dir with 0701 umasking
perl -MFcntl -e 'sysopen(my $h,"myfile", O_EXCL|O_CREAT,0604)'

Also, note, the unix utility of mkdir allows for umasking as well, as follows:

mkdir -m 750 <dirname> //basically is a umask of 0750 

CRYPT:
crypt or crypt_r is used for encrypting password data. What follows, is general syntax:

crypt(<key>, <salt>, <data>) //key is a usertyped password, salt is 2 chars from [a-zA-Z0-9./] that is used for perturbing the algorithm

If successful, it returns a pointer to the encrypted password. So this is basically a function that encrypts passwords.

NOTE: the crack() method checks for "Common pattern passwords", thus, you should not run with the most common and easy variants of passwords, as per such.

RETURNING TO VARIABLES, CONTINUING AFTER GID:

histfile : Holds the full pathname of the file that saves the history list between login sessions. The default is ~/.history

history : variable for size of the histfile

home or HOME : Variable for path of home

mail : Specifies files and directories of which mail is to be searched for. If a numeric argument preceds this, it checks it every <Numeric Argument> seconds.
Default is 10 minutes, otherwise.

owd : Equivilant to ~- , old working directory

path or PATH : The variable that holds the maps searched upon attempts of execution of a file.

We can set the path variable, as follows:

tcsh $ set path = ( /usr/bin /bin /usr/local/bin /usr/bin/Xll ~/bin . )

prompt : Holds the primary prompt value. Can be modified with commands to display different things. 
If this is unset, it is > in default mode and # in root mode.

If we wish to modify it, we can do as follows:

set prompt = '! $ '

The ! part of this, expands it into the current event number.

What follows, is a list of some of the commands for formatting a prompt:

%/ 	: Value of cwd

%~  : Same as %/, but replaces the path of the user's home dir with a ~

%! or %h or ! : Current event number

%d : day of the week

%D : Day of the month

%m : Hostname without the domain

%M : Full hostname, including the domain

%n : user's username

%t : Time of day to current minute

%p : time of day to the current second

%W : month as mm

%y : year as yy

%Y : year as YY

%# : A # sign if one is in root, otherwise >

%? : Exit status of the preceeding command

prompt2 : This variable holds the second prompt, which shows up when it awaits input on a multiline command

The secondary prompt, for tcsh, is %R? , where it replaces the %R with nothing and leaves us with ?. An example:

% echo "Please enter the three values \
? required to complete the transaction." //Entire line is escaped with ", causes literal interpetation, the ? is not written, its a secondary prompt

prompt3 : Holds the prompt used during asking for input regarding spelling correction. The default is:
CORRECT>%R (y|n|e|a)? where R is replaced by the corrected string

savehist : Specifies the number of commands saved upon logging out

Initialies with ~/.history as the historylist for next time upon running

shell : holds the name of the shell you are running

shlvl : Holds the level of the shell. This variable is increased by 1 for each intiialized shell, and reduces by 1 for each closed shell.
Basically how many shells we are running.

status : Contains the exit status of the last command, similar to the $? in Bash

tcsh : Holds the version number of the tcsh we are running

time : Provides two functions, one which has automatic timing of commands using the time builtin and the format used by time. 

We can set this variable to either a simple number or an array, holding a numeric value and a string.

The numeric value is run to check the automatic timing, any command that takes more than the defined amount of seconds by this variable,
displays the commands statistics when it finishes execution. A value of 0 results in statistics being displayed after every command. The string
controls the formatting of the statitics using formatting sequences.

What follows, is a list of time formatting sequences:

%U : Time the command was spent running user code, in CPU seconds (user mode)

%S : Time the command spent running system code, in CPU seconds (kernel mode)

%E : Wall clock time (total elapsed) taken by the command

%P : Percentage of time the CPU spent on this task during this period, computed as (%U+%S)/%E

%W : Number of times the command's processes were swapped out to disk

%X : Average amount of shared code memory used by the command, in kb

%D : Average amount of data memory used by the command, in kb

%K : Total memory used by the command (as %X+%D), in kb

%M : Maximum amount of memory used by the command, in kb

%F : Number of major page faults (pages of memory that had to be read from disk)

%l : Number of input operations

%O : Number of output operations

By default, the time variable uses the following String:

"%Uu %Ss %E %P%    %X+%Dk   %I+%Oio  %Fpf+%Ww"

which generates:

tcsh $ time
0.200u 	0.340s 	17:32:33.27 0.0% 		0+0k 0+0io 	1165pf+0w

This is useful for seeing the performance of operations.
If there are many page faults and swaps, The system probably does not have enough memory and needs more memory allocated.

tperiod : Controls how often, in minutes, the shell executes the special periodic alias

user : The shell sets this variable to your username

version : The shell sets this variable to contain detailed information about the version of tcsh
that the system is running.

watch : Set to an array of user and terminal pairs to watch for logins and logouts. 
the any command searches for as said, any.

An example follows:

(zach tty$1 any console $user any) watches for zach on tty$1, any user who access the system console and logins from people who login on your account

A numeric value can preceed these, to control how often the display is prompted. If we set it to 1, it is set to check every 1 minute. Reports are displayed
before the next prompt shows up.

The log builtin forces a check immedeatly when it is run.

We can control the format of watch messages, by virtue of the who command, as follows:

%n : Username

%a : Action was taken by the user

%l : terminal on which the terminal took place

%M : Full hostname of remote host (or local if none), from which the action took place

$m : hostname without domain name

The default for watch messages, when it is unset, is as follows:

"%n has %a %l from %m"

which can produce, for instance:

sam has logged on tty2 from local

$ : This variable holds the PID number of the current shell, use it as $$

The following list, is of variables that act as switches, i.e, they just need to be turned on:

autocorrect : Causes shell to attempt to autocorrect spellings before attempts of auto completion

dunique : Normally pushd blindly pushes the new wd onto the dir stack, meaning that you can end up with many duplicate entries in the stack.
Set dunique to cause the shell to look for and delete any entries that duplicate the one it is about to push.

echo : Causes the shell to display each command before it executes that command. Set echo by calling tcsh with the -x option or by using set

filee : Enables filename completion when running tcsh as csh (and csh is linked to tcsh)

histlit : Displays the literal version of the command in history, instead of expanded

ignoreeof : Prevents exit by CTRl+D, must declare exit or logout to get out of a shell.

listjobs : Causes the shell to list jobs whenever a job is suspended.

listlinks : Causes the ls-F builtin to show the type of file each symbolic link points to instead of marking the symbolic link with an @ symbol.

loginsh : Set by the shell if the current shell is running as a login shell.

nobeep : Disable all beeping by the shell.

noclobber : Prevents you from causing accidental overwriting in regards to redirected output and standard error output. Also prevents us from
creating a file when we attempt to append output to a non-existing file. To override noclobber, add an exclamation point to the symbol you
use for redirecting or appending output (for example, >! or >>!)

How noclobber works:

				Noclobber off 						Noclobber on

x > <fileout> : Redirects standard output 			Redirects standard output from process x to <fileout>.   //Basically like write
				from process x to <fileout> 		Gives an error if <fileout> exists and does not overwrite
				. Overwrites <fileout> if it
				exists

x >> <fileout> : Append instead of write. 			Appends if exists, error if does not exist and does not create file. //basically like append


noglob : Prevents the shell from expanding ambigious file names. Allows you to use *, ?, ~ and [] literally on the command line or in scripts, without escaping them.

nonomatch : Causes the shell to pass an ambigious file references that does not match a filename to the command being called. The shell does not expand the file reference.
When you do not set nonomatch, tcsh generates a No match error message and does not execute the command. What follows is an example:

tcsh $ cat questions?
cat: No match
tcsh $ set nonomatch
tcsh $ cat questions?
cat: questions?: No such file or directory 

notify : When set, tcsh sends a message to the screen immedeatly whenever a background job completes. Ordinarily tcsh notifies you about job completion just before
displaying the next prompt. 

pushdtohome : Causes a call to pushd without any arguments to change directories to your home directory (equivilant to pushd-)

pushdsilent : causes pushd and popd not to display the the dir stack.

rmstar : Causes the shell to request confirmation when you give an rm * command.

verbose : Causes the shell to display each command after a history expansion. Set verbose by calling tcsh with the -v or by the set command.

visiblebell : Causes audible beeps to be replaced by flashing the screen.

CONTROL STRUCTURES:

if statements can be written as :

if (expression) simple-command

A simple if in tcsh only allows for simple commands ; that means no pipes, no chain of commands.

If we wish to do such, we could instead use the if...then command. 

An example of if usage, follows:

tcsh $ cat if_1
#!/bin/tcsh
#Routine to show the use of a simple if control structure

if ( $#argv == 0 ) echo "if_1: There are no arguments" //if the reference of argv numeral is 0, that means 0 commands were issued with the if statement, and we then
//echo out that if_1: There are no arguments.

The if_1 script is a simple script that checks to see if a if statement has 0 arguments upon being called.

We can also return states of files, as following per this general syntax:

-n filename

If we recieve a logical 1, it is true, if it is a logical 0, it's false.

An example of using this simple script follows, which checks to see if a file is of a simple format (as in, no special device or printer etc., just a file):

tcsh $ cat if_2
#!/bin/tcsh
if -f $1 echo "$1 is an ordinary or directory file."

What follows, is a list of all the commands an if can take on, in a tcsh shell:

<text> : Our own set of text

b : File is a block special file

c : File is a character special file

d : File is a directory file

e : File exists

f : File is an ordinary or directory file

g : File has set-group-ID bit set

k : File has sticky bit set

l : File is a symbolic link

o : File is owned by user

p : File is a named pipe (FIFO structure, first in first out)

r : The user has read access to the file

S : File is a socket special file (? Check up later)

s : File is not empty (has nonzero size)

t : File descriptor (a single digit replacing filename) is open and connected to the screen (? Look into)

u : File has set-user-ID bit set (? Check up)

w : User has write access to the file

X : File is either a builtin or an executable found by searching the dirs in $path 

x : User has execute rights for specified file

z : File is 0 bytes long (empty file)

We can chain commands that make sense, as follows:

-ox <filename> //Tells us if we are the owner of the file and we have execution rights of it.

This is the same thing as

-o <filename> && -x <filename>

Some operators return useful information about a file other than reposting true or false. 
They use the same format as the statements above -<option> <filename>, a list of these operators, follows:

A : The last time the file was accessed. *Time measured in seconds from the epoch (usually at the start of January 1, 1970)

A: : The last time the file was accessed displayed in human-readable format

M  : The last time the file was modified.*Time measured in seconds from the epoch (usually at the starto f January 1, 1970)

M: : The last time the file was modified displayed in human-readable format

C  : The last time the file's inode was modified.* (? check up inode) *Time measured in seconds from the epoch (usually at the start of January 1, 1970)

C: : The last time the file's inode was modified, displayed in human-readable format.

D  : Device number for the file. This number uniquely identifies the device (a disk partition) on which the file exists.

I  : Inode number for the file. The inode number uniquely identifies a file on a particular device.

F  : A string of the form device:inode. This string uniquely identifies a file anywhere on the system.

N  : Number of hard links to the file.

P  : The file's permissions, shown in octal, without a leading 0 (basically we get the umask numbering of permissions)

U  : The numeric user ID of the file's owner

U  : Username of the file's owner

G  : Numeric id of the file's group

G: : Name of the file's group

Z  : Number of bytes in the file

You can use only one of these operators in a given test, and it must appear as the last operator in a multiple-operator sequence.
Because 0 can be a valid response from some of these operators (such as size of a File), the failure state returned is -1 instead of 0.
The one exception is F - if it cannot find the Device:inode, it returns a : , .

If we wish to try these out, outside of a control structure expresion, we can use the filetest command, as follows:


NOTE: the if_1 is a custom made script in this case, to showcase interactions and control structures

tcsh $ filetest -z if_1 //checks to see if the file is empty, as in 0 kb
0  //it returns 0, thus it is false
tcsh $ filetest -F if_1 //Checks to see what device and inode the file belongs to
2051:12694 //device:inode displayed
tcsh $ filetest -Z if_1 //Checks to see the size
131 //131 kb


We can use the following command to navigate lines:
goto

It has the following syntax:

goto <label>

An example of how to use goto follows:

tcsh $ cat goto_1
#!/bin/tcsh //comments
#
# test for 2 arguments
#
if ($#argv == 2) goto goodargs //if the if statement triggers, we have a goto statement that redirects control to the label of goodargs
echo "Usage: $0 arg1 arg2"  //only triggers if there is more or less than 2 commands. Echoes "Usage: command name arg1 arg2"
exit 1 //Exit with an error status
goodargs: //label to go to if everything went well
...

We can use the command of onintr to transfer control when we interuppt a shell script. The following format is used:

onintr <label>

onintr can be used to clear out operations upon declared interupption through the interuppt keys

The following script runs a loop until the point of interuppt keys being used as input:

tcsh $ cat onintr_1
#!/bin/tcsh
#demonstration of onintr
onintr close //if a interuppt command is given, jump to the label called close
while ( 1 ) //infinite loop
 	echo "Program is running." //print program is running
 	sleep 2 //sleep for 2 seconds
end //end of loop
close: //label to jump to
echo "End of program." //prints end of program

if a script creates temp files, we can remove em with onintr:

close:
rm -f /tmp/$$* //remove all files that exist in tmp that has a PID of the current shell

What follows, is a control structure that has 3 forms, and is called if...then...else:

the if...then...else can chain commands in it's if part.

the first form is as follows:

if (expression) then //if statement followed by then, then is used in the more complicated format of if statements
	commands //commands to execute
endif //End of if

This is a one branch way structure that only triggers if the expression is true

The second form, is as follows:

if (expression) then //if followed by a then
	commands //Commands
else //if the if is not true
	commands //commands
endif //end of entire if

The third form, is as follows:
it runs tests until it finds one that fits the condition, and then ends the if statement. It's a two way branching statement that directs
based on conditions.

if (expression) then //If something is true, do something
	commands //commands to do
else if (expression) then //if first statement is not true, check this out instead
	commands // do stuff
...
else //if nothing else was true
	commands //commit commands
endif //end of the if statement

For clarity, the following example declares a variable pre-empatively (class) and for clarity, it assigns the value of the first commandline argument to
(number):

tcsh $ cat if_else_1 //Write to a file called if_else_1
#!/bin/tcsh //notifies it being a shell script
#routine to categorize the first
#command-line argument
set class //pre-empative declaration of class
set number = $argv[1] //declares number to be the first argument of the command line

if ($number < 0) then
	@ class = 0 //numeric assignment must be preceded by declaration of that it is a number, since all default values are string otherwise
else if (0 <= $number && $number < 100) then //if 0 is less or equal to $number and number is less than 100 
	@ class = 1 //numeric assigment of class
else if (100 <= $number && $number < 200) then //if 100 is less than or equal to $number and $number is less than 200
	@ class = 2 //numeric assignment
else
	@ class = 3 //numeric assignment
endif
#
echo "The number $number is in class ${class}." //use {} to seperate variable from the following characters


We also have foreach loops, in tcsh, which we can write as follows:

foreach <loop-index> (argument-list)
	commands
end

The loop runs through commands. The first time, it assigns the first command to the <loop-index> part, to operate upon the result.

It repeats this pattern, until the argument-list has been parsed.

Since we are going to be using the sed utility, here is a list of the modifiers for sed:

!cmd : Do not apply to specified addresses, basically a skip modifier

# : A comment

:label : places a label

= : displays line number

D : Deletes the first part of the pattern space

G : Append contents of hold area

H : Append pattern space on buffer

N : append next line

P : print first part of the pattern space

a : append text

blabel : branch to label

c : change lines

d : delete lines

g : get contents of hold area

h : hold pattern space (in a hold buffer)

i : insert lines

l : list lines

n : next line

p : print

q : quit

r file : read the contents of file

tlabel : tests substituion and branch on successful substitution

w file : write to file

x : exchange buffer space with pattern space

{ : group commands

s/RE/replacement [flags] : substitute

y/list1/list2 : translates list1 into list2



What follows, is a loop structure that uses the sed utility, which replaces the first given argument with the second:

tcsh $ cat ren
#!/bin/tcsh //begin with declaration of script
# usage: 		ren arg1 arg2
# 				changes the string arg1 in the names of files
# 				in the working dir into the string arg2

if ($#argv != 2) goto usage //if the amount of arguments is not 2, go to the label called usage
foreach i ( *$1* ) //a foreach loop with iterating element being called i, the argument list is the first command
	mv $i `echo $i | sed -n s/$1/$2/p` //mv renames or moves from one dir to another. In this case, it renames.
	//command substituion of $i in the `'s, remember, this is tcsh, so we must refer to the older syntax
	//we then pipe the echoed argument to sed  which takes the next line and then substitutes the first reference with the second reference
end
exit 0 //Exit with clean status

usage: //label in case of more than 2 arguments
echo "Usage: ren arg1 arg2"
exit 1
	
The following script, uses a foreach loop to assign command-line arguments to the elements of an array named buffer:

tcsh $ cat foreach_1
#!/bin/tcsh
#routine to zero-fill argv to 20 arguments
#
set buffer = (0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0) //20 slots array
set count = 1
#
if($#argv > 20) goto toomany //in case amount of arguments provided is too many, go to the toomany label
#
foreach argument ($argv[*]) //itterate through the entirety of commandline arguments
	set buffer[$count] = $argument //insert arguments to positions in the array
	@ count++ //arithmetic operation, thus @ must be declared
end
#REPLACE command ON THE NEXT LINE WITH
#THE PROGRAM YOU WANT TO CALL
exec command $buffer[*] //execute commands, does not spawn a new subshell and only executes command until completion
#
toomany:
echo "Too many arguments given." //print there was an error
echo "Usage: foreach_1 [up to 20 arguments]" //print instructions
exit 1 //exit with error sign

We then also have while loops, that allows for iteration while a condition is true:

while (expression)
	<commands>
end

An example of a while loop follows:

tcsh $ cat while_l
#!/bin/tcsh //signify script initation
#Demonstration of a while control structure
#This routine sums the numbers between 1 and n;
#n is the first argument on the command line.
#
set limit = $argv[1] //limit is set to the first command on the commandline
set index = 1 //start of index value
set sum = 0 //start of sum value
#
while ($index <= $limit) //while with a condition of index being lower than the limit set
	@ sum += $index //add to sum the current index
	@ index++ //increment index and @ notification for numeric processing
end //End of while loop
#
echo "The sum is $sum"

The above just calculates the sum of all the elements of the range of 1 to n

We can use break and continue statements in these loops, albeit they finish their respective line when they are called upon:

break transfers control to the end statement, finishing the loop, continue transfers control to the end statement, but continues execution of the loop
(just skips one step)

The switch statement is much akin to the Java's switch statement, as follows:

switch (test-string)
	
	case pattern:
		commands
	breaksw

	case pattern:
		commands
	breaksw

	case pattern:
		commands
	breaksw
	...
	default:
		commands
	breaksw

endsw

We can use any type of special character for the pattern part of switch statements, except for | (pipes)

tcsh $ cat switch_l
#!/bin/tcsh
#Demonstration of a switch control structure.
#This routine tests the first command-line argument
#for yes or no in any combination of uppercase and
#lowercase letters
#
#test that argv[1] exists
if ($#argv1 != 1) then //if the number of arguments is not 1
	echo "Usage: $0 [yes|no]"
	exit 1 //exit with error status
else
	#argv[1] exists, set up switch based on its value
	switch ($argv[1])
	#case of YES
		case [yY][eE][sS]: //checks for a case of lowercase y or uppercase Y, lowercase/uppercase E, lowercase/uppercase S, This basically means that we check for each
		//char in the ordering we specified, so it can be any combination of yeS or YEs etc. etc.
		echo "Arugment is yes."
		breaksw
	#
	#Case of NO
		case [nN][oO]: //same as above, except for N and o
		echo "Argument is no."

	breaksw
	#
	#default case
		default:
		echo "Argument one is not yes or no."
		breaksw
	endsw //ends entire casing of switch case
endif //marks end of if statement

BUILTINS:

Builtins are processes that are already builtinto the system, and thus does not require a forked process or a explicit shell to be run.

if it searches for simple filenames in terms of something that is NOT a builtin, it searches by help of the $PATH variable.
As far as builtin goes, it does not need to search for it, at all.

Note: All of the control structures in tcsh, is builtins (if's, while, switch etc.)

What follows, is a list of builtins for tcsh:

% job : A synonym for the fg builtin (foreground). The job is the job number which you wish to reference to bring forth to the fg

% job & : A synonym for the bg builtin (background). The job is the job number which you wish to reference to bring to the background.

@ : Similar to the set builtin, except it only refers to numerals.

alias : Creates and displays aliases. bash uses different syntax on this, other than tcsh.

alloc : Displays a report of the amount of free and used memory.

bg : Moves a suspended job into the background

bindkey : Controls the mapping of keys to the tcsh command-line editor commands.
	bindkey : Without any arguments, it lists all the currently bound keybindings in terms of the command-line editor commands

	bindkey -l : Lists all available editor commands and gives a short description of each

	bindkey -e : Puts the command-line editor in emacs mode.

	bindkey -v : Puts the command-line editor in vi(m) edit mode.

	bindkey <key> <command> : binds the <key> to <command>

	bindkey -b <key> <command> : Similar to the previous form of the bindkey argument, except, allows binding of ctrl+<key> or alt+<key> bindings

	bindkey -c <key> <command> : binds the <key> to the command <command>, where <command> is a shell builtin or a exec, thus NOT an editor command.

	bindkey -s <key> <string> : Causes tcsh to substitute <string> when you press <key>

builtins : displays a list of all the builtins

cd or chdir : Changes the cwd to the cd

dirs : Displays the directory stack

echo : Displays its arguments. We can prevent the return statement on the end by -n option or \c by end of command to escape the return char.
		basically this is a lot like the echo utility

eval : Scans and evaluates the command line. When you put eval in front of a command, the command is scanned twice by the shell before execution.
	   useful in context of command substitution and the likes, due to ordering of how substitution occurs (thus, if we have multiple substitutions we can
	   chain the eval to cause several evaluations of the same command)

NOTE ON eval: Given sufficiently complex operations, the bash parser will reach the memory cap limit of 10k variables. Since the eval statement, as far as
bash is concerned, technically is 3 parts, due to containing parts such as optional argument declarations, it is so that the max limit for instance
with eval would be 3333 pipes in regards to eval. We could break down the amount of operations in terms of parsing, which originates from association
in terms of left and right association, where Bash is a left line associated parser : Thus, if we wish to circumvent this, we would need to encase
things in () to account for as being 1 set of fewer arguments for the bash parser to interpet when it puts things on the stack.

exec : Overlays the program currently being executed with another program in the same shell. The original program is list. //basically erases the old running program
//in respective shell and runs the execution of exec on command instead.

exit : Exits from a TC shell. When you follow exit with numeric argument, tcsh returns that number as the exit status.

fg : Moves a job into the foreground.

filetest : Takes one of the file inquiry operators followed by one or more filenames and applies the operator to each filename. Returns the same result as a 
SPACE-seperated list.

glob : like echo, but does not display spaces between arguments and does not follow up by a newline variable

hashstat : Reports on the efficiency of tcsh's hash mechanism. The hash mechanism speeds the process of searching through the directories in your search path.

history : brings up the historylist of commands

jobs : Displays a list of jobs (Suspended commands and jobs being run in the background)

kill : terminates a job or process

limit : Limits the computer resources that the current process and any process it creates can use. You can put limit to the number of seconds of CPU time the
process can use, the size of files that the process can create, and so forth

log : Immedeatly produces the report that the watch shell variable would normally cause every 10 minutes.

login : Logs in a user. Can be followed by username

logout : Ends a session if you are using your original (login) shell

ls-F : similar to ls -F, but faster. (this is ls -F without the space)

nice : Lowers the process priority of a command or a shell. This builtin is useful if you run a command that makes large demands
on the system and you do not need the output right away. if you are working with root privs, you can use nice to raise the prirotiy of a command.
(Modifies the priority of a command : To heighten requires root privilages)

nohup : Allows you to log out without terminating processes running in the background. Some systems are set up this way by default.

notify : Causes the shell to notify you immedeatly when the status of one of your jobs changes.

onintr : Controls the action of an interupption command within a script. Interuppts works in tcsh, bash's coresponding function is trap, which catches a signal.

popd : Changes the cwd to the popd element from the dir stack, removes the popd element from teh dir stack.

printenv : Displays all environmental variables names and values.

pushd : changes the cwd to the cd and puts the cd on the top of the dirstack

rehash : Re-creates the internal tables used by the hash mechanism. Whenever a new instance of tcsh is invoked, the hash mechanism creates a sorted list of all
available commands based on the path variable. After you add a command to a dir in path, use re-hash to re-create the sorted list of commands. If you do not,
tcsh may not be able to find the new command.

repeat : Takes two arguments- a count and a simple command (no pipes or chain of commands are allowed), and repeats said command <count> times.

sched : Executes a command at a specific time. For example, the following command causes the shell to display the message Dental Appointment
at 10 am:

tcsh $ sched 10:00 echo "Dental Appointment." //scheduel a command for a arbitrary timepoint of which we do a command

set : Declares, initialies and displays local variables.

setenv : Declares, intialies and displays environmental (global) variables.

shift : Analogous to the bash shift builtin. Without any argument, shift promotes the indexes of the argv array (the array of references to command-line arguments)
(increases all indexes by 1). can be used on any array to do the same principle.

source : Executes the shell script given as its argument: source does not fork another process. It is similar to the . builtin in bash, the source builtin
expect a TC shell, thus no #! notification of the shell script is nessecary for the start of said source script. The current shell executes source; thus the
script can contain commands, such as set that affect the current shell. This means that we can run the commands from .tcshrc or .login with the
source command to initiate commands from said script without having to log in and out from the shell.

You can nest source builtin commands.

stop : Stops a job or process that is running in the background. It is similar to suspend key, which stops jobs running in the foreground. This builtin
will not suspend a login shell.

time : Executes the command you give it as an argument. It displays a summary of time-related information about the executed command, according to the
<time> shell variable. Without an argument, time displays the times for the current shell and it's children.

umask : Identifies or changes the default privleges of file creation in tcsh

unalias : Removes an alias

unhash : Turns off the hash mechanism 

unlimit : Removes limits on the current process.

unset : Removes a variable declaration.

unsetenv : Removes a global variable declaration.

wait : Causes the shell to wait for all child processes to terminate. When you give a wait command in response to a shell prompt,
tcsh does not display a prompt unitl all of the background processes have finished executing. if you interuppt it with a interuppt key command
wait displays a list of background processes before the tcsh displays a prompt

where : When given the name of a command as an argument, locates all occurences of the command, and for each, tells you wether it is an alias, a builtin
or an exec program in our path

which : Similar to where, but reports only the command that would be executed, not all occurences. This builtin is much faster than the linux utility
and reports on alises and builtins.

PROGRAMMING THE BOURNE SHELL : PERL, AWK PROCESSING LANGUAGE, SED EDITOR, RSYNC.

the Bash shell uses the same language concepts as C.

When we make scripts, do not name them test. There already is a utility in linux called this, which could cause confusion.

In Bash, we have the following control structures: if...then, for...in, while, until and case statements, break and continue.

We have the normal if statement, as in if...then, which is as follows:

if <test-command>
	then //unlike tcsh, the then in Bash comes under the if statement
		commands
fi //fi stands for finish

What follows, is a script that reads the input of two seperate words, and then compares them with the test utility, to make a equality operation on the two inputs:

//the test utility returns true if two words are the same, and false otherwise
$ cat if1
echo -n "$word 1: " //having " notations for arguments, assured literal interpetation of the argument after variable expansion, exclude trailing character by -n notation
read word1 //reads a line from the standard input and assign the value to word1
echo -n "$word 2: " //prompt that we are asking for the second word
read word2 //reads the line from standard input and assigns it to word2

if test "$word1" = "$word2" //" notations to account for entirety and still allow for variable expansion
	then //Designator of then must follow if, in BASH it's put below the if
		echo "Match" //Echo match, since we have found a match if the if statement is true
fi

echo "End of program" //echo that its the end of the script

$./if1 //Access if1 in the cwd 
word 1: <input>
word 2: <input2>
//if input == input2 here, we get : <result>, in this case, Match
End of program

There are two versions of test, in regards to bash : A builtin one (part of the shell), and a utility one, that is found in:

/usr/bin/test

If there is a builtin, you usually use the builtin, otherwise you use the utility version.

To compare numbers with the test utility, we have to run the -eq modification on it.

In BASH we also have the $#argv ,except we omit the argv part ; They fullfill the same function, though (the amount of commands on the commandline)

$ cat chkargs 
if test $# -eq 0 //This is a if statement that checks to see if test and $#(argv) are not equal (as a logical 1 would signify they are equal)
	then //then desgination followed the if statement
		echo "You must supply at least one argument."  //print out that instructions due to error
		exit 1 //Exit with error status of 1
fi //fi for ending if statemenet
echo "Program running." //Echo that the progam is running


$ ./chkargs //Attempt to run the script form the cwd
You must supply at leasto ne argument //We must at least give one argument
$ ./chkargs abcs //run it with one argument
Program running //Program is running as specified, we get a echo of that it's running


What follows, is a test script that checks to see if the provided argument is a ordinary file (not a dir or any other type of file):

$ cat is_ordfile
if test $# -eq 0 //Checks to see if argv is 0, which means no arguments provided
	then
		echo "You must supply at least one argument."
		exit 1
fi
if test -f "$1" //Checks to see if the first command on the commandline is a ordinary file
	then //then must followed if statement
		echo "$1 is an ordinary file in the working dir"
	else //else can follow if, but must be on same indention as then to function
		echo "$1 is NOT an ordinary file in the working dir"
fi

What follows, is a list of the option modifiers for test:

-d : Exists and is a dir

-e : Exists

-f : Exits and is an ordinary file (not a dir)

-r : Exists and is readable

-s : Exists and has size greater than 0 bytes

-w : Exists and is writable

-x : Exists and is executable

Note: It is good praxis to test arguments in scripts, to ensure integrity and safety of running the script

What follows, is a seperate example of how to test arguments in Linux, in a more traditional matter, which is with [] brackets: //the [] must be surrounded with 1 space on each side

$ cat chkargs2
if [ $# -eq 0 ] //if the numer of argvs is 0, the [ ] is a synonym to test, if written in the specified way, in BASH
	then //then must follow a if statement in BASH
		echo "Usage: chkargs2 argument..." 1>&2 //Redirect standard output to standard error output
		exit 1 //exit with error status of 1
fi
echo "Program running." //Echo that program is running
exit 0 //exit with clean status, if we omit 0, it will default to exit with 0, thus, when we get errors, we need to exit with 1


$ ./chkargs2 //attempt Run chkargs2 in the cd
Usage: chkargs2 argument... //Error due to difference of arguments provided and arguments required

$ ./chkargs2 abc //succesful run
Program running.

A lot of utilities in Linux has default messaging in terms of if you used the util in the wrong way, as is shown by the cp command:

$ cp
cp: missing file operand
Try "cp --help" for more information.

We can do if...then...else statements as well, in BASH, as follows:

if <test-command> //if command evaluates to true
	then //Then must followed if statement 
		<commands> //Commands to run
	else //Else
		<commands> //Commands to run
fi //fi must follow if to finish if call

We could also do a ; line seperator to fullfill the same structure, but in a different looking matter:

if <test-command>;then //; seperator can be used to account as being a seperation of line and allows to write the syntax like this
		<commands>
	else
		<commands>
fi //finish if statement


the if...then...else statement is basically just a branching if and else statement, finished by a fi statement.

What follows, is a script that uses both shift, less and redirection of outputs, to display how the script works, and to accept inputs in form of that
it checks to see if the file is called with the -v option (verbose):

$ cat out
if [ $# -eq 0 ] //if the amount of arguments on the command line is 0 (if argv is 0)
	then
		echo "Usage: out [-v] filenames..." 1>&2 //Echo how to use the script
		exit 1 //Exit with error status of 1
fi

if [ "$1" = "-v" ] //if the 1:st argument on the commandline equals to -v
	then //then must followed a if statement in bash
		shift //Shift it one step to the right to remove the $1 argument, which is the -v command
		less -- "$@" //output the list of all the given arguments through a less command. The -- utility is that no more options follow on the commandline,
		//and to ignore all trailing -'s , as they are discarded. This allows us to remove files that have - in their name or to ignore attempted additional
		//options to a command that we do not wish to handle
	else
		cat -- "$@" //Display the literal interpetation of all the arguments provided ("$@" is a reference to a list of ALL the arguments on the commandline)
fi //Finish with fi


We also have elif statements in Bash, which allows for further branching, as follows:

if <test-command> //a if statement is like a block declaration in bash, where a 4 line indention is past the if, and every single further nesting follows the same algorithm
	then //4-indent after if 
		<commands> //commands
	elif <test-command> //else if
		then //then
			<commands> //commands
... //arbitrary amount of commands can follow
	else //finish with else, optional
		<commands>
fi //finish a if statement with fi

We also have and statements in the Bash langauge, except we write them as -a. What follows, is an example of Bash script:

$ cat if3
echo -n "word 1: "
read word1 //assign from standard input to word1
echo -n "word 2: "
read word2 //assign from standardi npuit to word2
echo -n "word 3: "
read word3 //assign from standard input to word3
if [ "$word1" = "$word2" -a "$word2" = "$word3" ]
	then
		echo "Match: words 1, 2, & 3"
	elif ["$word1" = "$word2"]
	then
		echo "Match: words 1 & 2"
	elif [ "$word1" = "$word3"]
	then
		echo "Match: words 1 & 3"
	
	elif [ "$word2" = "$word3" ] //ifs and elifs must follow by a then
	then
		echo "Match: words2 & 3" //double quotes around input to prevent expansion of &, thus literal input
	else
	 	echo "No match"
fi //finish


An example of the script being run:

$ ./if3
word 1: apple
word 2: orange
word 3: pear
no match

$ ./if3
word 1: apple
word 2: orange
word 3: apple
Match: words apple & apple

$ ./if3
word 1: apple
word 2: apple
word 3: apple
Match: words 1, 2, & 3


What follows, is a script that searches for hard links to the first provided file and hard links to the dir in the second argument:

$ cat lnks
#!/bin/bash //Signify that it's a function
#Identify links to a file
#Usage: lnks file [directory]

if [ $# -eq 0 -o $# -gt 2 ]; then //if the number of arguments is 0 or the number of arguments is greater than 2
	echo "Usage: lnks file [directory]" 1>&2 //display usage
	exit 1 //Exit with error 
fi //fi after if

if [ -d "$1" ]; then //if the first argument is a Dir
	echo "First argument cannot be directory." 1>&2 //Prompt info
	echo "Usage: lnks file [directory]" 1>&2 //How to use it
	exit 1 //exit with error
else //else, its not a dir
	file="$1" //assign file variable to be the first argument
fi //finish the if

if[ $# -eq 1 ]; then //if number of arguments is 1
		directory="." //dir gets assigned to be the cd
	elif [ -d "$2" ]; then //else if the second argument is a dir
		directory="$2" //directory gets assigned the second argument
	else //íf second argument is not a dir and we do have 2 arguments
		echo "Optional second argument must be a dir." 1>&2 //prompt that second arg must be dir
		echo "Usage: lnks file [directory]" 1>&2 //Show usage
		exit 1 //exit with error status
fi //finish the if with fi

#Check that files exists and is an ordinary file
if [ ! -f "$file" ]; then //the ! is a not operator, looking for if something is false, so it checks to see if the file does not exist or is not a ordinary file
	echo "lnks: $file not found or special file" 1>&2 //Echo message and redirect standard output and error
	exit 1 //Exits with error status
fi //fo to finish the if

#Check link count on file
set -- $(ls -l "$file") //assign ls of said file to find linkcount later

linkcnt=$2 //Second part of link ls -l command is amount of links
if [ "$linkcnt" -eq 1 ]; then //if there is only 1 hard link
	echo "lnks: no other hard links to $file" 1>&2
	exit 0
fi

#get the inode of the given file
set $(ls -i "$file") //get ls with index retrieval

inode=$1 //index is the first part of command

#Find and print the files with that inode number
echo "lnks: using find to search for links..." 1>&2
find "$directory" -xdev -inum $inode -print //Find the directory with expansion, -xdev : do not descend into dirs on other file systems, -inum : run check to see index number on   //file followed by numeral to compare to, which in this case is $inode (files index), -print to print it out
//includes the echo due to the fact that find takes a long time to run

To run the script, we could do as follows:

$ ./lnks letter /home //call script, supply arguments
lnks: using find to search for links...
/home/max/letter
/home/zach/draft

We can define what shell we want, by virtue of the #!<shell path> command, as example of:

#!/bin/bash

If we wish to find out the commands that a script runs, we can run the script with the -x modifier, as follows:

$ bash -x lnks letter /home
+ '[' 2 -eq 0 -o 2 -gt 2 ']'
+ '[' -d letter ']'
+ file=letter
+ '[' 2 -eq 1 ']'
+ '[' -d /home ']'
+ directory=/home
+ '[' '!' -f letter ']'
...

We can declare the 4th prompt, PS4, which is for debugging purposes. However, if we declare it in the same script that runs the scripts which we want to debug,
we have to export it, as follows:

$ export PS4='>>>> '

we can also enable the -x switch by declaring it inside of the script, where we wish to start debugging:

set -x

if we wish to turn it off, we do the:

set +x

Alternatively, we could use an alternative syntax to it, as follows:

set -o xtrace and set +o xtrace, both of these are teh same thing as set -x and set+x

We also have for loops, that we can declare as for...in.

What follows, is an example of interaction in the for...in loop:

$ cat fruit //output the contents of fruit
for fruit in apples oranges pears bananas //Declaration of argument list to operate upon
do //Initiation of loop
	echo "$fruit" //the commands to comit
done //signaling completion of loop
echo "Task Complete"

The following example, showcases how to look through all the elements in the cwd directory and print out all that is dirs:

$ cat dirfiles
for i in * //* is all the files in cwd, gets expanded before looping, uses result list to operate on it
do //declare do in loop
	if [ -d "$i" ] //if the i:th element is a dir
		then
			echo "$i" //print out the dir
	fi //finishes if statement
done

We also have a plain for loop (NOTE: NOT AVAILABLE IN TCSH), which has the general syntax as follows:

for <loop-index>
do
	<commands>
done

In the following example, we see a implied reference to all current commands on the commandline, as follows:

$ cat for_test
for arg 	//this implies for arg in "$@", which implies for arg in all the commandline arguments
do
	echo "$arg"
done
$ for_test candy gum chocolate
candy
gum
chocolate

What follows, is an example of how to use the gawk utility:

$ cat whos
#!/bin/bash

if[ $# -eq 0 ] //if the argv equals 0
	then
		echo "Usage: whos id..." 1>&2
		exit 1
fi //finish
for id //due to implied "$@" here, it passes the entire elements as one element at a time, which means that it regenerates correctly with spaces, due to 1 argument being passed
//with "" marks keeping the whole thing as a literal thing
do
	gawk -F: '{print $1, $5}' /etc/passwd | //gawk, which also works the same as awk or mawk, extracts $1 and $5 in each line for /etc/passwd
	//by running gawk with the -F: option, it breaks each part it finds into fields with : as a seperator, allowing for allocation by virtue of $<position>, the output is then
	//piped down to the grep where it ignores casing with -i, and searches for "$id", which is the arguments placed in the commandline
	//NOTE: When a pipe | is the last part of a line, it allows for piping to the next command, regardless of newline character coming after it or not.
	grep -i "$id"
done



An example of showcasing it : This script is basically like finger, but gives less info:

$ ./whos chas "Marilou Smith"
chas Charles Casey
msmith Marilou Smith



We also have a plain while loop (NOTE: NOT AVAILABLE IN TCSH), which has the general syntax as follows:

while <test-command>
do
	<commands>
done

The following loop uses the comparator operators, as we know, em less than, greater than, equal to, not equal to.
Except, in Bash, they are in alphabetical form, as follows:

-ne - Not equal to

-eq - equal to

-gt - Greater than

-ge - greater than or equal to

-le - less than or equal to

-lt - Less than

All of the above, works for NUMBERS. If we want to compare STRINGS, we have to use != (not equal to) and = (equal to).

What follows is a while loop that runs while a number is less than 10:

$ cat count
#!/bin/bash
number=0
while [ "$number" -lt 10 ] //while number is less than 10
	do //do something
		echo -n "$number" //echo how much the number is without the end of line char
		((number += 1)) //perform arithmetic operations within (())
	done
echo //empty echo to have prompt occur on next line, instead of after the 9


$./count
0123456789
$

The aspell utility (linux only), allows us to match a list against a dictionary for a attempt at correcting spelling, as follows:

$ aspell list < letter.txt //We take the contents of letter.txt and run list on it with aspell. This returns all potentionally misspelled words back to standard output.

$ aspell list < letter.txt
quikly
portible
frendly

What follows is a script that runs the contents of a list to another list, where one list is correct, and the other is not. If it finds a incorrect value in
the list that is not corrected yet, it disposes of the word:

$ cat spell_check
#!/bin/bash
#remove correct spellings from aspell output

if [ $# -ne 2 ] //If the number of argv is not equal to 2
	then
		echo "Usage: spell_check file1 file2" 1>&2
		echo "file1: list of correct spellings" 1>&2
		echo "file2: File to be checked" 1>&2
		exit 1
fi

if [ ! -r "$1" ] //if the first file is not readable
	then
		echo "spell_check: $1 is not readable" 1>&2
		exit 1
fi

if [ ! -r "$2" ] //if the second element is not readable
	then
		echo "spell_check: $2 is not readable" 1>&2
		exit 1
fi

aspell list < "$2" |
while read line
do
	//NOTE: the "^line$" means explicitly line, due to ^ begins with $line and $ means ends with, followed by nothing it means it must be the end of the word
	//since the only relevant part is the actual exit code, we just throw the results into the garbage bin, which is /dev/null
	//if grep did not find a match, it means it's a faulty word, and it then echoes it
	//terminates loop upon EoF

	if ! grep "^$line$" "$1" > /dev/null //this line can be rewritten as: if ! grep -qw "$line" "$1" //where -q surpressed output and forces only exit code
																									 //w forces grep to only match whole words
		then
			echo $line
	fi
done

An example of the interaction of this script, follows:

$ aspell list < memo
Blinkenship
Klimowski
targat
hte
$cat word_list
Blinkenship
Klimowski
$ ./spell_check_word_list memo
targat
hte

THus, we are only left with a list of words that we consider are incorrectly spelled.

WE also have another keyword in terms of structure, called until (DOES NOT EXIST IN TCSH).

The until command is like a while loop, except it does it until something returns a true statement, instead of while it returns true.

The general syntax for until loops, is as follows:

until <test-command>
do
	<commands>
done

What follows, is an example of a script that has you guessing, until the point of where you are right:

$ cat until1
secretname=zach
name=noname
echo "Try to guess the secret name!"
echo
until [ "$name" = "$secretname" ] //Keep looping until said condition is fullfilled
do
	echo -n "Your guess: " 
	read name //ask for input and register it in name
done
echo "Very good."

$ ./until
Try to guess the secret name!

Your guess: helen
your guess: barbara
your guess: derp
your guess: zach
Very good.

What follows, is a script that allows for requiring typing in your password to access the termial, without actually having to logout:

$ cat locktty
#! /bin/bash

trap '' 1 2 3 18 //the trap command catches any signals with the numeric input, as each command has a bound numeric input, meaning you catch
//that signal. THis prevents people from doing stuff like trying to run the interrupt key on it and stuff.
stty -echo //Shuts off echo for the terminal
echo -n "Key: "
read key_1
echo
echo -n "Again: "
read key_2
echo
key_3= 		//null value
if[ "$key_1" = "$key_2"] //if the first value matches the second
	then
		tput clear //clears the screen
	    until [ "$key_3" = "$key_2"] //runs until you re-enter the value of key_2
	    do
	    	read key_3
	    done
	else
		echo "locktty: keys do not match" 1>&2
fi //Finish the if statement with fi
ssty echo //turns on echo again, if we passed the test

We can use break statements and continue statements in bash as well, here is an example:

$ cat brk
for index in 1 2 3 4 5 6 7 8 9 10 //iterate over a range
	do
		if [ $index -le 3 ] ; then //if index is less than or equal to 3
			echo "continue"
			continue //skip the rest and restart with next element
		fi

	echo $index //we only get here if the continue does not trigger

	if [ $index -ge 8 ] ; then //if we hit number 8
		echo "break" 
		break //break
	fi //declare if finished
done //declare loop done


$ ./brk
continue
continue
continue
4
5
6
7
8
break

We can also use case (switch in tcsh), which is basically a switch case:

case <test-string>
	<pattern 1>)
		<commands-1>
		;;
	<pattern 2>)
		<commands-2>
		;;
	<pattern 3>)
		<commands-3>
		;;
...
esac //Declares end of switch case

What follows is an example of a switch case that registers inputs:

$ cat casel
echo -n "Enter A, B, or C: " //Echo without new line
read letter //read standard input and assign it to letter
case "$letter" in //start the case with referal of variable to compare
	A) //the Case is A
		echo "You entered A"
		;;
	B) //the case is B
		echo "You entered B"
		;;
	C) //The case is C
		echo "You entered C" 
		;;
	*) //* is a ambigious reference to any letter
		echo "You did not enter A B or C" //echo output
		;; //break statement for case in bash
esac //Finish case (or switch case)

$ ./casel
Enter A,B, or C: B
You entered B

We can also rewrite the script above, to account for lowercase or uppercase, with or statements in the cases:

$ cat casel
echo -n "Enter A, B, or C: "
read letter //read standard input
case "$letter" in
	A|a) //The case is A or a
		echo "You entered A"
		;;
	B|b) //The case is B or b
		echo "You entered B"
		;;
	c|C) //There is C or c
		echo "You entered C"
		;;
	*)
		echo "You did not enter A, B, or C"
		;;
esac

$ ./case2
Enter A, B, or C: b
you entered B

The following patters, are the ones accepted in terms of casing:

* : Ambigious string reference, matches any string of characters

? : Ambigious letter reference, matches any single character

[...] : Defines a list of character classes, each character enclosed in these brackets is tested seperately, a - between chars means a range of the chars

| : or statement

The following example, showcases how one can create a menu from using a case structure:
NOTE: This can be coded a lot easier with the usage of select control structures.

NOTE: When we run the -e command on echo, it allows for special characters, as follows:

\a : alert (bell)

\b : backspace

\c : suppress trailing newline

\e - escape

\f - form feed

\n - new line

\r - carriage return

\t - horizontal tab

\v - vertical tab

\\ - backslash

\NNN - The chars who's octal code is NNN, if no match is found, it's printed literally

\xnnn - The chars who's hexadecimal is nnn (one to three digits)

$ cat command_menu
#!/bin/bash
# menu interface to simple commands

echo -e "\n 	COMMAND MENU\n"
echo " a. Current date and time"
echo " b. Users currently logged in"
echo " c. Name of the working dir"
echo -e " d. Contents of working dir \n"
echo -n "Enter a, b, c or d: "
read answer
echo

case "$answer" in
	a) 
		date
		;;
	b)
		who
		;;
	c)
		pwd
		;;
	d)
		ls
		;;
	e)
		echo "There is no selection: $answer"
		;;
esac

$ ./command_menu

		COMMAND MENU
	a. Current date and time
	b. users currently logged on
	c. Name of the wd
	d. Contents of the wd

Enter a, b, c, or d: a

Wed Jan 3 12:37:12 PST 2009

We can also make scripts that interact based on an arbitrary amount in the argv, as follows:

$ cat safedit
#!/bin/bash

PATH=/bin:/usr/bin //Sets path to bin and /usr/bin to assure that utilities run, are standard utils. If we would not do this,
//then it could go on the PATH we had otherwise set, which could end up in running utils that are not the standard ones, which causes errors.
//Thus, we assure it's running standard utils.
//We could also declare it to have absolute pathnames, but that would make it less portable due to only adapting to a absolute structure

script=$(basename $0) //assign the basename which is the $0:th argument to script (safedit)
//By creating a derivation of the basename, we can allow for modification of name of the script, and still have reprint of the name be correct,
//instead of a hardcoded value which fails upon change of name

case $# in
	0) //if there are 0 arguments provided, run vim with an empty file
		vim
		exit 0
		;;
	1) //if there is 1 argument provided, go here
		if [ ! -f "$1" ] //if the file is not found, create it in vim
			then
				vim "$1"
				exit 0
			fi
		if [ ! -r "$1" -o ! -w "$1" ] //if it exists but we do not have access to read or write in it, go here
			then
				echo "$script: check permission on $1" 1>&2 //prompt to check access
				exit 1 //exit with error
			else
				editfile=$1 //Otherwise, make so that the editing file is the first argument
			fi
		if [ ! -w "." ] //If we don't have wrigin permissions for cd, we cannot save a backup there
			then
				echo "$script: backup cannot be " \ //prompt with info
					"created in the working dirr" 1>&2
				exit 1 //exit with error code
			fi
		;;
    *) //any other case, give prompt of usage
    	echo "Usage: $script [file-to-edit]" 1>&2
    	exit 1 //exit with error code
    	;;
esac


tempfile=/tmp/$$.$script //by having the $$ (this is the PID value), we assert a unique name. ANd we put it first, because older systems have a 14 char
//limit, to counteract this, we keep the filename unique with the PID value first in the name, and it also assures a unique name of the file.

cp $editfile $tempfile //make a copy of the editfile
if vim $editfile 
//This calls vim to edit the editfile. Since vim can return a exit code, based on it's result (i.e, it can return 1(error) if a kill sequence was given
//in the vim editing session, instead of actualy exiting as it should (which would then return 0))

	then //if true, it exited as it should
		mv $tempfile bak.$(basename $editfile)
		echo "$script: backup file created"
	else //an error occured, puts tempfile into editerr and says that there was an error in editing
		mv $tempfile editerr
		echo "$script: edit error--copy of " \
		"original file is in editerr" 1>&2
fi


We can also use another control structure, which is the select structure. (NOT AVAILABLE IN TCSH)

It's general syntax, is as follows:

select <varname> [in arg ...]
do
	<commands>
done

It selects a prompt value based on the arg list it gets. If there is no list in the arg and in is ommited, then it accepts
commandline references instead.

For example, this is how select would show things:

select fruit in apple banana blueberry kiwi orange watermelon STOP

gives

1) apple 	3) blueberry 	5) orange 	7) STOP
2) banana 	4) kiwi 		6) watermelon

The select structure has some related variables that defines it's formatting, as follows:

LINES = 24 (default)

COLUMNS = 80 (default)

If we were to set COLUMNS to 20, we'd get the following output, instead:

1) apple
2) banana
3) blueberry
4) kiwi
5) orange
6) watermelon
7) STOP

After displaying the menu, select displays the P3 prompt, which is the select prompt.
By default, it's ?#, but it's usually designated to something else.

When we put a valid number in the range, it assigns the value to REPLY.
If we do not apply a valid number, it assigns it null.

if we were to just press enter, it re-asks for input.

The select statement then performs a do command, and repeats the process of asking, until it reaches a exit statement in some form.
What follows, is an example:

$ cat fruit2
#!/bin/bash
PS3="Choose your favorite fruit from these choises: "
select FRUIT in apple banana shit derp lol wtf STOP
do
	if [ "$FRUIT" == "" ]; then
		echo -e "Invalid entry. \n"
		continue
	elif [ $FRUIT = STOP ]; then
		echo "Thanks for playing, lol"
		break
	fi
echo "You chose $FRUIT as your favorite."
echo -e "That is choice number $REPLY.\n"
done

$ ./fruit2
1) apple 	3) shit 	5) lol 	7) STOP
2) banana 	4) derp 		6) wtf
Choose your favorite fruit from these possibilities: 3
You chose shit as your favorite.
That is choice number 3.

Choose your favourite fruit from these possibilities: 99999
Invalid entry.

Choose your favourite fruit from these possibilities: 7
Thanks for playing, lol

If we wish to redirect input to a shell script, from within the script itself, we can use the here command, as follows:

$ cat birthday
grep -i "$1" <<+
Max 	June 22
Barbara February 3
Darlene May 8
Helene March 13
Zach January 23
Nanycj June 28
+
$ ./birthday Zach
Zach January 23
$ ./birthday june
Max 	June 22
Nancy 	June 26


What follows, is a clever use of the here mechanic. It redirects info from the script within the script itself to create a script that holds
several other dirs, as follows:

$ cat bundle
#!/bin/bash
#bundle: group files into distribution package

echo "# To unbundle, bash this file"
for i
do
	echo "echo $i 1>&2"
	echo "cat >$i <<'End of $i'"
	cat $i //By concatenating the $i:th element ,we put things in a here script, which basically becomes like a winrar, kind of
	//we put files into this file and then have the script recreate the files contained that the script has compiled together.
	echo "End of $i"
done

The result of this, is that we have managed to create a script that accepts several arguments on the commandline, and put them into a shell script,
as follows:

$ cat file1
This is a file.
It contains two lines.
$ cat file2
This is another file.
It contains
three lines.

$ ./bundle file1 file2 > bothfiles
$ cat bothfiles
#To unbundle, bash this file
echo file1>&2
cat >file1 <<'End of file1'
This is a file.
It contains two lines.
End of file1
echo file2 1>&2
cat >file2 <<'End of file2'
This is another file
it contains
three lines.
End of file2.

We then showcase the usage:

$rm file1 file2
$ bash bothfiles
file1 //creation of files from the bashing of the files is echoed
file2
$ ls
bothfiles
file1
file2


A file descriptor is something that is the number of an allocated process.

By default, there are three file descriptors, in linux, as follows:

standard input (file descriptor 0), standard output (file descriptor 1), standard error (file descriptor 2)

The bash, opens files in the following way:

exec <name>> <outfilename>
exec <m>< <infilename>

To duplicate a file descriptor, we can use the command of <& (copy a input file descriptor), >& (copy a output file descriptor)

Once we have opened a file, we can use the I/O redirection on any command line, with >&<name> and <&<name> //This is redirect standard output and standard input respectively

When we finished using a file, we can close it with the following syntax:

exec <name>&-

note: A function is not a shell script. In the following example, we can run it as a shell script, but it will just produce a script that is not that long lived
and will not work as you expect it to.

The following example, works as follows:
if we give 2 arguments, it copies the file named by the first arg to the file named by the second arg. If we supply one arg, the script copies the file
named by the arg to standard output. if you invoke mycp with no args, it copies standard input to standard output.


function mycp ()
{
case $# in //case check against argv, which is the number of arguments on the command line
	0)
		#Zero args
		#File descriptor 3 duplicates standard input
		#File descriptor 4 duplciates standard output

		exec 3<&0 4<&1 //Execute the command of redirecting standard input to the third one, and standard output to the 4th file descriptor
		;;
	1)
		#one argument
		#Open the file named by the argument for input
		#and associate it with the file descriptor 3
		#File descriptor 4 duplicates standard output

		exec 3< $1 4<&1 //first ocmmandline argument associated to file descriptor 3, and standard output directed to 4th file descriptor
		;;

	2)
		# two arguments
		# Open the file named by the first argument for input
		# and associate it with file descriptor 3
		# Open the file named by the second argument for output
		# and associate it with file descriptor 4

		exec 3< $1 4> $2 //since the first argument is for input, it's a < , as in directed association, where as of the second arg is for output,
		//so it redirects with >
		;;
	*)
		echo "Usage: mycp [source [dest]]"
		return 1
		;;
esac

#Now we are done with the association part, now we just need to arrange the actual directioning:

cat <&3 >&4

# We must then close the file descriptors:

exec 3<&- 4<&-
}

What follows, is a script that takes two filenames on the commandline, sorts them both, sends the output to temp files.
The program then merges the sorted files to standard output, preceeding each line by a number that indicates which file it came from.

$ cat sortmerg
#!/bin/bash
usage()
{
	if [ $# -ne 2 ]; then //if the argv (number of commandline args) does not equal 2
		echo "Usage: $0 file1 file2" 2>&1
		exit 1
		fi
}

# Default temp dir
: ${TEMPDIR:=/tmp} 


//the : signification, is a earlier builtin to reflect a statement of true. Derives from fact of that earlier bash shells did not have
//the true command. They were instead, simply refered to as : , and was still a vital part of functions such as follows:
//if <command>; then :; else ...; fi
//since a if command must have a then command, and it must be followed by something, and a comment is not nothing, then : fills that space
//as being a empty statement. 

#Check argument acount
usage "$@" //find all $ reference commands in usage, display them intact with spaces between due to "" quotation

# Set up temporary files for sorting
file1=$TEMPDIR/$$.file1 //assign file1 to be the file specified in TEMPDIR with the PID of this current shell and has the name of file1
file2=$TEMPDIR/$$.file2 //as above, but with file2


sort $1 > $file1 //Sort first argument into the first temp file
sort $2 > $file2 //Sort the second argument into the second temp file

# Open $file1 and $file2 for reading. Use file descriptiors 3 and 4.

exec 3<$file1 //open temp file1 with file descriptor 3
exec 4<$file2 //open temp file2 with file descriptor 4

# Read the first line from each file to figure out how to start

read Line1 <&3
status1=$? //This holds the exit status of the command above, to see the exit status of Line1 from the file

read Line2 <&4 
status2=$? //This holds the exit status of the command above, to see the exit status of Line2 from the file

# Stratergy: while there still is a line in both of the files to read, 
# output the new line that should come first
# Read a new line from the file that the line came from.

while [ $status1 -eq 0 -a $status2 -eq 0 ] //while the exit status of both file1 and file2 is 0, which means there is a line left to read
	do
		if [[ "$Line2" > "$Line1"]]; then  	//Arithmetic expression to evaluate if Line2 is greater than line1
			echo -e "1. \t$Line1" //Echo it out with a tab character 
			read -u3 Line1 //Read it with the 3rd file descriptor
			status1=$? //Exit status to determine wether there is more to read or not, in file1
		else //Line1 comes sorted before line2
			echo -e "2 .\t$Line2"
			read -u4 line2 //Read it with the 4th file descriptor
			status2=$? //exit status to determine wether there is more to read or not, in file2
		fi
	done

# Now one of the files is at the EoF point
# Read from each file until the end
# First file1:

while [ $status1 -eq 0 ]
	do
		echo -e "1.\t$Line1"
		read Line1 <&3 //attempt to read the first line from the first file
		status1=$? //Determine status to wether loop should continue or not, ie, if there is more text or not, for file1
	done
#Next file2:

while [[ $status2 -eq 0 ]]
	do
		echo -e "2. \t$Line2"
		read Line2 <&4 //Attempt to read from the first line of the second file
		status2=$? //Determine status to wether loop should continue or not, ie, if there is more text to read or not, for file2
	done

#Close and remove both input files

exec 3<&- 4<&- //Close file descriptors 
rm -f $file1 $file2 //remove input files
exit 0 //Exit with  cleared status

Bash can hold arrays, as follows: //Note: Arrays in bash work in the traditional way of starting with 0.

<name>=(<element1> <element2>)

An example follows:

$ NAMES=(max helen sam zach)

An example of reference:

$ echo ${NAMES[2]}
sam

When we wish to get all the elements, there are two ways to do this. Either with:

@ : This duplicates the array it references to

* : This copies all the elements of the array and splits the inputs with the char specified in the IFS (usually a space). This will put
all the elements into one element of the array, merely split by the IFS character.

An example, follows:

$ A=("${NAMES[*]}")
$ B=("${NAMES[@]}")


What follows, is a listing of the commands and interactions of declare:

declare [-afrXi] [-p] [name[=value]]

-a : Every name is a array variable

-f : Use function names only

-F : Inhibit the display of function definitions; only the functions name and attributes are printed.

-i : The variable is to be treated as a integer. Arithmetic evaluation is performed when the variable is assigned a value

-p : Display the attributes and their values of each name. When '-p' is used, additional options are ignored.

-r : Makes names readonly. These names cannot then be assigned values by subequent assignments or unset.

-x : Mark each name for export to global env.

We can use the declare keyword, to find exact info on the arrays:

$ declare -a
declare -a A='([0]="max helen sam zach")'
declare -b B='([0]="max" [1]="helen" [2]="sam" [3]="zach")'

declare -a NAMES='([0]="max" [1]="helen" [2]="sam" [3]="zach")'

If we try to access an index that is out of reach, it provides us with a null value:

$ echo ${A[1]}
	//empty
$ echo ${A[0]}
max helen sam zach
$ echo ${B[1]}
helen
$ echo ${B[0]}
max

To find out how many elements, there are in an array, we can access the index of * by providing the name of the Array with #:

$ echo ${#NAMES[*]}
4 //There are 4 elements

$ echo ${#NAMES[1]}
5 //There are 4 + 1 elements

We can also assign values to arrays:

$ NAMES[1]=max
$ echo ${NAMES[*]} //This will simply give all the elements, instead of the number of elements
max max sam zach

What follows is an example of exported variable interaction;

$ cat extest1
cheese=american
echo "extest1 1: $cheese"
subtest
echo "extest1 2: $cheese"

$ cat subtest
echo "subtest 1: $cheese"
cheese=swiss
echo "subtest 2: $cheese"

$ ./extest1
extest1 1 : american
subtest 1 :    		//prints null, due to reference of nothing
subtest 2: swiss
extest1 2: american

If we exported the variable, it allows to carry over to the other file:

$ cat extest2
export cheese=american
echo "extest2 1: $cheese"
subtest
echo "extest2 2: $cheese"
$ ./extest2
extest2 1: american
subtest 1: american 	//has a value, due to export declaration
subtest 2: swiss
extest2 2: american

We can optionally declare export statements, as follows:

export cheese=american

is the same as

cheese=american
export cheese

Due to that functions run in the same environment as the shell that calls them, variables are implicitly shared by a shell and a function it calls.

$ function nam () {
> echo $myname
> myname=zach
}

$ myname=sam
$ nam
sam
$ echo $myname //myname is changed in the end of the nam function
zach  

To assure that a variable is only declared and changed in a function, we can use the builtin of typeset, that allows for variables to be local
to whatever function it is in:

$ function count_down () { 
>typeset count //uses identical name of variable as outside variable, but becomes local due to typeset declaration
>count=$1
>while [ $count -gt 0 ] //while count is greater than 0
>do
>echo "$count..."
>((count=count-1)) //arithmetic expression in (())'s
>sleep 1 //sleep for 1 sec
>done //loop finisher
>echo "Blast Off."
>}

$ count=10
$ count_down 4
4...
3...
2...
1...
Blast Off.
$ echo $count
10
}

There are a number of special parameters, that begins with the $ variable, as follows:

$$ : Process ID number

We can call $$ to showcase the fact that both $$ without args and ps give the same result:

$ echo $$
5209
$ ps
	PID TTY 		TIME CMD
  5209  pts/1 	00:00:00 bash
  6015  pts/1 	00:00:00 ps


What follows is an example of passing the value of PID to a name of a file:

$ echo $$
8232
$ cp memo $$.memo
$ ls
8232.memo memo

Using the $$ for naming conventions, is good if we want unique names. Also, a seperate $$ is used for subshells, as follows, by calling
the script of id2:

$ cat id2
echo "$0 PID=$$"
$ echo $$
8232
$ id2
./id2 PID= 8362 //different PID due to the fact of that id2 is not a builtin, so it reports the $$ of the subshell that runs it
$ echo $$
8232

if we wish to find the last PID of a process that ran in the background, we can give the command $!
What follows, is an example:

$ sleep 60 &
[1] 8376
$ echo $!
8376

Note: !$ is not available in the tcsh shell.

If we wish to find the exit status of a command, we can give the $? command ($status in the tcsh shell)
As per usual, an exit status of 0 means success, a exit status of 1 usually means an error (outside of custom assigning this value to exiting):

$ ls es
es
$ echo $?
0 //successful
$ ls xxx
ls: xxx: No such file or dir
$ echo $?
1 //failure

We can also specify the exit status of a command:

$ cat es
echo This program returns an exit status of 7.
exit 7
$ es
This program returns an exit status of 7.
$ echo $?
7 //exitstatus of es was 7
$ echo $?
0 //exit status of echo $? was 0

When we refer to the commands on the commandline, they are positional arguments.
In Bash, we can change the value of these arguments by virtue of the set command (Does not do the same thing in tcsh)

Also, we cannot change the value of the command name from within the script.

What follows is a listing of some of these arguments:

$# //pretty much like argv

$0 //name of the calling program, basically just a positonal argument to the first element which happens ot be hte command name 

We can find out the name of the programming calling, by virtue of $0, as follows:

$ cat abc
echo "The command used to run this script is $0"
$ ./abc
The commmand used to run this script is ./abc
$ ~sam/abc
The command used to run this script is /home/sam/abc

We can also use the basename utility and $0 reference to extract a simplename of the program:

$ cat abc2
echo "This command used to run this script is $(basename $0)"
$ ~sam/abc2
The command used to run this script is abc2

When we talk about ranges of commands on the commandline, single digit commands, such as $0 and $5 requires no []. 
References of 10+ requires []:encasing, as follows: $[10] and $[15] etc.

Note: We can refer to commands that are out of index range in Bash, albeit it returns a null value.

We can also "Promote" arguments in terms of their position, with the virtue of shift. Shift can be followed by a numeric argument,
to promote by <numeric input> ranks. Anything that promotes past $1, gets discarded.

When we call upon set in Bash (not supported in tcsh), it assigns commandline arguments based on the commands fed to the set statement.
As such, set this is a test, leaves us with $1 being this, $2 is, $3 a, $4 test etc. etc.

What follows, is an example of showcasing how set can be used:

$ date
Wed Aug 13 17:35:29 PDT 2008
$ cat dateset
set $(date)
echo $*
echo
echo "Argument 1: $1"
echo "Argument 2: $2"
echo "Argument 3: $3"
echo "Argument 6: $6"
echo
echo "$2 $3, $6"

$ ./dateset
Wed Aug 13 17:35:34 PDT 2008

Argument 1: Wed
Argument 2: Aug
Argument 3: 13
Argument 6: 2008

Aug 13, 2008 //result of the echo "$2, $3, $6"

We can call set with options to modify the shell in BASH (not available in tcsh)

If we call set without any arguments, we find that the list we get, is the same as declare and typeset without arguments (in Bash)

If we wish to refer to all commandline arguments, we can use $* or $@, where they differ slightly in formatting, as follows:

Note: If we use positional references to commandline arguments, we must refer to them with ""s which causes them to be literal and intact.
This means, that if we were to call a argument that is null, we'd literally get a empty value.

If we expand a value that is null or not set, it will produce a null string.

If we wish to use the default value of something, we can access that with :-, as follows:

${name:-default}

The shell interpets this as if name is unset or null, expand it and use it, otherwise, use the default instead.

The default can also hold expanding references, as per shown:

$ ls ${LIT:-$HOME/literature} //LIT is just a dir name variable

if we wish to assign the value of something, to be the default value, we could use the following format:

${name:=default} //This assigns the default value of name to be <default>. It expands the reference first, and then declares the default value to be the expanded
referenced value.

To set the default name for a unset or null variable in a shell script, we can write as:

: ${name:=default}

An example, of how to assign the default value to a null, is as follows:

: ${TEMPDIR:=/tmp} //If references uses TEMPDIR to be the temp dir of a place, we can assign it the standard value of /tmp by this syntax, assuming TEMPDIR is null

When we need to set a default value for something, but we can't at that time point, we can display an error message with :?<message>. If we omit <message>,
we get the default error that we would expect to get:

In the following example, TESTDIR is not set, so we display this error for it

cd ${TESTDIR:?$(date +%T) error, variable not set.}
bash: TESTDIR: 16:16:14 error, variable not set

What follows, is a list of builtins:

type (which in tcsh) : Explains about a command that is given, as follows:

$ type cat echo if lt
cat is hashed (/bin/cat) //if the command has been run before in the current shell, it has been hashed
echo is a shell builtin
who is /usr/bin/who
if is a shell keyword
lt is aliased to 'ls -ltrh | tail'

read : reads one line from standard input and assigns it to something

What follows, is an example of the usage of read:

$ cat read1
echo -n "Go ahead: "  //Surpresses new line char, allowing for same line input
read firstline
echo "You entered: $firstline" //quotation avoids unintended use
$ ./read1
Go ahead: <user typed input>
You entered: <user typed input>


$ cat read1_no_quote
echo -n "Go ahead: "
read firstline
echo You entered: $firstline

$ ./read1_no_quote
Go ahead: *
You entered: read1 read1_no_quote script.1 // * without quotation, means expand to all the files in the cwd
$ ls
read1 read1_no_quote script.1

We can also use the -p command on the read builtin, to allow for prompting the user for info, instead of needing to supply another echo

$ ./read1
Go ahead: *
You entered: *

What follows, is a script that reads input from the commandline and then tries to execute the command:

$ cat read2
read -p "Enter a command: " cmd
$cmd
echo "Thanks"

Some examples of using the read2 utility:

$ ./read2
Enter a command: echo Please display this message.
Please display this message
Thanks

$ ./read2
Enter a command: who
max 	pts/4 	Jun 17 07:50 	(:0.0)
sam 	pts/12 	Jun 17 11:54 	(bravo.example.com)
Thanks

If were were to try to expand into a non-legit command, the following would occur:

$ ./read2
Enter a command: xxx
./read2: line 2: xxx : command not found

We can also do multiple assignments at once:

$ cat read3
read -p "Enter something: " word1 word2 word3
echo "Word 1 is: $word1"
echo "Word 2 is: $word2"
echo "Word 3 is: $word3"

$ ./read3
Enter something: this is something
Word 1: this
Word 2: is
word 3: something

if there is a remainder, it gets put to the last variable, as follows:

$ ./read3 
Enter something: this is something kappa kappa kappa
word 1 is: this
word 2 is: is
word 3 is: something kappa kappa kappa

What follows, is a list of some of the utilities supported by the read command:

-a <array-name> : Assigns all the inputs to an array called <array-name>

-d <delim> : Uses delim to terminate the input instead of NEWLINE.

-e : if input comes from the keyboard, it uses the Readline utility to get input

-n <num> : Number of chars, reads <num> chars

-p <prompt> : Displays prompt on standard error without a terminating newline before reading the input.
Displays prompt only when input comes from the keyboard.

-s : Does not echo characters

-u<n> : Uses the n:th file descriptor to associate the data with.  read -u4 arg1 arg2 is the same as read arg1 arg2 <&4
It returns an exit status of 0 if it successfully reads any data. It has a non-0 exit status when it reaches the EOF.
(Basically, it returns 0 if it's successful in reading data, 1 if it reached the EoF)

We can also run a while loop from the commandline, as follows:

$ cat names
Alice Jones
Robert Smith
Alice Paulson
John Q. Public

$ while read first rest
> do
> echo $rest, $first
> done < names  //a while loop that terminates after reaching the end, reading two elements at a time, first and rest, echoes them, and gets its input from names

Jones, Alice
Smith, Robert
Paulson, Alice
Q. Public, John
$

Each time you redirect input, the shell opens the input and repositions the read pointer at the start of the file:

$ read line1 < names; echo $line1; read line2 < names; echo line2
Alice Jones
Alice Jones

We could re-write this, with paranthesis, that allows for one entire piece which only launches the command once:

$ (read line1; echo $line1; read $line2; echo $line2) < names
Alice Jones
Robert Smith

Another way to do it, is to hold a file descriptor with the relevant named related to it, get the info, and then close the descriptor:

$ exec 3< names
$ read -u3 line1; echo $line1; read -u3 line2; echo $line2
Alice Jones
Robert Smith
$ exec 3<&-

We can use exec to overlay the current process to commit the commands we wish.
Mainly, we use exec for two things: redirecting info from file descriptors or to execute things without rendering a new subshell that gets spawned from executing the command.

There are some similarities between the exec and the . commands, albeit there is a few important details:

exec can run programs, scripts etc. (. only runs scripts).

exec does NOT return control to the original script running it (Which means we must always denote exec as the last command in a context, if we run it)
Altho, it runs faster than the . command, due to this, as well.

exec does NOT allow access to local variables (. does).

What follows, is an illustration of this:

$ cat exec_demo
who
exec date
echo "This line is never displayed."

$ ./exec_demo
<output of date>

Echo is never run, due to date being told to exec

The following example, is a modified example of an earlier script we wrote, where we execute all the commands given:

$ cat out2
if [ $# -eq 0 ]
	then
		echo "Usage: out2 [-v] filenames" 1>&2
		exit 1
fi
if [ "$1" = "-v" ]
	then
		shift //shift all the args by one to avoid the verbose argument
		exec less "$@" //execute the commands with less formatting
	else
		exec cat -- "$@" //Execute cat on all the parameters with no modifications
fi

An example of using exec to redirect input from standard input to the input from a file:

exec < <infile>

whilst what follows, is an example of a exec that feeds out standard output to outfile and standard error to errfile:

exec > outfile 2> errfile

When we redirect things, with exec, we do NOT lose control. It's only if we execute a command with exec, that it does not return control.

If we want to send input to a users terminal, the one being used, we can access the /dev/tty . This command, /dev/tty, always refers to the terminal
of which the user is using, regardless of where they are logged in etc.

Even if standard output and standard err has been redirected, the output to /dev/tty does not get diverged.

What follows, are some examples of interacting with this:

$ cat_to_screen1
echo "Message to standard output"
echo "Message to standard error" 1>&2
echo "Message to the user" > /dev/tty

$ ./to_screen1 > out 2> err
message to the user
$ cat out
message to standard output
$ cat err
mesage to standard error

If we wish to redirect output from a script to the screen, we are using, we could use the command of:

exec > /dev/tty

A disadvantage of exec, is that to redirect it again, once directed to the screen, you need to redirect it again.

if we were to redirect /dev/tty, it will redirect standard input to come from the keyboard.

Some examples:

read name < /dev/tty //read input from keyboard

exec < /dev/tty

If we wish to catch a signal from something, as in, we wish to know any kind of signal (error, completion etc.),
we can use the trap command for that.
 
Trap is intended for using to catch a singal and do something, when X signal is given.

What follows, is a list of utilities of signals:

Type 						Name 			Number 			Generating Condition

Not a real signal 		    EXIT 		    0 				Reached EoF or exit command. (still useful in terms of trap)

Hang up 				    SIGHUP or
						    HUP 				1 				Disonnect the line

Terminal interuppt 			SIGINT or INT 		2 				Press the interuppt key (CTRL+C)

Quit 						SIGNQUIT or QUIT    3 				Press the QUIT key (Ctrl+Shift+| or Ctrl+Shift+\)

Kill 						SIGKILL or KILL 	9 				The kill builtin with the -9 option (CANNOT BE TRAPPED)

Software Termination 		SIGTERM or TERM 	15 				Default of the kill command

Stop 						SIGSTP or TSTP 		20 				Press the suspend key (Ctrl+Z)

Debug 						DEBUG 								Executes commands specified in the trap statement
																after each command.

Error 						ERR 								Executes commands specified in the trap statement
																after each command that returns a non-zero exit status


The default condition of a trap is to exit the script.

The general syntax to be able to trap a signal, is as follows:

trap ['commands'] [signal]

The optional commands part defines what commands we run when we trap said signal.

The signal part can consist of either numbers (coresponding to the number of the signal) or the name of a signal.

Generally, we should use '' for notation on the commands in which we wish to trap, due to that it forces the commands to expand upon signal
occuring, instead of it being expanded afterwards.

Also, we can prevent exits, with trapping, if we wish, by virtue of citing an empty string, as follows:

trap '' 15 //Traps the signal number 15, and continues afterwards.

What follows, is an example of a command that traps an interupption signal and then prints an output that it was interuppted:

$ cat inter
#!/bin/bash
trap 'echo PROGRAM INTERRUPTED; exit 1' INT //We can use SIGNINT, INT or 2 to specify the signal //Could also replace exit 1 with : , which is a synonym for true
while true
do
	echo "Program running."
	sleep 1
done

$ ./inter
Program running.
Program running.
Program running.
Ctrl+C
PROGRAM INTERUPPTED
$

Usually, Trap causes termination of temp files. What follows, is an example of this:

$ cat addbanner
#!/bin/bash
script=$(basename $0)

if [ ! -r "$HOME/banner" ]
	then
		echo "$script: need readable $HOME/banner file" 1>&2
		exit 1
fi

trap 'exit 1' 1 2 3 15 //trap any premature or error signal
trap 'rm /tmp/$$.$script 2> /dev/null' 0 //On success, terminates temp file, redirects strd error to trashbin in case of attempt to remove a non-existant temp file

for file
do
	if [ -r "$file" -a -w "$file" ]
		then
			cat $HOME/banner $file > /tmp/$$.$script  //puts on a banner on top of the file after it's been redirected to the temporary file
			cp /tmp/$$.$script $file //copy the edited contents from the temp file into the specified file
			echo "$script: banner added to $file" 1>&2 //banner was added, give prompt of info
		else
			echo "$script: need read and write permissions for $file" 1>&2
		fi
done


To abort a process, we can use the kill command, as follows:

kill [-signal] PID

signal is what kind of signal we wish to send, PID is the processes ID //Note: We can signify the job number as %<job number> instead of PID

if we provide no signal, it defaults to a TERM signal, a termination signal that stops execution of the process.

Note: It is very good praxis to write handling of cancellation in terms of signals, for Bash. Due to the fact of temporary files being created
or the equals. To handle them, we need to provide the trap command to assure that we handle the signal.

The following example sends a 1 signal (Term) to a process, regardless of fg or bg:

$ kill -TERM %1

To find a list of signal names, we can write : kill -l

A program can be written to ignore interupption signals, such as CTRl+C. To ensure a sure kill, we need to write the command.

If we wish to parse command-line arguments and keep them to a better standard, we can use the getopts builtin.
The syntax for it, is as follows:

getopts <optstring> <varname> [arg ...]

optstring is a list of valid option letters, varname is the variable that recieves the commands,
and arg is the optional list of arguments to be processed.

if arg is not present, the command uses the ones on the commandline instead.

if optstring starts with : , the script must handle error messages, otherwise getopts will print error messages.

What follows, is an example of a string that could be fed to getopts as an argument:

dxo:lt:r //This would signify getops should search for -d, -x, -o, -l, -t and -r options, and that the -o and -t options take
arguments.

We could use getopts to run it in a while loop to see each option.

If we were wanting to write a program that does the following:

A -b option that indicates that the program should ignore whitespace at the start of input lines

A -t option followed by the name of a dir name that acts as the storage of temp files, if none specified, use /tmp

a -u option indicating that the program should translate all of the output to uppercase

If there is -- , it should ignore all other options.

The following program, completes this, without getopts:

SKIPBLANKS=
TMPDIR=/tmp
CASE=lower
while [[ "$1" = -* ]] //Run the loop while the first command is a option
do
	case $1 in  //if $1 matches any of the below things
		-b) 	SKIPBLANKS=TRUE ;; //if the command is -b
	 	-t) 	if [ -d "$2" ] //if the command is -t, and the second argument is a dir
	 				then
	 				TMPDIR=$2 //assigns tmpdir to be the 2 command
	 				shift //shift everything forward to keep processing commands
	 			else
	 				echo "$0: -t takes a dir argument." >&2 //prompt about info
	 				exit 1
	 			fi ;; //finish of if statement
	 	-u) 	CASE=upper ;; 	//turn on uppercasing
	 	--) 	break 	   ;; #Stop processing options 		// Stop processing options, because a -- command was given
	 	*) 		echo "Invalid option $1 ignored." >&2 ;; //Any other format of a command is ignored
	 	esac //end of case
	shift //shift one step forward, regardless of outcomes
done //end of loop signification


What follows, is a similar script, but built with the getops builtin:

SKIPBLANKS=
TMPDIR=/tmp
CASE=lower

while getopts :bt:u arg //getops strips off the hypen of the letter to process
do
	case $arg in
		b) 		SKIPBLANKS=TRUE ;;
		t) 		if [ -d "$OPTARG" ] //if the optional arg is a dir
					then
					TMPDIR=$OPTARG
				else
					echo "$0: $OPTARG is not a dir." >&2
					exit 1
				fi ;;
		u) 		CASE=upper ;;
		:) 		echo "$0: Must supply an argument to -$OPTARG." >&2
				exit 1 ;;

		\?) 	echo "Invalid option -$OPTARG ignored." >&2 ;;
		esac
done

What follows, is a list of some of the builtins in bash:

: : Returns 0 or true 

. : executes a shell script as part of a process

bg : puts a supended job in the background

break : Exits from a looping structure

cd : changes wd so that cwd becomes cd

continue : Start with the next iteration of a loop structure

echo : Displays its arguments

eval : Scans and evaluates the commandline

exec : Executes a script or a program in place of the current process

exit : Exits from the current shell (usually the same as CTRL+D in an interactive shell)

export : Makes a variable global

fg : brings a bg job to the fg

getopts : Parses arguments to a script

jobs : Lists all bg jobs

kill : sends a singal to a job or a process

pwd : dislays the cwd

read : reads a line from standard input

readonly : Declares a variable to be readonly

set : Sets shell flags or command-line argument variables; if no argument, lists all variables

shift : Promotes each command-line argument 

test : compares arguments

times : Displays total times for current shell and all of it's children

trap : traps a signal

type : Displays how each argument would be interpeted as a command

umask : returns the value of the umask

unset : undeclares a variable or command

wait : wait for a bg process to terminate

What follows, is three ways of writing the same arithmetic expression:

$ let "VALUE=VALUE * 10 + NEW" 

or

$ ((VALUE=VALUE * 10 + NEW)) //Note about *, since Bash does not perform pathane expansion on the right hand side of a = operator, we can use * freely here,
in terms of quoting and stuff

$ let VALUE=VALUE*10+NEW 

Note: Every = assignment in a let statement is a seperate command, thus, we can chain them:

$ let "COUNT = COUNT + 1" VALUE=VALUE*10+NEW //ERRORNOUS WRITING; NEEDS TO BE WRITTEN AS FOLLOWS:

$ let ((COUNT = COUNT +1, VALUE=VALUE*10+NEW))

What follows is a showcasing of arithmetic expression:

$ cat age2
#!/bin/bash
echo -n "How old are you?"
read age
if((30 < age && age < 60)); then
		echo "Wow, in $((60-age)) years, you'll be 60!"
	else
		echo "You are too young or too old to play"
fi

$ ./age2
How old are you? 25
You are too young or too old to play

What follows, is a bit more of a complicated chain of conditional commands, that does the following:

Tests the bin and src/myscript.bash exists. if its true, cp copies src/myscript.bash to bin/myscript, if the copy succeeds,
chmod makes myscript executable. If any of the steps fails, echo displays a message.

$ [[ -d bin && -f src/myscript.bash ]] && cp src/myscript.bash \
bin/myscript && chmod +x bin/myscript || echo "Cannot make myscript executable"

if we wish, there is String matching as well, as far as parsing commands goes, which uses the following list:

# : Removes minimal matching prefixes

## : Removes maximal matching prefixes

% : Removes minimal matching suffixes

%% : Removes maximal matching suffixes

An example of these usages, is as follows:

$ SOURCEFILE=/usr/local/bin/src/prog.c

$ echo ${SOURCEFILE#/*/}
local/bin/src/prog.c

$ echo ${SOURCEFILE##/*}
prog.c

$ echo ${SOURCEFILE%/*}
/usr/local/src

$ echo ${SOURCEFILE%%/*}
 		//temp

$ echo ${SOURCEFILE%.c}
/usr/local/src/prog

$ CHOPFIRST=${SOURCEFILE#/*/}
$ echo $CHOPFIRST
local/src/prog.c

$ NEXT=${CHOPFIRST%%/*}
$ echo $NEXT
local

An example of finding the length of a string:

$ echo $SOURCEFILE
/usr/local/src/prog.c
$ echo ${#SOURCEFILE}
21

What follows, is a list of operators that are sorted in descending order. The order, defines the predecence of said operators:

var++ : post increment
var-- : post decrement

++var : pre increment
--var : pre decrement

- : unary minus

+ : unary plus

! : boolean not

~ : bitwise complement

** : Power to

* : Multiplication

/ : Division

% : Remainder

- : Subtraction

+ : Addition

<< : left bitwise shift

>> : right bitwise shift

<= : less than or equal to

>= : greater than or equal to

< : less than

> : greater than

== : equality

!= inequality

& : bitwise and

^ : bitwise XOR

| : bitwise OR

&& : boolean and

|| : boolean OR

?  : Ternary operator

=, *=, /=, %=, +=, -=, <<=, >>=, &=, ^=, |= : As per usual

, : comma

We can change the predecence of ordering by virtue of (), which is as per usual math rules.

Note: Pipes take predecence over normal operators, as follows:

$ cmd1 | cmd2 || cmd3 | cmd4 && cmd5 | cmd 6

is the same thing as

$ ((cmd1 | cmd2) || (cmd3 | cmd4) && (cmd5 | cmd6))

We can chain conditional operators in terms of commandline execution, as follows:

$ mkdir bkup && cp -r src bkup //if mkdir is succesful, copy copies src to bkup recursively

An example of using an or statement in a commandline:

$ mkdir bkup || echo "mkdir or bkup failed" >> /tmp/log

or

$ (mkrdir bkup && cp -r src bkup) || echo "mkdir failed" >> /tmp/log

We can also assign base numerals to different numbers, such as base numerals of Binary systems, as follows:

$ ((v1=2#0101)) //5 in binary
$ ((v2=2#0110)) //6 in binary
$ echo "$v1 and $v2"
5 and 6

A bitwise operator, operates on bit, as follows:

5 (0101) and 6 (0110), the bit that is on for both is 0100, which is 4
$ echo $(( v1 & v2 ))
4

Without (), both || and && have the same predecence, and are grouped left to right.
Thus, we should use () to seperate relevant parts.

Bash has a list of utilities that is based on taking forth the procedural utilities, which is found in C and Perl.

The benefits of a procedural language, is as follows:

Declare, assign and manipulate variables and constant data.

Break large problems into smaller : bash allows for subshells and recursion

Conditional executions

iterative executions

transfer data to and from the program

What follows, is an example of a function that recursively creates a path, if one is provided to it:

function makepath()
{
	if [[ ${#1} -eq 0 || -d "$1" ]] //Examines path argument, if the first argument is null or the first argument is a dir, do nothing
		then
			return 0 		#Do nothing
	fi
	if [[ "${1%/*}" = "$1" ]] //if the name is a simple name, such as max, they are equal. If the name is a /<name>, it is turned into a null string.
		then
			mkdir $1 //make the dir for the first argument
			return $? //return the exit code of the last command run
	fi

	makepath ${1%/*} || return 1
	mkdir $1
	return $?
}


A good way to figure out the process of how a function does something, is that we can set debugging on, with the following syntax:

$ set -o xtrace

an example follows, after having activated it:

$ ./makepath a/b/c
+ makepath a/b/c
+ [[ 5 -eq 0 ]]
+ [[ -d a/b/c ]]
+ [[ a/b = \a\/\b\/\c ]]
+ makepath a/b
+ [[ 3 -eq 0 ]]
+ [[ -d a/b ]]
+ [[ a = \a\/\b ]]
+ makepath a
+ [[ 1 -eq 0 ]]
+ [[ -d a ]]
+ [[ a = \a ]] //By breaking down the pathname, the recursive function reaches a point of where it can start, where of it then recursively builds up the things, with the reverse
//path of what it found.
+ mkdir a
+ return 0
+ mkrdir a/b
+ return 0
+ mkrdir a/b/c 
+ return 0

An example, follows of a failed recursive function call:

$ ./makepath /a/b
+ makepath /a/b
+ [[ 4 -eq 0 ]]
+ [[ -d /a/b ]]
+ [[ /a = \/\a\/\b ]]
+ makepath /a
+ [[ 2 -eq 0 ]]
+ [[ -d /a ]]
+ [[ '' = \/\a ]]
+ makepath
+ [[ 0 -eq 0 ]]
+ return 0
+ mkdir /a //This would attempt to create a in the root dir, which unless you have permissions to do, will not allow you to do that.
//which means that it reports an error and fails.
mkdir: cannot create dir '/a': permission denied
+ return 1
+ return 1


Note: When we deal with recursive functions, it implies that every single run of the function, will have a seperate set of attributes, which means that
we should allocate the variables to be local, to make it so that it does not interfere with the same variable in unintended ways.
We can achieve this, by making sure that all interacting variables are set with typeset.

What follows, is a quiz script, based on the idea of having several dirs that holds the questions and answers to a quiz app:

//begin with the skeleton

function initialize
{
trap 'summarize ; exit 0' INT 	#Handle user interupptions
num_ques=0 						#Number of questions asked
num_correct=0 					#number of correct answers
first_time=true 				#True until first question asked
cd ${QUIZDIR:=~/quiz} || exit 2 //make the current dir to Quizdir, and set Quizdir to ~/quiz if its null
}

function choose_obj
{
subjects=($(ls)) //take all of the elements in the cwd
PS3="Choose a subject for the quiz from the preceding list: "
select Subject in ${subjects[*]}; do
	if [[ -z "$Subject" ]]; then //if the size of the Subjects is empty, it quits
		echo "No subject chosen. Bye." >&2
		exit 1
	fi
	echo $Subject //Echoes the subject chosen
	return 0
done
}



function scramble
{
typeset -i index questcount
questions=($(ls))
questcount=${#questions[*]} 	#number of elements
((index=questcount-1))
while [[ $index > 0 ]]; do
	((target=RANDOM % index)) //generates a random number modulus the index (RAndom is from 0 to 32767)
	exchange $target $index //exchanges the positions of index and target element, we define exchange ourselves
	((index -= 1)) //index reduced by one
done
}

function exchange () //Switches palces of two elements
{
temp_value=${questions[$1]}
questions[$1]=${questions[$2]}
questions[$2]=$temp_value
}

function ask
{
# Reads a question file, asks the question, and checks the
# answer. Returns 1 if the answer is correct, 0 otherwise.
#if it encounters an invalid question file, exits with status 2

exec 3<$1 //We open the question file with the third prompt
read -u3 ques || exit 2 //Attempts to read the question, exits due to error if it can't find it in the selected file
read -u3 num_opts || exit 2 //attempts to display the amount of options, exits due to error if it can't find it in the selected file

index = 0
choices=() //Create a null string
while (( index < num_opts )) ; do //while there are still options to read
	read -u3 next_choice || exit 2  //read the next choice from the third file descriptor
	choices=("${choices[@]}" "$next_choice") //generates choices
	((index += 1))
done
read -u3 correct_answer || exit 2 //Attempts to read correct answer, exits if it can't find it
exec 3<&- //close 3:rd file descriptor

if [[ $first_time = true ]] ; then //if its your first time
	first_time=false //Cancel it
	echo -e "You may press the interupt key at any time to quit.\n" //display info prompt
fi

PS3=quest"	" 				#Makes $ques the prompt for select
							# and add some spaces for readability

select answer in "${choices[@]}"; do //select structure for you to select a element from choices
	if [[ -z "$answer" ]]; then //if the size of the answer is 0, ie you fed in something invalid
			echo "Not a valid choice. please choose again" //not valid 
		elif [[ "$answer" = "$correct_answer" ]]; then //if you have the correct answer
			echo "Correct!" //Yay!
			return 1 //Return with a true
		else
			echo "No, the answer is $correct_answer." //Correct the user
			return 0 //return with False
	fi
done
}

function summarize
{
#Presents the user's score
echo 				//formatting
if (( num_ques == 0 )); then
	echo "You did not answer any questions!"
	exit 0
fi

(( percent=num_correct*100/num_ques ))
echo "You answered $num_correct questions correctly, out of $num_ques total questions."
echo "Your score is $percent percent."
}

# Main program
initialize 			#Step 1 in a top level design

subject=$(choose_subj) 		#step 2
[[ $? -eq 0 ]] || exit 2 	#if no valid choise, exit

cd $subject || exit 2 		#step 3
echo 						#Formatting with empty line
scramble 					#step 4

for ques in ${questions[*]}; do 		#step 5
	ask $ques
	result=$?
	(( num_ques=num_ques+1 ))
	if [[ $result == 1 ]]; then
		(( num_correct += 1 ))
	fi
	echo 					#Formatting with empty line
	sleep ${QUIZDELAY:=1} 	#if it is null, set it to 1, or otherwise set it to null as well
done

summarize 				#step 6
exit 0

PERL:

Perl scripts usually start with #!/usr/bin/perl

An example of finding any files that contain the above, recursively:

$ grep -r /usr/bin/perl /usr/bin /usr/sbin | head -4
/usr/bin/defoma-user:#! /usr/bin/perl -w
/usr/bin/pod2latex:#!/usr/bin/perl
/usr/bin/pod2latex: 	eval 'exec /usr/bin/perl -S $0  ${1+ "$@"}'

The above files are examples taht we can access and modify.

If we wish to have warnings displayed, we can activate the -w option or use warnings.

If we wish to find manual help akin to man, we can use the perldoc utility. It works with a language called pod (plain old documentation)
to integrate documentation of code and the project, in your code.

What follows, is an example of a perl file with some documentation attached to it:

$ cat pod.exl.pl
#!/usr/bin/perl

=head1 A Perl Program to Say I<Hi there.>

This simple Perl program includes documentation in B<pod> format.
The following B<=cut> command tells B<perldoc> that what follows is not documentation.

=cut
#A Pearl Program
print "Hi there.\n";

=head1 pod Documentation Resumes with any pod command

See the B<perldoc.perl.org/perlpod.html> page for more info on B<pod> and B<perldoc.perl.org> for more complete Perl documentation

We can run the program, by the virtue of the following:

$ perl pod.exl.pl
Hi there.

or we can use the perldoc utility to display the information:

$ perldoc pod.ex1.pl
POD.EX1(1) 			User Contributed Perl Documentation 		POD.EX1(1)

A Perl Program to Say Hi there.
		This simple Perl program includes documentation in a pod format. The
		following =cut command tells perldoc that what follows is not documentation.

pod Documentation Resumes with Any pod Command
		See the perldoc.perl.org/perlpod.html page for more information on pod
		and perldoc.perl.org for complete Perl documentation.

We can call upon a function without arguments to find info about it:

$ perldoc -f print
	print FILEHANDLE LIST
	print LIST
	print Prints a string or a list of strings. Returns true if succesful.
		  FILEHANDLE may be a scalar variable name, in which case the variable
		  contains the name of or a reference to the filehandle, thus introducing
		  one level of indirection. (NOTE: If FINEHANDLE is a variable and the next
		  token is a term it may ...)


Perl uses itself of Modules, and we can find info about them, as is followed by this example of Timestamp:

$ perldoc Timestamp::Simple
Timestamp::Simple(3) 	User Contributed Perl Documentation Timestamp::Simple(3)

NAME
		Timestamp::Simple - Simple methods for timestamping
SYNOPSIS
			use Timestamp::Simple qw(stamp);
			print stamp, "\n";

To get the man page, we can give the command of:

man perldoc

or

perldoc.perldoc to display the perldoc man page and read more about this tool.

When we talk about modules, modulenames are differentiated by :: (end point of a name)

We also have packages in perl, they are written as follows:

$WWW::Mechanize::ex , $ex is a scalar variable in the WWW::Mechanize namespace.

Just as in Java, we can interact with different packages and retrieve attributes from them, as follows:

$Text::Wrap::columns

By default, a variables are package variables unless we define them as lexical variables.

A lexical variable, is basically the equivilant of a local variable.

We have lists (same as Python with any type included), and arrays.

Arrays, in perl, are declared as follows:

@<array name> = (2, 4, 'Zach')

We also have compound statements, which is a statement consisting of several statements.

To open a file in perl, we can give the command -e, as follows:

$ perl -e 'print "Hi there.\n"'
Hi there.

If we wish to enter the program that we just closed with perl, we can enter it with:

$ perl
print "Hi there. \n";
CTRL+D
Hi there.

We can do the simple operations on it, as follows:

$ cat simple.pl //pearl files usually have an extension of .pl
print "Hi there.\n"

We can also run it's argument with perl, to run the file:

$ perl simple.pl
Hi there.

To see what version we run, we can use the -v command:

$ perl -v

This is perl, v5.10.0 built for i486-linux-gnu-thread-multi

in perl, we have another way of print, which is say, and to use this, we use the keyword of use:

$ cat 5.10.pl
use feature 'say';
say 'Output by say.';
print 'Output by print.';
say 'End.'
$ perl 5.10.pl
Output by say.
Output by print.End.
$

To get the same results in earlier versions of Perl (because say is a 6.0 feature), we must use print but with \n chars:

$ cat 5.8.pl
print 'Output by print in place of say.', "\n";
print 'Output by print.';
print 'End.', "\n";

$ perl 5.8.pl
Output by print in place of say.
Output by print.End.

In perl, each command is finished by ; and can be put together to chain commands on a line, as follows:

$ cat statement1.pl
$n=4;
say "Answer is ", $n + 2;
$ perl statement.pl
Answer is 6

$ cat statement2.pl
$n = 4; say "Answer is", $n+2; //two seperate commands, ; being a command seperator
$ perl statement2.pl
Answer is 6

String quotation is the same as in the shell, which usually means "" allows interpetation of escape chars and the likes,
whilst '' is the literal interperation

What follows, is a list of examples in terms of String interaction:

$ cat string1.pl
$string="5"; 		//Declared as string, but will make no difference

print '$string+5\n'  //Literal showing, due to '' quotation

print "\n$string+5\n" //Expands the variable, but it remains string

print $string+5, "\n" //Accounts string to be number due to involved in numerical expression, followed up with newline

$ perl string.pl
$string+5\n //first print
5+5 //Second print
10 //third print

What follows, is an example of how to run a regex on a string for a string:

$ perl -e 'if ("hours" =~ /our/) {say "yes";}'

or

$ perl -e 'use feature "say"; if ("hours" =~ /our/) {say "yes";}'

In a regex, perl does not expand meta chars (like \n)

Comments are as per usual with #.

What follows, is a list of special characters that are found in perl:

\0xx 		: The ASCII char for octal value xx

\a 			: An alarm (bell or beep) character

\e 			: An escape character

\n 			: A NEWLINE char

\r 			: a RETURN char

\t 			: a TAB char

In perl, there are three types of variables : 

singular (String and Numbers) (called Scalar) (begins with $)

array (plural) (begins with @)

hash variable (plural) (called Associative Array) (begins with %)

Every time we reference a variable of respective kind, in perl, we must lead with the reference variable for that specific variable, as follows:

$ name="Zach" ; echo "$name" //This is for bash
Zach

$ perl -e '$name="Zach" ; print "$name\n";' //This is for perl
Zach

All perl variables are global, unless specified to be lexical, which can can be done by put the keyword of my before the variable.

NOTE: If two variables have the same name, in Perl, with one being a global and the other lexical, then the lexical takes predecence above the other.

Perl does not care for wether you initialized a variable or not, and you can refer unintiailzied variables, as well, if you wish.
However, the variable comes into existence when you declare it.

An example of this, follows:

$ cat variable.pl
#!/usr/bin/perl
my $name = 'Sam';
print "Hello, $nam, how are you?\n"; # Typo, e left off of name

$ ./variable.pl
Hello, , how are you? //becomes null due to unknown reference, which results in null, but perl does not care

If we declare the command of use strict, perl interacts in a stricter way, as follows:

$ cat variableb.pl
#!/usr/bin/perl
use strict;
my $name = 'Sam'; //Variable declared as being lexical, i.e, local, higher predecence in local context
print "Hello, $nam, how are you?\n"; #Typo, e left off of name

$ ./variableb.pl
Global symbol "$nam" requires explicit package name at ./variableb.pl line 4,
Execution of ./variableb.pl aborted due to compilation errors.

NOTE: Declaring a variable as my is extremely good praxis and should always be done, for completion sake.

If we wish to use -w, we can do it in the following ways:

$ cat variabela.pl
#!/usr/bin/perl -w //Declares warnings to be part of the perl script
my $name = 'Sam';
print "Hello, my name is $nam, how are you? \n"; #Prints warning due to typo and -w mode

$ ./variabela.pl
Name "main::nam" used only once: possible typo at ./variablea.pl line 3.
Use of unintialized value $nam in concatenation (.) or string at ./variablea.pl line 3.
Hello, , how are you?

The second way, is if we use the -w option. IMPORTANT: When we have a option like -e (execute), we must have -e before the filename.

$ perl -w -e 'my $name = "Sam"; print "Hello, $nam, how are you?\n"'
Name "main::nam" used only once: possible typo at -e line 1.
Use of uninitialized value $nam in concatenation (.) or string at -e line 1.
Hello, , how are you?

in perl, variables that are undefine, have a special type, called undef. These are 0 in numeric operations and "" in string terms.
To find out, wether a variable is undefined, we can use the defined command, as follows:

$ cat variable2.pl
#!/usr/bin/perl
if (!defined($name)){
print "The variable '\$name' is not defined. \n"
};

$ ./variable2.pl
The variable '$name' is not defined.

If we used -w there, it would warn us, as it does that when there are undefined vars as well.

In perl, it can detect when things are string and when they are numbers, and conversion is done by perl itself, as follows:

$ cat scalars1.pl
#!/usr/bin/perl -w

$name = "sam";
$n1 = 5; $n2 = 2;

say "$name $n1 $n2";
say "$n1 + $n2";
say '$name $n1 $n2';
say $n1 + $n2, " ", $n1 * $n2; //say accepts a list of arguments, split by , parameters, lining them all up, unless otherwise told different
say $name + $n1;

$ ./scalars1.pl
Sam 5 2
5 + 2
$name $n1 $n2
7 10
Argument "Sam" isn't numeric in addition (+) at ./scalers1.pl line 1.
5 //When it finds that something is not a numeric, it converts it to 0, so basically "sam" + 5 becomes 0 + 5

Note: perl does not distinguish an operator in quotations

Arrays in Perl expand as needed, and if you refer to out of boundry, it gives a undef value.

Arrays in perl, are ordered, hashes are not.

When refering to a singular value, we must refer to it with $, its type notation

NOTE: Arrays in Perl start with 0

$ cat arrayvar1.pl
#!/usr/bin/perl -w
@arrayvar = (8, 18, "Sam");
say $arrayvar[1];
say "@arrayvar[1,2]";

$ perl arrayvar1.pl
18
18 Sam

Note: When we are in a scalar context, perl understands a arraysname as it's length.

What follows, is an example of array interaction:

$ cat arrayvar2.pl
#!/bin/usr/perl -w
@arrayvar2 = ("apple", "bird", 44, "Tike", "metal", "pike");

$num = @arrayvar2; 		//the number of elements in the array
print "Elements: ", $num, "\n"; 	//The following print statements are equal
print "Elements: $num\n";

print "Last: $#arrayvar2\n"; 	#index of last element in the arrayvar2

$ .arrayvar2.pl
Elements: 6
Elements: 6
Last: 5 //Last index, showcasing beginning index of 0

In perl, we can concatenate strings, in Arrays, with . , in the declaration, as follows:

$ cat arrayvar3.pl
#!/usr/bin/perl -w
$v1=5, $v2=8;
$va = "Sam"; $vb = "uel";
@arrayvar3 = ($v1, $v1 * 2, $v1 * $v2, "Max", "Zach", $va, $vb);

print $arrayvar3[2], "\n"; 			#One element of an array is a scalar
print @arrayvar3[2,4], "\n"; 		#plural, becomes array
print @arrayvar3[2..4], "\n\n"; 		#Range from 2 to 4

print "@arrayvar3[2,4]", "\n"; 		#a list, elements seperated by Spaces
print "@arrayvar3[2..4]", "\n\n";   #a slice, elements seperated by spaces //When quoted with "" allows for space seperation in arrays
									#NOTE ABOUT RANGE: Range in Perl includes the max limit, so 2-4 is 2,3,4

print "@arrayvar3\n"; 				# an array, elements seperated by spaces

$ ./arrayvar3.pl
40
40Zach
40MaxZach

40 Zach
40 Max Zach

5 10 40 Max Zach Samuel


What follows, is an example of interactions upon arrays with Stack commands, such as pop, shift, push, and splice.
They do the following:

pop : Returns the last element from the array and removes it

shift : Returns the first element from the array and removes it

push : like Append, adds to the end of the array

splice : Inserts in given position of the array and replaces from position to length of inserted array

What follows, is said example of these interactions:

$ cat ./shift1.pl
#!/usr/bin/perl -w

$colors = ("red", "orange", "yellow", "green", "blue", "indigo", "violet");

say "										Display array @colors";
say " Display and remove first element of array: ", shift (@colors); //Commands that are to be executed can be chained afterwards a say statement
say "		Display remaining elements of array: @colors";

push (@colors, "WHITE");
say "	Add element to the end of the array and display:  @colors";

say " Display and remove last element of the array: ", pop (@colors);
say "	Display remaining elements of array: @colors";

@ins = ("GRAY", "FERN");
splice (@colors, 1, 2, @ins); //injects and replaces the ins array into colors at index 1
say "Replace second and third elements of array: @colors";

$ ./shift.pl
						Display array: red orange yellow green blue indigo violet
Display and remove first element of array: red
 	Display remaining elements of array: orange yellow green blue indigo violet
 Add element to end of array and display: orange yellow green blue indigo violet WHITE
 Display and remove last element of the array: WHITE
 Display remaining elements of array: orange yellow green blue indigo violet
 Replace second and third elements of array: orange GREY FERN blue indigo violet

When we have a hashed list, it's like a Dictionary in Python, except unordered, and works to extract
a certain value by providing the relevant key.

A hashed list, is effectively random.

What follows, is examples of declaring a hashed list:

$ cat hash.pl
#!/usr/perl -w
$hashvar1{boat} = "tuna";
$hashvar1{"number five"} = 5;
$hashvar1{4} = "fish";

@arrayhash1 = %hashvar1;
say "@arrayhash1";

$ ./hash1.pl
boat tuna 4 fish number five 5

This way is most akin to how python handles dictionaries, except without gets and sets.

The next example, is the other way to create hash arrays:

$ cat hash2.pl
#!/usr/bin/perl -w

%hash2 = ( //declares a hash array, syntax is akin to functional programing
	boat => "tuna"
	"number of five" => 5,
	4 => "fish",
	); //create a hash table

@array_keys = keys(%hash2); //assign keys into an array by calling the keys function on the hash table
say " 	Keys: @array_keys"; //print out the keys array

@array_values = values(%hash2); //retrieve values with the values function on the hash table
say "Values: @array_values"; //print out the values array

$ ./hash2.pl
Keys: boat 4 number of five
Values: tuna fish 5

Note: Perl automatically quote names on the left side of a => declaration

In perl, we have if and unless statements, where unless is the statement that equals to if not

An example:

if (expression) { // do stuff }

unless (expression) { //do stuff }

An example of a expression, is a option such as -r, which attempts to read a file.
It can fail on two parts; Permission or existence.

Thus, it can be used as a existence check of a certain file, in attempts of reading said file.

An example in a script:

$ cat if1.pl
#!/usr/bin/perl -w
if (-r "memo1") {
	say "The file 'memo1' exists and is readable.";
}

$ ./if1.pl
The file 'memo1' exists and is readable.

Another way we can do ifs in Perl, is postfix ifs, as follows:

$ cat if1a.pl
#!/usr/bin/perl -w
say "The file 'memo1' exists and is readable." if (-r "memo1");

If we wish to read a line from standard input and assign it to a variable in the same swoop, we can do the following:

$ $entry = <>; //reads from standard input and assigns to entry

In perl, when we compare numbers, we use the == operator, and the format of writing the number and type does not matter, as follows:

28 == 28.0

28 = 0000028.0

28 = 28

All of the above hold true in perl.

An example of comparing with this, follows:

$ cat if2.pl
#!/usr/bin/perl -w
print "Enter 28: ";
$entry = <>;
if ($entry == 28) {
	print "Thank you for entering the number 28.";
}
print "End.\n";

$ ./if2.pl
Enter 28: 28.0
Thank you for entering 28.
End.

What follows is a list of comparators for perl:

Numeric 			Strings 			Values
== 					eq 					True if equal
!= 					ne 					True if not equal
< 					lt 					True if less than
> 					gt 					True if greater than
<= 					le 					True if less than or equal to
>= 					ge 					True if greater than or equal to
<=> 				cmp 				0 if equal, 1 if greater than, -1 is less than

What follows in said example, is a situation where we use the chomp utility.
This utility, removes trailing newlines from a string. This is nessecary, otherwise comparisons will never yield true:

$ cat if2a.pl
#!/usr/bin/perl -w
print "Enter the word 'five': ";
$entry = <>; //read and assign
chomp ($entry); //remove trailing newline
if ($entry eq "five") {
	print "Thank you for entering 'five'.\n";
}
print "End.\n";

$ ./if2a.pl
Enter the word 'five': five
Thank you for entering 'five'.
End.


Akin to Bash, we have the if and else structure in perl, as follows:

if <expression> { //do stuff } else { //do stuff }

The following example asks for two numbers and assigns them to $num1 and $num2, if the user enters the same number twice,
the program reacts with a die signal.

A die signal, is basically like the bash equivilant or kill with a TERM signal.
it sends a message to standard error and aborts the entire program.

Here is the example:

$ cat ifelse.pl
#!/usr/bin/perl -w
print "Enter a number: ";
$num1 = <>;
print "Enter another, different number: ";
$num2 = <>;

if ($num1 == $num2) { 
	die ("Please enter two different numbers.\n"); //aborts execution and prints a message to standard error
}
if ($num1 > $num2){ // due to operator format, instead of gt, this allows for check against decimals 
	print "The first number is greater than the second number.\n";
}
else{
	print "The first number is less than the second number.\n";
}

$ ./ifelse.pl
Enter a number: 8
Enter another, different number: 8
Please enter two different numbers.

$ ./ifelse.pl
Enter a number: 5.5
Enter another, different number: 5
The first number is greater than the second


in Perl, we also have the if...elsif...else

Note: if we wish to redirect standard error in Perl, we can use the STDERR handle for that:

$ cat ifelsif.pl
#!/usr/bin/perl -w
print "Enter a number: ";
$num1 = <>;
print "Enter another, different number: ";
$num2 = <>;

if ($num1 > $num2) {
	print "The first number is greater than the second number.\n}";
	}
	elif ($num1 < $num2){
		print "the first number is less than the second number.\n}";
	}
	else {
		print "Please enter two different numbers.\n";
	}

for loops in perl can be written as foreach or for,it does not matter, they both do the same thing.
The general syntax is as follows:

foreach/for [var] (list) {...} //If we do not specify the var, it assigns values to the $_ variable

In the following example, we showcase loops: 
NOTE: If it complains about use not being available, we can do the use feature 'say', instead:

$ cat foreach.pl
foreach $item ("Mo", "Larry", "Curly") {
	say "$item says hello.";
}

$ perl foreach.pl
Mo says hello.
Larry says hello.
Curly says hello.

If we wish to refer to the loop without designating a value to the iterating value, we can use the $_ variable, as follows:

$ cat foreach.pl
foreach ("Mo", "Larry", "Curly") { //Note: with the $_ variable, we do not need to define a referencing current variable, it's stored in the temp value of $_ instead
	say "$_ says hello.";
	}

Another example of using for loops:

$ cat foreachb.pl
@stooges = ("Mo", "Larry", "Curly");
foreach (@stooges) { //argument list of array to iterate over
	say "$_ says hello.";
}

Antother example of using for loops, using the postfix:

$ cat foreachc.pl
$stooges = ("Mo", "Larry", "Curly");
say "$_ says hello." foreach $stooges;

What follows, is an example of loop interaction in terms of affecting variables and the actual array itself:
NOTE: the uc command is uppercase:

$ cat foreachd.pl
@stooges = ("Mo", "Larry", "Curly");
foreach $stooge (@stooges) {
	$stooges = uc $stooge; //Converst the current stooge element to uppercase, and since we iterate through the whole thing, we effectively make all UC as we go
	say "$stooge says hello.";
	}
say "$stooges[1] is uppercase" //; is only needed to signifiy end of partioning, since nothing follows, we can skip it here

$ perl foreachd.pl
MO says hello.
LARRY says hello.
CURLY says hello.
LARRY is uppercase.

In perl, we have break and continue concepts, except they are called last and next, instead.

What follows is an example of a next statement:

$ cat foreach.pl
foreach $item ("one", "two", "three") {
	if ($item eq "two") {
		next;
		}
	say "$item";
	}

$ perl foreach1.pl
one
three

The second kind of syntax, we can use with foreach, is as follows:

foreach/for (initialization of number; condition ; operation) {
	//do stuff
}

What follows, is an example of this usage of the foreach loop: //basically like for loops in Java or C

$ cat ./foreach2.pl
#!/usr/bin/perl -w

print "Enter starting number: ";
$start = <>;

print "Enter ending number: ";
$end = <>;

print "Enter increment: ";
$incr = <>;

if ($start >= $end || $incr < 1) {
	die ("The starting number must be less than the ending number\n", " and the increment must be at least 1.\n");
	}

foreach ($count = $start+0; $count <= $end; $count += $incr) { //the $start+0 forces the variable to be numeric, stripping away the \n char that would be there otherwise
	say "$count";
	}

$ ./foreach2.pl
Enter starting number: 2
Enter ending number: 10
Enter increment: 3
2
5
8

In perl, we have the while and until loops, as per usual:

What follows is an example of a loop that reads data until there is no more data to read ; in this case, it's standard input, but it can be from a file as well:

$ cat while1.pl
#!/usr/bin/perl -w
$count = 0;
while ($line = <>) {
	print ++$count, ". $line";
}
print "\n$count lines entered.\n";

$ ./whilel.pl
Good Morning.
1. Good Morning.
Today is Monday.
2. Today is Monday.
CTRL + D //signals EoF if done on a empty line

2 lines entered.

in perl, we can refer to having a count variable (account for how many inputs have been read) in the form of the $. variable, as follows:

$ cat whilea.pl
#!/usr/bin/perl -w
while ($line = <>) {
	print $., ". $line"; //the $. replaces count, in terms of how many inputs we have read
	}
print "\n$. lines entered.\n";


When we refer to a iterating element in perl, we can use the $_ variable.
What follows, is a loop that runs a regex on input from the commandline until it runs out of inputs:

while (my $line = <>) {
	chomp $line;
	if ($line =~ /regex/) ... //the <line> =~ <pattern> is the regex matching in perl
}

can be rewritten with the $_ as follows:

while (my $_ = <>) {
	chomp $_; //Chomp it to remove /n chars
	if ($_ =~ /regex/) ... //replacing the $line variable with $_ because $_ is the iterating element
}

We can further rewrite it, because the $_ char is the default iterating char in this context, we can simply omit it:

while (<>) { //while $_ = <> (data read in from standard input)
	chomp; //chomp $_
	if (/regex/) ... //if regex matches against $_
}


in perl, we have file handlers, which are named by default, the following:

STDIN //the handler for standard input

STDOUT //the handler for standard output

STDERR //The handler for standard error

The general syntax of opening a file in perl, is as follows:

open (file-handle, ['mode',] "file-ref");

What follows is a list of modes:

< or nothing //opens a file for reading 

> //overwrite

>> //appends

To write to a file, we use the print command as per:

print [file-handle] "text";

What follows is an example of how to read a file from a file handler that is associated with IN:

$line = <IN>; //reads one line from the IN file handle

If we wish to simply read from standard input or a filename on the commandline, we can use the <> notation as follows:

$line = <>;

What follows is an example of redirections and file handles:

$ cat file1.pl
print STDOUT "Enter something: ";
$userline = <>;
print "1>>>$userline<<<\n";
chomp ($userline);
print "2>>>$userline<<<\n";
print STDERR "3. Error message.\n";

$ perl file1.pl 2> file1.err
Enter something: hi there
1>>>hi there
<<<
2>>>hi there<<<

$ cat file1.err
3. Error message.

What follows, is an example of opening files and directing output:

$ cat file2.pl
open (my $infile, "/usr/share/dict/words") or die "Cannot open directory: $!\n"; //the lexical (local) $infile becomes handle to the specified file to open
																			     //The or die is for error handling, in case of an error, we give a message and shut
																			     //down the process
while ($line = <$infile>) { //reads a line from the $infile file handle and puts them into $line
	print $line; //print the $line
	}

$ perl file2.pl | head -4 //open the file2, pipe the output to a head format and show the 4 first lines.
		//empty
A
A's
AOL

If we wish to access the last system error reported, we can get it with the !$ variable:

In a numeric context, !$ reports the system error number, in a string context it tells the string name of the error.

Note: When something can we wrong, we should always check what that error was.

What follows is a loop that loops through files and brings out the first line of each file:

$ cat file3.pl
foreach $filename (@ARGV) { //Iterate through the array that the commandline it was run with
	open (IN, $filename) or die "Cannot open file '$filename': $!\n"; //open $filename  with the IN file handle or Shut down the program and report the error
	print "$filename: ", $line = <IN>; //print the name and the first line from IN handler
	close (IN); //close the IN handler
	}

$ perl file3.pl f1 f2 f3 f4
f1: First line of file f1.
f2: First line of file f2.
Cannot open file 'f3': No such file or directory.

What follows, is a structure that is similar to the one above, except it does not quit when it gets hit by an error:

$ cat file3a.pl
my $filename; //declare a scalar variable
while (<>){ //while there is more to read from the commandline
	if ($ARGV ne $filename) { //if the commandname does not equal to filename, ensuring that only unique elements are printed and treated
		print "$ARGV: $_"; //print the file name and current iterating element
		$filename = $ARGV; //assign the filename variable to whatever current command is being processed on the commandline
	}
}

$ perl file3a.pl f1 f2 f3 f4
f1: First line of file f1.
f2: First line of file f2.
Can't open f3: No such file or dir at file3a.pl line 3, <> line 3.
f4: First line of file f4.

If we wish to get a array that is in reversed order, we can call the reverse function on an array to get a reversed ordered one returned to us.

What follows, is some examples of how to perform sorting on arrays:

$ cat sort3.pl
@colors = ("red", "Orange", "yellow", "green", "blue", "indigo", "violet");

say "@colors";

@scolors = sort {$a cmp $b} @colors; 				//Ascending sort of colors with an explicit block
say "@colors";

@scolors = sort @colors; 							//ascending sort of colors with an implicit block
say "@colors";

$scolors = sort {$b cmp $a} @colors; 				// descending sort
say "@scolors";

@scolors = sort {lc($a) cmp lc($b)} @colors; 		//Ascending sort, but with folding included
say "@scolors";

$ perl sort3.pl
red Orange yellow green blue indigo Violet
Orange Violet blue green indigo red yellow
Orange Violet blue green indigo red yellow
yellow red indigo green blue Violet Orange
blue green indigo Orange red Violet yellow

To sort numbers, we must use the <=> operator, as follows:

$ cat sort4.pl
@numbers = (22, 188, 44, 2, 12);

print "@numbers\n";

@snumbers = sort {$a <=> $b} @numbers;
print "@snumbers\n";

@snumbers = sort {$b <=> $a} @numbers;
print "@snumbers\n";

$ perl sort4.pl
22 188 44 2 12
2 12 22 44 188
188 44 22 12 2

in perl, we have something called subroutines, which are routines commited in not the main program, but in a lower form of routine.
Which is basically like a Method in java, as follows:

$ cat subroutine1.pl
$one = 1;
$two = 2;
add();
print "Answer is $ans\n";

sub add {
	$ans =$one + $two
	}

$ perl subroutine.pl
Answer is 3

Since we did not declare any of the variable to be lexical, they are available to the entirety of the program.

We can also add return statements to methods in perl, as follows:

$ cat subroutine2.pl
$one = 1;
$two = 2;
$ans = add();
print "Answer is $ans\n";

sub add {
	return ($one + $two)
	}

$ perl subroutine2.pl
Answer is 3

The argumentlist that gets passed into subroutines, is the @_ variable, which holds the commandline of a function, when it is called:

$ cat subroutine3.pl
$one = 1;
$two = 2;
$ans = addplusone($one, $two);
print "Answer is $ans\n";
print "Value of 'lcl_one' in main: $lcl_one\n";
print "Value of 'one' in main $one\n";

sub addplusone {
	my ($lcl_one, $lcl_two) = @_; //Declare the lexical variables to be the argumentlist of the function, with @_ being the commandline of the function
	$lcl_one++;
	$lcl_two++;
	print "Value of 'lcl_one' in sub: $lcl_one\n";
	return ($lcl_one + $lcl_two)
	}

$ perl subroutine3.pl
Value of 'lcl_one' in sub: 2
Answer is 5
Value of 'lcl_one' in main: //since it was declared lexical, it does not have any value in the main, since its undefined there
Value of 'one' in main: 1

What follows is an example of utilizing the array which was passed to the function:

$ cat subroutine4.pl
$one = 1;
$two = 2;
$ans = addplusone($one, $two);
print "Answer is $ans\n";

sub addplusone {
	return ($_[0] + $_[1] + 2);
	}

$ perl subroutine4.pl
Answer is 5

And finally, we have an example that holds a more typical example, where we get the max value of a value that we parse in:

$ cat subroutine5.pl
$ans = max (16, 8, 64, 2);
print "Maximum value is $ans\n";

sub max {
	my $biggest = shift; //assign the first value to be biggest, then shift everything by 1 to iterate over the rest
	foreach (@_) {
	$biggest = $_ if $_ > $biggest; //Assigns biggest to be the current iterating element if the curent element is bigger than biggest
	}
return ($biggest);
}

$ perl subroutine5.pl
Maximum value is 64

If we wish to apply chomp to every line (remove new line char), we can set the -l option to our commands.

What follows, are some examples of interactions in terms of regexes:

$ perl -le 'if ("aged" =~ /ge/) {print "true";}'
true

is the same thing as:

$ perl -le 'print "true" if "aged" =~ /ge/'

Where both search for a regex of ge, in a specified string, in this case, aged.

We can use the ! operator to inverse the effect of the regex, as follows:

$ perl -le 'print "true" if ("aged" !~ /xy/)' //Check to see if aged has no match with xy
true

In terms of regex, when we do the . characeter, it's a wildcharacter for matching any char, thus, we can see as follows, what occurs:

$ perl -le 'print "true" if ("aged" =~ /a..d/)'
true

We can assign a regex to a string, in perl, as follows:

$ perl -le '$re = qr/a..d/; print "true" if ("aged" =~ $re)' //to assign a regex as a string, we must write it as $variablename = qr/<regex pattern>/

What follows, is an example of a regex that c ontains /, that are escaped with \'s, as follows:

$ perl -le 'print "true" if ("/usr/doc" =~ /\/usr/)'
true

Another way to use / in regexes, is by virtue of the m{^<regex>} declaration, as follows:

$ perl -le 'print "true" if ("/usr/doc" =~ m{^/usr})'
true

We can also use the same syntax for a regular expression when we assign it to a variable:

$ perl -le '$pn = qr{^/usr}; print "true" if ("/usr/doc" =~ $pn)'
true

We can also use a regex to replace things, as follows:

$ cat re10a.pl
$stg = "This is the best!";
$stg =~ s/best/worst/; //substitute best with worst in the string
print "$stg\n";

$perl re10a.pl
This is the worst!

What follows, is a list of some of the regular expression metacharacters that we can run into, in perl:

^ : anchors a regular expression to the beginning of the line

$ : Anchors a regular expression to the ending of the line

(...) : brackets a regex

. : Any char except \n

\\ : A backslash

\b : A word boundry (zero width match)

\B : A word nonboundry (same as [^\b])

\d : A single decimal digit ([0-9])

\D : A single non-decimal digit ([^0-9] or [^\d])

\s : A single whitespace char (SPACE, NEWLINE, RETURN, TAB, FORMFEED)

\S : A single non-whitespace char ([^\s])

\w : A single word char (a letter or digit; [a-zA-Z0-9])

\W : a single nonword char ([^\W])

By default, perl uses greedy matching, which means it tries to match the longest string it can find.

What follows, is an example of a regex that accounts for {}'s:

$ cat 5ha.pl
$string "A line {remove me} may have two {keep me} pairs of braces.";
$string =~ s/{.*} //;
print "$string\n";

$ perl 5ha.pl
A line pairs of braces.

The follopwing example, includes a regex that removes the shortest enclosure of a {<text>} + space word

$ cat re5b.pl
$string = "A line {remove me} may have two {keep me} pairs of bracers.";
$string =~ s/{[^}]*} //;
print "$string\n";

$ perl re5b.pl
A line may have two {keep me} pairs of braces.

There is a shortcut for declaring a quantifier to be non-greedy, as follows:

$ cat re5c.pl
$string = "A line {remove me} may have two {keep me} pairs of braces.";
$string =~ s/{.*?} //; //Makes it a non-greedy quantifier, which means it'll match against shortest strings, instead of longest
print "$string\n";

$ perl re5c.pl
A line may have two {keep me} pairs of braces.

Note: To run a ( in a regex in perl, you must quote it.

If we wish to "split" a regex into parts that account for different segments, we can do as follows:

$ cat rell.pl
$stg = "My name is Sam";
$stg =~ s/My name is (.*)/$1/; //the $1 refers to the first occurence of the () section, which in this case, means anything that would occur there,
//due to . being any char, and * being a quantifier of rest, so basically anthing beginning with .
//Replaces the match with the first occurence of whatever was bracketed, which in this case, happens to be Sam.
print "Matched: $stg\n";

$ perl rell.pl
Matched: Sam //matches against sam, due to it being the first occurence to be in ()'s

What follows is a regex that runs against numbers:

$ cat re8.pl
$string = "What is 488 minus 78?";
$string2 = $string;
$string =~ s/\D*(\d+).*/$1/;
$string2 =~ s/\D*\d+\D*(\d+).*/$1/;

print "$string\n";
print "$string2\n";
print $string - $string2, "\n";

$ perl re8.pl
488
78
410

What follows, is a failed example of a regex, that misses a ) to match:

$ perl -le 'if ("ag(ed)" =~ /ag(ed)/) {print "true";} else {print "false";}' //This will fail, due to ( being a special char, it needs to be quoted in a Regex.
false

What follows, is a fixed version of said regex:

$ perl -le 'if ("ag(ed)" =~ /ag\(ed\)/) {print "true";} else {print "false";}'
true

Next example is what can occur when a parenthesis is unquoted:

$ perl -le 'if ("ag(ed)" =~ /ag(e/) {print "true";} else {print "false";}' //Error, unmatched (
Unmatched ( in regex; marked by <-- HERE in m/ag( <-- HERE e/ at -e in line 1

Escaping it, fixes it:

$ perl -le 'if ("ag(ed)" =~ /ag\(e/) {print "true";} else {print "false";}' //When we quote it, all is well
true

In perl, we can download tons of modules, ready for us to be used, from the following place: search.cpan.org

Assume that we have a module downloaded, here are some examples of how we would interact with them:

$ tar xzvf Timestamp-Simple-1.01.tar.gz
//lines about decompressing the file

We then locate the dir which we uncompressed the module:

$ cd Timestamp-Simple-1.01 //change dir to this dir
$ perl Makefile.PL //Call this, to try and create the module from the unpacked data
Checking if your kit is complete...
Looks good
Writing Makefile for Timestamp::Simple

Assuming everything is fine, (if it isn't, we'll get warnings from Makefile.PL) - we can run the make utility to proceed:

$ make
cp Simple.pm blib/lib/Timestamp/Simple.pm
Manifying blib/man3/Timestamp::Simple.3pm

And we need to run a maketest to make sure it's working as it should:

$ make test
PERL_DL_NONLAZY=1 /usr/bin/perl "-Iblib/lib" "-Iblib/arch" test.pl
1..1
# Running under perl version 5.1 for Linux
# Current time local: Fri Sep 4 18:20 2009
# Current time CMT: Sat Sep 5 01:20:41 2009
# Using Test.pm version 1.25
ok 1
ok 2
ok 3

Finally, we run the install with root privs, to assure that we have all the rights:

# make install
Installing /usr/local/share/perl/5.10/Timestamp/Simple.pm
Installing /usr/local/man/man3/Timestamp::Simple.3pm
Writing /usr/local/lib/perl/5.10/auto/Timestamp/Simple/.packlist
Appending installation info to /usr/local/lib/perl/5.10/perllocal.pod

Sometimes, a module comes with documentation of how it works. We can then just see the contents with cat and check it out:

$ cat times.pl
use Timestamp::Simple qw(stamp);
print stamp, "\n";

$ perl times.pl
20090905182627

The following example uses timestamp to create a file with a unique name:

$ cat fn.pl
use Timestamp::Simple qw(stamp);

#Save timestamp in a Variable
$ts = stamp, "\n";

#Stip off the year
$ts =~ s/....(.*)/\1/;

#Create a unique filename
$fn = "myfile." . $ts;

#open, write to, and close the file
open (OUTFILE, '>', "$fn");
print OUTFILE "Hi there.\n";
close (OUTFILE);

$ perl fn.pl
$ ls myf*
myfile.0905183010

We can use substrings in perl, as well:

$ts = substr ($ts, 4); //$ts is assigned to be $ts[4:] basically

What follows, is a list of example programs:

The first, is a script that displays users and what group they are affiliated with:

$ cat groupfind.pl
$user = shift || $ENV{"USER"}; //assigns value of USER from the hashed %ENV variable that holds all variables for the Shell that ran it
say "User $user belongs to these groups:"; //print info
@list = (); //initializes the array
open (my $fh, "grep $user /etc/group |") or die "Error: $!\n"; //Attempt to open the users groups with our fileholder, which is lexical
while ($group = <$fh>) { //While there is something to read, from the filehandle
	chomp $group; //chomp the /n char
	$group =~ s/(.*?):.*/$1/; //regex so that only group is sorted out
	push @list, $group; //push adds the value to the list
}
close $fh; //close the file handle
@slist = sort @list; //sort the list
say "@slist"; //echo out the list

$ perl groupfind.pl
User sam belongs to these groups:
adm admin audio cdrom dialout dip floppy kvm lpadmin ...
}

The next example, uses opendir and readdir:

$ cat dirs2a.pl
#!/usr/bin/perl
print "The working directory contains these directories:\n";

opendir my $dir. '.' or die "Could not open directory: $!\n"; //Open the cwd '.' through opendir with the lexical handler $dir. 
while (my $entry = readdir $dir) { //Reads all the files in the dir, assigning the variable it reads over to the lexical version of $entry
	if (-d $entry) { //if $entry is a dir
		print $entry, ' ' unless ($entry eq '.' || $entry eq '..'); //print $entry, unless it's the working dir or the previous working dir
	}
}
closedir $dir; //close the file handler
print "\n"; //print for formatting

$ ./dirs2a.pl
The working dir contains these directories:
two one

If we want, we can split strings, as follows:

split (<regex>, <string>)

The following program, reads all the User IDs that are above 100, and splits them at the Colon, then prints em out:

$ cat split3.pl
#!/usr/bin/perl -w

open ($pass, "/etc/passwd"); //open the passwd file with a filehandle called $pass
open ($sortout, "| sort -n"); //sort the output
while ($user = <$pass>) { //While there is more passwords to read
	@row = split (/:/, $user);	//row is an array that is defined by splitting UID and username by the :
	if ($row[2] >= 100) {	//if the UID is greater than 100
		print $sortout "$row[2] $row[0]\n"; //passes info to sortout that is piped through sort, print the info
		}
	}

close ($pass); //Close filehandles
close ($sortout); //Close file handles


The last example, is a script that counts the amount of arguments it was called with:

$ cat 10.pl
#!/usr/bin/perl -w

$count = 0; 
$num = @ARGV; //The arguments that its being called with
print "You entered $num arguments on the commandline:\n";
foreach $arg (@ARGV) {
	print ++$count, ". $arg\n"; //print the count, . and the argument that is being iterated from the @ARGV list
	}

$ ./10.pl apple pear banana watermelon
You entered 4 arguments on the command line:
1. apple
2. pear
3. banana
4. watermelon

AWK PATTERN PROCESSING:

The AWK language is a data driven language that performs patternmatching against records (usually lines).

You can use the AWK language to generate reports or filter text.

AWK contains the following features:

Flexible format

Conditional execution (ifs etc.)

Loops

Numeric values

Strings

Regex

Relational Expressions

C's prinf

Coprocess execution (gawk only)

Network data exchange (gawk only)

The general syntax of gawk is as follows:

gawk [options] [program] [file-list] //program is a gawk program, putting it on the commandline allows for writing a short one without having to make a program-file
//to prevent the shell from understanding it as shell commands, we must enclose them in '' quotes
gawk [options] -f program-file [file-list] //file-list contains pathnames of the ordinary files that gawk processes, they are the input files,
//if we do not specify them, they come from standard input or specified by the getline or a coprocess

gawk takes input from either the commandline or from standard input.
If we wish to define more specifically, we can use the getline utility.

Using a coprocess, gawk can interact with another program or exchange data over a network (N/A in awk or mawk)

output from gawk goes to standard output

NOTE: gawk is the graphical version of awk, mawk is a lightweight stripped down version that is faster, awk is the original.
if one does not work in the other utilities, we will write so.

What follows, is a list of options, for awk:

NOTE: the -- format of options only works for gawk.

	gawk 							awk 					Does
--field-seperator <fs> 				-F <fs> 				sets the IFS (input field seperator, which chars account for as splitting, such as space)

--file <program-file> 				-f <program-file> 		Reads from the specified gawk program, instead of the command line. Can be defined more than once, on a line

--help OR -W help 											Summarizes how to use gawk

--lint OR -W lint 											Provides linting (red line under errornous input)

--posix or -W posix 										Runs a POSIX version of gawk.

--traditional or -W traditional 							Ignores the new GNU features in a gawk program, making the program only conform to UNIX awk

--assign <var>=<value> 				-v <var>=<value> 		Assigns a value to a variable. Takes prio before execution, can be included in the BEGIN pattern.
															Can be specified more than once on a commandline

The general pattern of which gawk follows, is as follows:

pattern { action }

If we omit a pattern, it comits its action on all the lines in the specified file

If we omit a action, gawk copies the specified lines to the standard output

It reads line for read, taking action accordingly to the patterns that applies to said line.

We can run regex on stuff enclosed in slashes, the syntax for comparison is same as !~ and =~ (do not match and match, respectively)

The relational operators (< > >= == != etc.) work on both numerals and string characters.

Two unique patterns in gawk, is BEGIN and END. BEGIN applies before gawk processes, END is after.

The range command, in pattern matching, is , , where <pattern1>,<pattern2> is a range of the match of pattern1 to a match of pattern2

If we omit a action, it resorts to default action, which is print. 

We can modify statements that are interacting with print, in a number of ways:

> : Send output to the specified file

>> : Append it to the specified file

| : Pipe it to some other program

|& : This is a coprocess. it is a two-way pipe that exchanges data with a program that runs in the background. (Works only in Gawk)

Unless we seperate items in a print command by , , gawk catenates the items. A comma causes gawk to seperate each item with the OFS (like ifs, except
for output in terms of print and stuff)

Commenting is done by # in gawk

gawk supports user variables and has default values of numerals to 0, string to null.

What follows, is a list of Program variables (think $0 and argv etc.):

$0 : The current record

$1-$n : Fields in the current record //so bascailly indexes of the current record

FILENAME : Name of the current input file (null for std input)

FS : Input field separator  //Default is space and tab

NF : Number of fields in the current record

NR : Record number of the current record

OFS : output field seperator (IFS except for output) //default is space

ORS : output record separator (seperator for output to records) //Default is /n

RS : Input record separator (separator for input to records) //Default is /n

We can also assign a variable on the command line, instead of in a gawk program, with the -assign or -v command.

What follows, are some of the interactions of gawk:

length(str) : Returns the number of chars in <str>. Without an argument, it returns the amount of chars in the current record

int(num) : returns the integer portion of num

index(str1, str2) : Returns index of str2 in str1 or 0 if str2 is not present

split(str, arr, del) : Places elements of str, delimeted by del, in the array arr[1]...arr[n]; returns the number of elements in the array

sprintf(fmt, args) : Formats args according to fmt and returns the formatted string. Mimics the C programming language function of the same name

substr(str,pos,len) : Returns the substring of str that begins at pos and is len characters long

tolower(str) : Returns a copy of str in which all uppercase letters are replaced with their lowercase counterparts

toupper(str) : Returns a copy of str in which all lowercase letters are replaced with their uppercase counterparts

We also have a list of arithmetic expressions, in awk:

** : raised to the power to

* : Multiplication

/ : Division

% : Modulus

+ : Addition

- : Subtraction

= : Assignment variable

++ : increment of variable before operator

-- : decrement of variable before operator

+= : adds an expression to the preceeding value of the operator

-= : same as above, except subtracts

*= : same as above, except multiplies

/= : same as above, except divides

%= : same as above, except modulus

Associoative arrays (hashes) are one of the most powerful features of the awk language.
it allows for arrays that are string based indexes, instead of numeric indexed.

It allows for mimicking of numeric ones, if you wish, as well.

We can assign a element to a associate array, with the general syntax of:

<array>[<string>] = <value>

We can loop through the associative array, if we wish, as follows:

for (<elem> in <array>) <action>

We can use the prinf, in associative arrays, as well, if we wish:

printf "control-string", arg1, arg2, ..., <argN>

the control string defines the format.
To cause conversion, we can use the following syntax:

%[<->][<x>[<.y>]]<conv>

- causes left justification of the arguments, x is minimum field width, .y is the number of places
to the right after a decimal point in a number.

the conv indiciates the conversion type, and the following list showcases the ones available:

d : decimal

e : Exponentional notation

f : floating point

g : use f or e, whicever is shorter

o : unsigned octal

s : String of chars

x : unsigned hexadecimal

What follows, is the usual lineup of control structures:

if...else : if(condition)
					(commands)
				else //optional
					(commands)

As per java, if it's a single line, we need not specify with {}'s:

if ($5 <= 5000) print $0

An example:

$ cat if1
BEGIN 	{
		nam="sam"
		if (nam == "max")
				print "nam is max"
			else
				print "nam is not max, it is", nam
		}

$ gawk -f if1
nam is not max, it is sam

while : 	while (condition)
				(commands)

An example:

$ cat while1
BEGIN{
	n = 1
	while (n <= 5)
		{
		print "2^" n, 2**n
		n++
		}
	}

$ gawk -f while1
1^2 2
2^2 4
3^2 8
4^2 16
5^2 32

etc.

We have for loops:

for (init;condition;increment)
	(commands)

$ cat for1
BEGIN 	{
		for (n=1; n <= 5; n++)
		print "2^" n, 2**n
		}

$ gawk -f for1
1^2 2
2^2 4
3^2 8
4^2 16
5^2 32


We also have a for loop syntax for elements in arrays:

for (<var> in <array>)
	//do commands

{for (name in manuf) print name, manuf[name]} //an example of a element iterating for loop

We have break statements (works as per normal) and continue (works as per normal)

What follows, are list of interactions with the gawk utility, interacting with files, as follows:

$cat cars
plym 	fury 	1970 	73 		2500
chevy 	malibu 	1999 	60 		3000
ford 	mustang 1965 	45 		10000
volvo 	s80 	1998 	102 	9850

//etc.

An example of gawk printing the elements of the file:

$ gawk '{ print }' cars
//prints the file

An example of selecting all specified elemtns:

$ gawk '/chevy/' cars
chevy 	malibu 	1999 	60 	3000
/There are more elements, but it just prints cars that contains the chevy string

To avoid error in terms of spaces and stuff, we should always quote our programs with '' to ensure literal interpetation to the shell

The next example selects elements from all lines in the file:

$ gawk '{print $3, $1}' cars //selects all lines from cars, prints em, selects $3 and $1 elements
1970 plym
1999 chevy
1965 ford
1998 volvo
... 

The next example is a very simple regex of a 'h':

$ gawk '/h/' cars
chevy 	malibu 1999 	60 		3000
ford thundbd 	2003 	15 		10500
//etc. matched all matches that contain h

The next example, is a regex that checks for a h in the first element:

$ gawk '$1 ~ /h/' cars
//will only display cars with h in its name

The next example, forces beginning of the line in the first element:

$ gawk '$1 ~ /^h/' cars
honda accord 2001 30 	6000

The next example, runs a regex on the second element with character classes t and m, adding a $ before the price:

$ gawk '$2 ~ /^[tm]/ {print $3, $2, "$" $5}' cars //checks for all strings that have their second element begin with t or m
1999 malibu  $3000
1965 mustang $10000
2003 thundbd $10500
2000 malibu  $3500
2004 taurus  $17000


What follows, is a regex that runs with the 3 possible uses of $ in terms of a regex for AWK:

1. Reference to an element

2. Itself.

3. Last part of the line

$ gawk '$3 ~ /5$/ {print $3, $1, "$" $5}' cars
1965 	ford 	$10000
1985 	bmw 	$450
1985 	chevy 	$1550

There is two following examples:

the first, showcases where the year matches the numerical evaluation

$ gawk '$3 == 1985' cars
bmw 	325i 	1985 	115 	450
chevy 	impala 	1985 	85 		1550

$ gawk '$5 <= 3000' cars //all cars that cost less than or equal to 3000
plym 	fury 	1970 	73 		2500

What follows, is two examples of attempting to do comparisons in Numerics, the first, being faulty:

$ gawk '"2000" <= $5 && < "9000"' cars
//will show a range of cars worth 2k to 8999

What follows, is an example of the range operator in terms of gawk (,):

$ gawk '/volvo/ , /bmw/' cars
//shows a range from volvo to bmw

What follows, is an example where the gawk finds 3 groups:

$ gawk '/chevy/ , /ford/' cars
chevy 	malibu 		1999 	60 		3000
ford 	mustang 	1965	45 		10000
chevy 	malibu 		2000 	50 		3500
bmw 	325i 		1985 	115 	450
honda 	accord 		2001 	30 		6000
ford 	taurus 		2004 	10 		17000
chevy 	impala 		1985 	85 		1550
ford 	explor 		2003 	25 		9500

When we write longer gawk programs, it might be a good prospect to put them into a file, with the following:

-f/--file <name> //Refers to the designated gawk program that contains a script

The following example, utilizes the BEGIN pattern to commit a header to the file:

$ cat pr_header
BEGIN {print "Make 	Model 	Year 	Miles 	Price"} 
	  {print}

$ gawk -f pr_header cars
Make 	Model 	Year 	Miles 	Price
//followed by the cars file, no formatting occured on it due to no action associated to it, only action was related to the header

What follows, is a more modified BEGIN statement:

$ cat pr_header2
BEGIN {
print "Make 	Model 	Year 	Miles 	Price"
print "---------------------------------------"
}

	 {print}

$ gawk -f pr_header2 cars
Make 	Model 	Year 	Miles 	Price
---------------------------------------
plym 	Fury 	1970 	73 		2500
chevy 	malibu 	1999 	60 		3000
ford 	mustang 1965 	45 		10000
volvo 	s80 	1998 	102 	9850
...

If we give the length function, we get the length of the commands on the commandline.
The following example showcases this, with a pipe to sort the output to create an ordered list based on line length;

NOTE: The list is sorted based on the amount of chars in that specific element

$ gawk '{print length, $0}' cars | sort -n
21 bmw
22 plym fury
23 volvo
24 ford explor
24 toyota
//etc


if we wish to access the current line number of a record, we can use the NR variable, as follows:

$ gawk 'length > 24 {print NR}' cars //prints the line numbers for elements that yield more than 24 chars
2
3
5
6
8
9
11

The next example, showcases a range with implemented NR as the min and max:

$ gawk 'NR == 2 , 'NR == 4' cars
chevy 	malibu 1999 	60 	3000
ford 	mustang 1965 	45 	10000
volvo 	s80 	1998 	102 9850

What follows, is an example of using the total amount of cars as an argument with the END designation:

$ gawk 'END {print NR, "cars for sale."}' cars
12 cars for sale.

What follows, is an example of how to integrate control structures into the mix:

$ cat separ_demo
{
if ($1 ~ /ply/) $1 = "plymouth"    //replaces all occurences of ply with plymouth
if ($1 ~ /chev/) $1 = "chevrolet"  //replaces all occurences of chev with chevrolet
print
}

$ gawk -f separ_demo cars //runs gawk on the cars file with refering to separ_demos script
//gives us a modified list

If we wish, we could also write a standlone script that we can open with a designated file - if we do this,  we must assure ourselves
of that we have reading and writing permissions for said files:

$ chmod u+rx separ_demo2
$ cat separ_demo2
#!/bin/gawk -f
			{
			if ($1 ~ /ply/) $1 = "plymouth"
			if ($1 ~ /chev/) $1 = "chevrolet"
			print
			}

$ ./separ_demo2 cars
//same modified list as last time we ran the script

If we wish, we can define variables in the gawk script, as follows:

$ cat ofs_demo
BEGIN {OFS = "\t"} //the OFS is output field seperator, which defines how chars are seperated, so now instead of a space it's a tab
	{
	if ($1 ~ /ply/)  $1 = "plymouth"
	if ($1 ~ /chev/) $1 = "chevrolet"
	print
	}

$ gawk -f ofs_demo cars
//get a somewhat fixed list

What follows ,is the entirely fixed format, where we use printf to achieve the desired format:

$ cat printf_demo
BEGIN {
	print " 								Miles"
	print "Make 	Model 		Year 		(000) 		Price"
	print "--------------------------------------------------"
}
{
if ($1 ~ /ply/) $1 = "plymouth"
if ($1 ~ /chev/) $1 = "chevrolet"
printf "%-10s %-8s 		%2d 	%5d 	$ %8.2f\n",\ // \ is a line continuation, printf works as per usual with inputting values and forming outputs
	$1, $2, $3, $4, $5
}

What follows, is an example of a program that redirects ouput to two other files, based on occurence:

$ cat redirect_out
/chevy/ 	{print > "chevfile"} //redirect to a file called chevfile if we occur chevy
/ford/ 		{print > "fordfile"} //redirect to a file called fordfile if we occur ford
END 		{print "done."}

$ gawk -f redirect_out cars
done.

$ cat chevfile
chevy 	malibu 1999 	60 		3000
//etc

What follows, is a gawk script example where we start using variables and interactions there of:

$ cat summary
BEGIN {
	  yearsum = 0 ; costsum = 0
	  newcostsum = 0 ; newcount = 0
	  }
	  {
	  yearsum += $3
	  costsum += $5
	  }
$3 > 2000 {newcostsum += $5 ; newcount ++}
END    {
		printf "Average age of cars is %4.1f years\n".\
			2006 - (yearsum/NR)
		printf "Average cost of cars is $%7.2f\n",\
			costsum/NR
			printf "Average cost of newer cars is $%7.2f\n",\
				newcostsum/newcount
	}

$ gawk -f summary cars
Average age of cars is 13.1 years
Average cost of cars is 6216.67$
Average cost of newer cars is $8750

The following example, refers to a specific user and accesses their pwd folder to see how their pwd structure is:

$ awk '/mark/ {print}' /etc/passwd
mark:x:107:100:ext 112:/home/mark:/bin/tcsh

The next example is of a gawk script that finds the Biggest ID, by sorting through em all and adding 1 to the last one , if used on the linux passwd file

$ cat find_uid
BEGIN 	{FS = ":" //Due to the fact that passwd files are delimeted by : , we change the FS seperator to be : to refelct this
		saveit = 0}
$3 > saveit {saveit = $3} //the $3rd element is the part of the ID that is the ID
END 	{print "Next available UID is " saveit + 1} //take the biggst one, + it with 1, claim that its the next available one

$ gawk -f find_uid /etc/passwd //run the script on passwd
Next available UID is 1092

$ cat price_range //sorts out the cars to be delimeted by price ranges
	{
	if 				($5 <= 5000) 				$5 = "inexpensive"
		else if 	(5000 < $5 && $5 < 10000) 	$5 = "please ask"
		else if 	(10000 <= $5) 				$5 = "expensive"
	#
	printf "%-10s %-8s 	%2d 	%5d 	%-12s\n",\
	$1, $2, $3, $4, $5
	}

$ gawk -f price_range cars
//formatted list with pricings and price ranges

What follows, is a associative (hashed) array of amount of cars of each type, in a file:

$ cat manuf
gawk ' 	{manuf[$1]++}
END 	{for (name in manuf) print name, manuf[name]}'
' cars |
sort

$ ./manuf
bmw 1
chevy 3
ford 4
honda 1
plym 1
toyota 1
volvo 1

What follows, is a example of a gawk script that checks for errors:

$ cat manuf.sh
if [ $# != 2 ] //argv or amount of commands is not 2
	then
		echo "Usage: manuf.sh field file" //prompt usage info
		exit 1
fi
gawk < $2 ' //redirect the file content to gawk
		{count[$'$1']++} //assigns the designated file elements of the field to an array called count, where it counts how many times said thing occurs
END 	{for (item in count) printf "%-20s%-20s\n",\ //iterates through the array, print a formated way of what the items are, and the contents of the count array
			item, count[item]}' | //pipe the output
sort //to a sort, since what we have is a number of frequency + item

$ ./manuf.sh
Usage: manuf.sh field file

$ ./manuf.sh 1 cars //Run it with refering to the first element on the file cars
bmw 			1
chevy 			3
ford 			4
honda 			1
//etc.

$ ./manuf.sh 3 cars //Run it with refering to the third element on the file cars
1965 			1
1970 			1
//etc.

Another way to implement the same thing, is to make the output verbose in terms of reading the elements to have gawk read the reference directly:

$ cat manuf2.sh
if [ $# != 2 ] //if the argv is not 2
		then 	
				echo "Usage: manuf.sh field file"
				exit 1
fi
gawk -v "field=$1" < $2 ' //same as before, but by calling the argument to gawk with a verbose format, we can access the field much easier
				{count[$field]++}
END 			{for (item in count) printf "%-20s%-20s\n", \
						item, count[item]}' |
sort

What follows, is an example of a word counter on a file:

$ cat word_usage
tr -cs 'a-zA-Z' '[\n*]' < $1 | //tr is for translate, -c is for translate it to a complement of the ASCII octal numeric set, 01 to 0377
							   //-s is for squeeze, it causes repeating characters to conform to the first occurence of the char
							   //the a-zA-Z is a regex, the '[\n*]' is what it does with anything that does not fit the regex, which means that it substitutes
							   //it with a newline char
							   //and it runs all of this on the input from the $1 specification file
							   //where it then pipes the ouput to gawk
gawk 	'
		{count[$1]++} 	//Increase the count on the occurence on said item
END 	{for (item in count) printf "%-15s%3s\n", item, count[item]}'  |	//loop through the items on the end, printing them by formatting
sort +1nr +0f -1 //n is for numeric sort, r is for reverse, -f is folding, the +1 +0 and -0 are POSIX settings 

$ ./word_usage textfile
//lists words and their number of occurences

The following, is the same thing, but does it in a slightly different manner: //it only accounts for words that occur at least 5 times

$ cat word_count
tr -cs 'a-zA-Z' '[\n*]' < $1 |
gawk ' {
		count[$1]++
}
END 	{
		for (item in count)
			{
			if (count[item] > 4)
				{
				printf "%-15s%3s\n", item, count[item]
				}
		}
} ' |
sort +1nr +0f -1

$ ./word_count textfile | tail
//words that occur 5 times at least

What follows, is an example of how to put a date on top of a report:

$ cat report
if (test $# = 0) then
	echo "You must supply a filename!"
	exit 1
fi
(date; cat $1) | //pipe the output of dates output to gawk 
gawk '
NR == 1 	{print "Report for", $1, $2, $3 ", " $6} //if its the first argument
NR > 1 		{print $5 "\t" $1}' //if its past the first argument

$ ./report cars
Report for Mon Jan 31, 2010
//listing of cars and prices

What follows, is a use of gawk where we process a list of numbers and values, do error checking, discard faulty lines, and format the output:

$ cat numbers
10 		20 		30.3 		40.5
20 		30 		45.7 		66.1
30 		xyz 	50 			70
40 		75 		107.2 		55.6
50 		20 		30.3 		40.5
60 		30 		45.0 		66.1
//etc.

$ cat tally
gawk ' BEGIN 	{ //begin by setting the Outgoing RS to be "", so the seperating char becomes ""
				ORS = ""
				}

NR == 1 {
	nfields = NF //initialize number of fields to be the amount of fields in the file
	}
	{
	if ($0 ~ /[^0-9. \t]/) //Run a regex against numbers . spaces and tabs, escape the escape char with / quotations
		{
		print "\nRecord " NR " skipped:\n\t"
		print $0 "\n"
		next 	//Skip the bad ones
		}
	else
		{
		for (count = 1; count <= nfields; count++) //For the legit, loop through them
			{
			printf "%10.2f", $count > "tally.out"
			sum[count] += $count
			gtotal += $count
			}
		print "\n" > "tally.out"
		}
	}

END 	{
	for (count = 1; count <= nfields; count++) 		//After the last record, print out the summary
		{
		print " 	------" > "tally.out"
		}
	print "\n" > "tally.out"
	for (count = 1; count <= nfields; count++)
		{
		printf "%10.2f", sum[count] > "tally.out"
		}
	print "\n\n 	Grand Total " gtotal "\n" > "tally.out"
} ' < numbers

$ ./tally
Record 3 skipped:
		30 		xyz 	50 		70

Record 6 skipped:
		60 		30 		45.0 	66.1

Record 11 skipped:
		110 	123 	50 		57a.5

$ cat tally.out
	10.00 		20.00 		30.30 		40.50
	20.00 		30.00 		45.70 		66.10
	40.00 		75.00 	   107.20 		55.60
	50.00 		20.00 		30.30 		40.50
	70.00 	  1134.70 		50.00 		70.00
	etc.
	---- 		---- 		---- 		----
	580.00 	  2633.15 		553.90 		490.50

	Grand Total 4257.55

The next example, reads the passwd file to register who has no password set and duplicate IDs

Note: the pwck (LINUX ONLY) does similar things. also, this example won't work under Mac OS X, due to that it uses open dir and not the passwd file.

$ cat passwd_check
gawk < /etc/passwd ' 		BEGIN 	{ ##Get info about the passwd
	uid[void] = "" 					##initialize an array
	}
	{ 								#No pattern means do all the records
	dup = 0 						#initialize the duplicate flag
	split($0, field, ":") 			#splint info fields delimited ":"
	if (field[2] == "")
		{
		if (field[5] == "")
			{
			print field[1] " has no password."
			}
		else
			{
			print field[1] " ("field[5]") has no password."
			}
		}
	for (name in uid)
		{
		if (uid[name] == field[3])
			{
			print field[1] " has the same UID as " name " : UID = " uid[name]
			dup = 1 				#set a duplicate flag
			}
		}
		if (!dup) 	#same as checking for equality to 0, since numeral in comaprison is equated to logical 0 or 1 in this case
			{
			uid[field[1]] = field[3]
			}
		}'

$ ./passwd_check
bill (ext 123) has no password.
toni (ext 357) has no password.
neil has the same UID as toni : UID = 164
dan has no password.
dave has the same UID as sales : UID = 108

what follows is a interactive shell of producing a report for cars:

$ cat list_cars
trap 'rm -f $$.tem > /dev/null;echo $0 aborted.;exit 1' 1 2 15
echo -n "Price range (for example, 5000 7500):"
read lowrange hirange

echo '
								Miles
Make 		Model 		Year 	(000)  			Price
------------------------------------------------------' > $$.tem
gawk < cars '
$5 >= '$lowrange' && $5 <= '$hirange' {
		if ($1 ~ /ply/) $1 = "plymouth"
		if ($1 ~ /chev/) $1 = "chevrolet"
		printf "%-10s %-8s 		%2d 	%d 		$ %8.2f\n", $1, $2, $3, $4,
$5
		}' | sort -n +5 >> $$.tem

cat $$.tem
rm $$.tem

$ ./list_cars 
Price range (for example; 5000 7500):3000 8000 //input for price range is based on users inputs

//listing

When we wish to get information about a line to utilize it's data for other things, we can access it with getline, as follows:

$ cat gl
BEGIN 	{
		getline aa
		print aa
}
$ echo aaaa | gawk -f gl
aaaa //Only reads in 1 line of the commandline

The next few examples, uses the alpha file as defined below:

$ cat alpha
aaaaaaaaaa
bbbbbbbbbb
cccccccccc
dddddddddd

Even if gl is given more than one line, it processes only 1 line:

$ gawk -f gl < alpha
aaaaaaaaaa

when getline doesn't get a argument, it modifies the argumentlist and taps into the next argument it can find:

$ gawk 'BEGIN {getline:print $1}' < alpha
aaaaaaaaaa

What follows, is a while loop that registers data input from a location and keeps it in a variable:

$ cat g2
BEGIN 	{
		while (getline holdme)
			print holdme
		}
$ gawk -f g2 < alpha
aaaaaaaaaa
bbbbbbbbbb
cccccccccc
dddddddddd

If a gawk program has a argument in it's body and not just a BEGIN block, thoose arguments are read into the current arg, $0:

$ cat g3
		{print NR, "$0:", $0}

$ gawk -f g3 < alpha
1 $0: aaaaaaaaaa
2 $0: bbbbbbbbbb
3 $0: cccccccccc
4 $0: dddddddddd

The next example, showcases that getline does not care for automatic reads and $0 references.

$ cat g4
		{
		print NR, "$0:", $0
		getline aa
		print NR, "aa:", aa
		}

$ gawk -f g4 < alpha
1 $0: aaaaaaaaaaa
2 aa: bbbbbbbbbbb
3 $0: ccccccccccc
4 aa: ddddddddddd



} 

What follows, is a getline program that discards b related programs:

$ cat g5
		# print all lines, except those read with getline
		{print "line #", NR, $0}

# if line begins with "b" process it specially
/^b/ 	{
		# use getline to read the next line into variable named hold
		getline hold

		# print value of hold
		print "skip this line.", hold

		# $0 is not affected when getline reads data into a variable
		# $1 still holds previous value
		print "previous line began with:", $1
		}

		{
		print ">>>>> finished processing line #", NR
		print ""
		}

$ gawk -f g5 < alpha
line # 1 aaaaaaaaaa
>>>> finished processing line # 1

line # 2 bbbbbbbbbb
skip this line: cccccccccc
previous line began with: bbbbbbbbbb
>>>> finished processing line # 3

line # 4 dddddddddd
>>>> finished processing line # 4

Coprocesses in Gawk (only supported in gawk), can be identified by the |& syntax.

It is a way to have a two-way pipe between a background process and a foreground one.

The output must be flushed after each use, and must be a filter that reads from standard input and writes to standard output

Since tr does not flush it's output by itself, we can remake it to make it available for a coprocess, as follows:

$ cat to_upper
#!/bin/bash
#set -x //uncomment for debug mode
while read arg
do
	echo "$arg" | tr '[a-z]' '[A-Z]'
done

$ echo abcdef | ./to_upper
ABCDEF

What follows, is a usage of the to_upper script:

$ cat g6
	{ //Because there is no pattern defined, the gawk does this one time for each line
	  //the entire thing composes to be a compound statement that enacts as one command, even if it consists of several ones
	print $0 |& "to_upper" //the quotation marks are required for this kind of a process
	"to_upper" |& getline hold
	print hold
	}

$ gawk -f g6 < alpha
AAAAAAAAAA
BBBBBBBBBB
CCCCCCCCCC
DDDDDDDDDD

If we wish to gain info from another system through the internet, we can use a special command that is formatted the following way:

/inet/<protocol>/<local-port>/<remote-host>/<remote-port>

where:

protocol is usually tcp but can be udp

local port is 0 if we wish for gawk to decide for us, otherwise we specify one

remote-host is the remote hosts IP OR fully qualified domain name of the remote host

remote-port is the port of the remote host ; But instead of a port, we could specify http or ftp

The second part of getting info from a Internet source, is to send a GET signal, along with the file and co-process it to the file, as follows:

$ cat g7
BEGIN
	# set variable named server
	# to special networking filename
	server = "/inet/tcp/0/www.rfc-editor.org/80"

	# use coprocess to send GET request to remote server
	print "GET /rfc/rfc-retrieval.txt" |& server

	#while loop uses coprocess to redirect
	# output from server to getline
	while (server |& getline)
		print $0
	}

$ gawk -f g7
			
			Where and how to get new RFCS
			=============================
//info 

THE SED EDITOR:

This is a noninteractive editor that is used a lot in pipes.
It only runs it's output once, so it's more efficient than interactive editors like ed.

The syntax of the sed editor, is usually as follows:

sed [-n] <program> [<file-list>]
sed [-n] -f <program-file> [<file-list>]

The sed takes input from specified files on the commandline or from STD input.
Output goes to STD output.

the first format is for shorter programs that are not allocated to a file.

The second format, uses a pathname of a file containing a sed program, the file-list containing the
pathnames of the ordinary files that sed processes, these are the input files.

If we do not specify a input file, sed takes input from std input.

What follows is a list of options, where the - options work only under Linux and OS X, where as -- works only in Linux

--file <programfile> OR -f <programfile> : Causes sed to read its program from the file instead of cmdline. can be used more than once on a cmdline

--help : Summarizes how to use sed.

--in-place[=suffix] OR -i[suffix] : Edits file in place. Without this option sed sends its output to standard output. with this option
sed replaces the file its processing with its output. When you specify a suffix, sed makes a backup of the original file.

The backup has the original filename with suffix appended. You must include a period in suffix if you want a period to appear between
the original filename and suffix.

--quiet or --silent or -n : Causes sed not to copy lines to standard output except as specified by the Print (p) instruction or flag.

A sed program, consists of the following general syntax:

[address[ ,address]] instruction [argument-list]

the adresses are optional, if we omit them, the sed processes all the lines of the input.

The instruction is what to do with the editing material

The number and kinds of arguments in the argument-list is defined by virtue of instruction
If we wish to set several sed commands on one line, separate the commands with semicolons(;)

The sed processes input, as follows:

1. Reads one line of input from <file-list> or std input

2. Reads the first instruction from <program> or <program-file>. if the adresses select the input line, acts on the input
line (possibly modified by the previous instruction) as the instruction specifies.

3. Reads the next instruction from the program or program-file. if the adresses if the adresses select the input line, acts on the
input line (possibly modified by the prev instruction) as the instruction specifies.

4. repeats step 3 until all commands have been executed.

5. Starts over with step 1 if another line of input remains ; otherwise, sed is finished

A adress is basically a index referal to position of commandlines, where $ is the last.

A regex is possible to be used with sed ; We can use any char other than a / or a /n for this purpose.

We can specify 0, 1 or 2 adresses. If we specify 0, we enact on all input lines, 1 enacts on all defined ones from that specific line,
2 enacts on groups of lines. (Basically 2 adresses is like a range)

Sed has two buffers; Pattern space, which is the place of the initial line to read for sed, and the second is Hold Space.

What follows, is a list of commands and interactions there of:

a (append) : appends to specified places, uses syntax of [adress[, adress]] a\ text\ text\ text
We must end the text part with a \, except if it's the last part to be appended

NOTE: Appended text is ALWAYS written out. Regardless of -n modifier or not.

c (change) : Changes specified lines with same syntax as append or insert

d (delete) : Will not write out the lines selected or finish processing the line. Then begins with the next inputline
and begins with the first instruction from the program or program-file

i (insert) : Like append, but puts it before the text instead of after

N (next without write) : reads the input line and appends it to the current line. can be used for removing /n chars

n (next) : writes out current line, selects next line, processes it

p (print) : writes specified things to standard output. overwrites -n option

q (quit) : causes sed to exit instantly

r <file> (read) : reads the specified file and puts it on the current line.

s (substitute) : regex, has syntax of [address[,address]] s/pattern/replacement-string/[g][p][w file]

The modifiers for the regex in sed is as follows:

g - global

p - print

w - like print, but directs output to specified file

w <file> (write) : like print, but sends to specified file

We have control structures in sed, as follows:

! : Not modifier, example of usage: 3!d (delete all except the third line), $!p (print all except the last line)

{ } : group declaration, used for selection of groups with inputs. ; to seperate multiple commands on a single line.

What follows, is a set of branch instructions:

:label : identifies a location within the sed program, it is useful as a target for the b and t branch instructions

b[label] : Unconditionally transfers control to label, without label, it acts like continue

t[label] : Transfers control to the label only if a substitute instruction has been succesful since the most recent
line of input was read (conditional branch). Without [label], it acts as a continue statement.

The commands so far, apply to Pattern space, which is considered the legit format of where we handle data.
The hold space, is a temporary buffer that we can hold data in whilst we work in the pattern space.

g : Copies the contents of the Hold space to the pattern Space. OVERWRITES THE CONTENTS OF PATTERN SPACE.

G : Appends a /n and the contents of Hold space to the Pattern space

h : Copies the contents of the pattern space to the hold space. OVERWRITES THE CONTENTS OF HOLD SPACE.

H : appends a /n and the contents of Pattern space to the Hold space

x : Exchanges the contents of the Pattern buffer and the hold buffer

What follows, are some examples of interaction with sed:

$ sed '/line/ p' lines //print all lines that contain line, due to no -n option, it also showcases the contents of the file, so it doubles output, almost
Line one.
The second line.
The second line.
The third.
This is line four.
This is line four.
Five.
This is the sixth sentance.
This is line seven.
This is line seven.
Eight and last.

If we were to display all of the relevant lines, we'd do as follows:

$ sed -n '/line/ p' lines
The second line.
This is line four.
This is line seven.

What follows is a print of line 3 through 6:

$ sed -n '3,6 p' lines
The third.
This is line four.
Five.
This is the sixth sentence.

What follows, is a set of instructions to only read the 5 first elements and then quit:

$ sed '5 q' lines
Line one.
The second line.
The third.
This is line four.
Five.

What follows, is the -n '3,6 p' command, but in a file format:

$ cat print3_6
3,6 p

$ sed -n -f print3_6 lines
The third.
This is line four.
Five.
This is the sixth sentence.

What follows, is an example of appending:

$ cat append_demo
2 a\ //append after the second line
AFTER.

$ sed -f append_demo lines
Line one.
The second line.
AFTER.
The third.
This is line four.
Five.
This is the sixth sentance.
This is line seven.
Eight and last.

$ cat insert_demo
/This/ i\  //insert before each occurence of This
BEFORE.

$ sed -f insert_demo lines
Line one.
The second line.
The third.
BEFORE.
This is line four.
Five.
BEFORE.
This is the sixth sentance.
BEFORE.
This is line seven.
Eight and last.

What follows is a change example:

$ cat change_demo
2,4 c\
SED WILL INSERT THESE\
THREE LINES IN PLACE\
OF THE SELECTED LINES.

$ sed -f change_demo lines
Line one.
SED WILL INSERT THESE
THREE LINES IN PLACE
OF THE SELECTED LINES.
Five.
This is the sixth sentence.
This is line seven.
Eight and last.

What follows, is an example of substitution:

$ cat subs_demo
s/line/sentence/p

$ sed -n -f subs_demo lines
The second sentence.
This is sentence four.
This is sentence seven.

What follows, is a substitution that redirects to another file:

$ cat write_demo1
s/line/sentence/w temp //runs substitution of line with sentence and write the output to temp

$ sed -f write_demo1 lines 
Line one.
The second sentence.
The third.
This is sentence four.
Five.
This is the sixth sentence.
This is sentence seven.
Eight and last.

$ cat temp
The second sentence.
This is sentence four.
This is sentence seven.

What follows, is a shell script that executes sed, as follows:

$ cat sub
for file
do
		echo $file
		mv $file $$.subhld
		sed 's/REPORT/report/g
			 s/FILE/file/g
			 s/PROCESS/process/g' $$.subhld > $file
done
rm $$.subhld

$ sub file1 file2 file3
file1
file2
file3

The next example, copies a piece of info, to another file:

$ cat write_demo2
2,4 w temp2

$ sed -n -f write_demo2 lines

$ cat temp2
The second line.
The third.
This is line four.

The next example, writes anything that is NOT in the range of 2,4:

$ cat write_demo3
2,4 !w temp3

$ sed -n -f write_demo3 lines

What follows is a next statement showcase:

$ cat next_demo1
3 n
p

$ sed -n -f next_demo1 lines
Line one.
The second line.
This is line four.
Five.
This is the sixth sentence.
This is line seven.
Eight and last.

It can also be used in terms of a textual adress:

$ cat next_demo2
/the/ n
p

$ sed -n -f next_demo2 lines
//skips 6, due to it containing literal match of "the"

What follows, is an example using the N, which is next without writing:

$ cat Next_demo3
/the/ N   //erases a /n char and puts match on current line
s/\n/ /
p

$ sed -n -f Next_demo3 lines
Line one.
The second line.
The third.
This is line four.
Five.
This is the sixth sentance. This is line seven.
Eight and last.


The next showcases how several commands combine to interact:

$ cat compound.in
1. The words on this page...
2. The words on this page...
//etc

What follows, is chained interaction:

$ cat compound
1,3 s/words/text/ 		//replaces text with words
2,4 s/text/TEXT/ 		//replaces with large letters
3 d 		//delets third line

$ sed -f compound compound.in
1. The text on this page...
2. The TEXT on this page...
//was removed due to 3 d
4. The words on this page...


What follows is an example of appending:

$ cat compound3
2 a\
This is line 2a.\
This is line 2b.
3 p

$ sed -f compound3 compound.in //Runs without the -n option, so no re-prints are obscured
1. The words on this page...
2. The words on this page...
This is line 2a.
This is line 2b.
3. The words on this page...
3. The words on this page...
4. The words on this page...
  

What follows, is a example of that appending still prints out lines, even when deleted afterwards:

$ cat compound4
2 a\
This is line 2a.\
This is line 2b.
2 d 

$ sed -f compound4 compound.in
1. The words on this page...
This is line 2a.
This is line 2b.
3. The words on this page...
4. The words on this page...

What follows, is an example of running a regex on a file to create indention:

$ sed 's/^./\t&/' lines //only puts indenture on the lines that match up with the regex, the regex is any char in the beginning, thus only non empty lines
		Line one.
		The second line.
		The third.
...

We could also use the simplified format of s/^/\t/ , albeit it puts tabs on every single line, instead.

What follows is putting said regex in a file:

$ cat ind
sed 's/^./\t&/' $*
$ chmod u+rx ind
$ ind lines
		Line one.
		The second line.
		The third.
...

The above approach, spawns a shell to interact with things. This is a performance overhead, that we can forego by virtue of just applying
that we wish to run sed directly, in the script:

$ cat ind2
#!/bin/sed -f
s/^./\t&/

What follows, is a regex that removes trailing whitespace on end of lines:

$ cat cleanup2
#!/bin/sed -f
s/ *$//

The following uses hold to interact between switching lines:

$ cat sl
h # Copy pattern space (line just read) to Hold space.
n # Read the next line of input into Pattern space.
p # Output pattern space.
g # Copy Hold Space to Pattern Space
p # output pattern space (which now holds the previous line)

$ sed -nf sl lines
The second line
Line one.
This is line four.
The third.
This is the sixth sentence.
Five.
Eight and last.
This is line seven.

The next showcases G to create a space on each and every other line:

$ sed 'G' lines
Line one.

The second line.

The third.

This is line four.

$

What follows is a showcasing of having a script that writes the file into a reversed order:

$ cat s2
2,$G 	# On all but the first line, append a NEWLINE and the
		# contents of the Hold space to the Pattern space.
h		# Copy the Pattern space to the Hold space.
$!d		# Delete all except the last line.

$ sed -f s2 lines
Eight and last.
This is line seven.
This is the sixth sentence.
Five.
This is line four.
The third.
The second line.
Line one.

RSYNC:

Rsync is a utility that opens a OpenSSH connection to copy a Hierarchy of dirs from a local system or another remote system on the network.

The rsync syntax is as follows:

rsync [options] [[user@]from-host:]source-file [[user@]to-host:][destination-file]

from-host is the system you are copying files from

to-host is the name of the host we are copying to

Note: if we do not specify a host, we default to the local system

The user part can be defined by user@ to define another user than that of the current one performing the action.

NOTE: rsync does not allow remote system copying, unlike scp

the source-file is what you are copying

destination file is the resulting copy //the name of the destination file can be ommitted if we are copying to a remote system, because it defaults to the current filename

We can specify files as relative or absolute pathnames.

On the local system, relative pathnames are relative to CWD, on Remote systems, it's relative to that users ~ variable

when the sourcefile is a dir, we have to use --recursive or --archive options to handle it.

IMPORTANT NOTE: to assure copy of contents of dir in terms of source-file designations, you have to comply with / upon the end. Without it, it simply copies the dir.

What follows, is a list of options: //NOTE: -- works under Mac OS X as well as under Linux

--acls 	-A 	: Preserves ACLs of copied files

--archive -a : Copies files including dereferenced symbolic links, device files and special files recursively, preserving ownership, group, permissions
and modification times assoicated with the file. It is basically the same as calling : --devices, --specials, --group, --links, --owner, --perms, --recursive and --times
it does not inlcude --acls, --hard-links or --xattrs. If we wish to have thoose, we must specify them out over archive

--backup -b : Renames files that otherwise would be deleted or overwritten. In renaming, it appends ~ to the filename. 
if we wish to modify the dir, we can access the --backup-dir=<dir> variable. We can also refer to --link-dest=<dir>

--backup-dir=<dir> : if run with --backup, it moves the old version of a file to the specified <dir>. If it's a relative path name, it's relative to destination-file.

--copy-unsafe-links : for each file that is a symbolic link that refers to a file outside the sourcefile-hierarchy, copies the file the link points to, not the symbolic
link itself. Without this option, rsync copies all symbolic links, even if it does not copy the file the link refers to.

-D : same as --devices --specials.

--delete : Deletes files in the destination-file that are not in the source-file NOTE: USE WITH CAUTION, DOES NOT PROMPT FOR WARNING WITHOUT -i

--devices : Copies device files (root user only)

--dry-run : Runs rsync without writing to disk. With the --verbose option, this option reports on what rsync would have done had it been run without
this option. Useful with the --delete option

--group  -g : preserves group associations of files copied

--hard-links -H : preserves hard links of files copied

--links -l : copies all symbolic links

--link-dest=<dir> : if rsync would normally copy a file (file is in src, but not in dst file or is changed in dst file), then rsync looks for the dir for the same file
if it finds it, it makes a hard link from file in dir to destination-file.

If it does not find an exact copy, it makes one.

--owner -o : preservers ownership rights, ROOT ONLY

--perms -p : Preserves permissions of copied files

--recursive -r : Recursively goes down a dir hierarchy and copies everything on the way

--specials : copies special files

--times -t : preserves time modifications for files. Speeds up copying, due to not copying identical time modifications and size mods, if present

--update -u : Skips files that are newer in the dest file than in the source-file.

--verbose -v : Displays what rsync does

--xattrs -X : Preserves extended attributes of copied files. Not compatible with all versions of rsync

--compress -z : Compresses files while copying them.

NOTE: rsync uses OpenSSH connections to copy stuff, the server you are copying from, must run a OpenSSH server to access it. If the SSH requires a pw, then so does rsync.

If we specify :: instead of : after the end of a remote system, we use the daemon instead of using the OpenSSH connection to get into it.

What follows, is a example of how to use --recursive and --verbose:

$ ls -l memos
total 12
-rw-r--r-- 1 max max 1500 Jun 6 14:24 0606
-rw-r--r-- 1 max max 6001 Jun 8 16:16 0608

$ rsync --recursive --verbose memos memos.copy
building file list ... done
created dir memos.copy
memos/
memos/0606
memos/0608

sent 7671 bytes recieved 70 bytes 15482.00 bytes/sec
total size is 7501 speedup is 0.97

$ ls -l memos.copy
total 4
drwxr-xr-x 2 max max 4096 Jul 20 17:42 memos

Note: Time of modification is updated with copying:

$ ls -l memos.copy/memos
total 12
-rw-r--r-- 1 max max 1500 Jul 20 17:42 0606
-rw-r--r-- 1 max max 6001 Jul 20 17:42 0608

note: if we put / after the source file, we do not copy the actual dir itself.

Note: If we wish to see the outcome of running utils without actually running them, we could do as follows:

$ ls -l memos memos.copy3
//files

$ rsync --archive --verbose --delete --dry-run memos/ memos.copy3 <- --dry-run makes this a demonstration instead of actually running it. Extremely useful
building file list ... done
deleting notes
./ //implies it will copy a backup
0610 